<!DOCTYPE html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="description" content="InferLLM 框架中 GPU 内核实现分析
InferLLM 框架中的 GPU 内核实现主要由 kernel_gpu.h 和 kernel_gpu.cu 两个文件组成，它们提供了大语言模型在 GPU 上运行所需的各种计算操作的实现。
1..." />
    <meta name="keywords" content="大语言模型,大语言模型推理,AI-Infra" />
    <link rel="stylesheet" href="https://solahome.github.io/media/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://solahome.github.io/styles/main.css">
    <!-- 代码渲染风格css -->
    
    <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.1/build/styles/vs2015.min.css">
    
    <!-- 代码高亮插件 -->
    <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.1/build/highlight.min.js"></script>
    <!-- 代码复制插件 -->
    <script src="https://cdn.jsdelivr.net/gh/850552586/ericamcdn@0.1/js/clipboard.min.js"></script>
    <!-- live2d插件 -->
    <link rel="stylesheet" href="https://solahome.github.io/media/css/live2d.css">
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js"></script>
    <!-- 最新版本的 Bootstrap 核心 CSS 文件 -->
    <link href="https://cdn.bootcdn.net/ajax/libs/font-awesome/5.13.1/css/all.min.css" rel="stylesheet">
    <!-- 数学公式 -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css"
        integrity="sha384-9tPv11A+glH/on/wEu99NVwDPwkMQESOocs/ZGXPoIiLE8MU/qkqUcZ3zzL+6DuH" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.js"
        integrity="sha384-U8Vrjwb8fuHMt6ewaCy8uqeUXv4oitYACKdB0VziCerzt011iQ/0TqlSlv8MReCm" crossorigin="anonymous">
        </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/contrib/auto-render.min.js"
        integrity="sha384-aGfk5kvhIq5x1x5YdvCp4upKZYnA8ckafviDpmWEKp4afOZEqOli7gqSnh8I6enH" crossorigin="anonymous">
        </script>
    <script>
        renderMathInElement(document.body, {
            delimiters: [{
                left: "$$",
                right: "$$",
                display: true
            },
            {
                left: "$",
                right: "$",
                display: false
            }
            ]
        });
    </script>
    
    <title>[InferLLM大模型推理框架项目](23)kern模块中GPU内核的实现(src/kern/naive/gpu/kernel_gpu.h+.cpp)</title>

    <!-- 大纲索引css调整 -->
    
    <style>
        .markdownIt-TOC {
            margin-left: 28%;
            padding-left: 2px;
            width: 80%;
        }

        .markdownIt-TOC li {
            padding-left: 8%;
        }
    </style>
    
</head>

<body>
    <div id="landlord">
        <div class="message" style="opacity:0"></div>
        <canvas id="live2d" width="240" height="250" class="live2d"></canvas>
    </div>
    <div id="domainname" style="display:none">https://solahome.github.io</div>
    <div id="content">
        <div>
            <!----------- 移动端导航栏👇 ----------------->
            <div id="bar">
                <style>
  .navmobile {
    z-index: 15;
    max-height: 40vh;
    overflow-y: scroll;
    overflow-x: hidden;
  }

  @media screen and (max-width: 1200px) {
    .navbar {
      max-width: 100vw;
    }
  }

  @media screen and (max-width:1200px) and (min-width: 768px) {
    @media (min-width: 768px) {
      .navbar-nav {
        float: right;
        margin: 0;
      }
    }
  }

  @media screen and (max-width:768px) {
    .mbapspan {
      padding: 0;
    }
  }
</style>
<div class="pjaxloading"  onclick="pjaxloadingClose()">
  <div class="loadingbackground"></div>
  <div id="loading">
    <div class="loading">
    </div>
  </div>
</div>
<nav class="navbar navbar-inverse navbar-fixed-top navmobile">
  <div class="container-fluid">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#barmenu"
        aria-expanded="false" id="barbutton">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a data-pjax class="navbar-brand" style="color:white" href="https://solahome.github.io">sola的小屋&nbsp;&nbsp;|</a>
      <span class="navbar-brand mbapspan"><i id="mbaplayer" class="fa fa-play-circle-o" aria-hidden="true"
          style="font-size: 28px;top: 12px;position: absolute;" onclick="aplayerPlay();"></i></span>
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="barmenu">
      <ul class="nav navbar-nav">
        
        
        <li>
          <a data-pjax href="/" style="color:white">
            首页
          </a>
        </li>
        
        
        
        <li>
          <a data-pjax href="/archives" style="color:white">
            归档
          </a>
        </li>
        
        
        
        <li>
          <a data-pjax href="/tags" style="color:white">
            标签
          </a>
        </li>
        
        
        
        <li>
          <a data-pjax href="/post/about" style="color:white">
            关于
          </a>
        </li>
        
        
        
        <li><a data-pjax href="https://solahome.github.io/talk" style="color:white">说说</a></li>
        
        
          
        <li><a data-pjax href="https://solahome.github.io/friends" style="color:white">友链</a></li>

        
        </ul>
      </div><!-- /.navbar-collapse -->
    </div><!-- /.container-fluid -->
  </nav>


  <!-- 引入jQuery核心js文件 -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js"></script>
  <script>
    let btstate = false;
    var bt = $("#barbutton");
    var bm = $("#barmenu");
    bt.click(function () {
      dropdown();
    })
    function dropdown() {
      console.log(btstate);
      //下拉
      if (btstate == false) {
        bt.removeClass("collapsed");
        bt.attr("aria-expanded", "true");
        bm.attr("aria-expanded", "true")
        bm.fadeIn(700);
        btstate = true;
      }
      else {
        bt.addClass("collapsed");
        bt.attr("aria-expanded", "false");
        bm.removeClass("in");
        bm.hide();
        bm.attr("aria-expanded", "false");
        btstate = false;
      }
    }
  </script> 
            </div>
            <!----------- 移动端导航栏👆 ----------------->

            <!--------------- pc端菜单栏👇 ---------------->
            <div class="side"><head>
    <link rel="stylesheet" href="https://solahome.github.io/media/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://solahome.github.io/styles/main.css">
    
    <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.1/build/styles/vs2015.min.css">
    
    <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.1/build/highlight.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/850552586/ericamcdn@0.1/js/clipboard.min.js"></script>
    <link rel="stylesheet" href="https://solahome.github.io/media/css/live2d.css">
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
    <!-- 最新版本的 Bootstrap 核心 CSS 文件 -->
    <link href="https://cdn.bootcdn.net/ajax/libs/font-awesome/5.13.1/css/all.min.css" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>
    <!-- fancybox -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
    <script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>


    <!--------- 数学公式👇---- -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css"
        integrity="sha384-9tPv11A+glH/on/wEu99NVwDPwkMQESOocs/ZGXPoIiLE8MU/qkqUcZ3zzL+6DuH" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.js"
        integrity="sha384-U8Vrjwb8fuHMt6ewaCy8uqeUXv4oitYACKdB0VziCerzt011iQ/0TqlSlv8MReCm" crossorigin="anonymous">
    </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/contrib/auto-render.min.js"
        integrity="sha384-aGfk5kvhIq5x1x5YdvCp4upKZYnA8ckafviDpmWEKp4afOZEqOli7gqSnh8I6enH" crossorigin="anonymous">
    </script>
    <script>
        renderMathInElement(document.body, {
            delimiters: [{
                    left: "$$",
                    right: "$$",
                    display: true
                },
                {
                    left: "$",
                    right: "$",
                    display: false
                }
            ]
        });
    </script>
    
    <!--------- 数学公式👆----------------- -->
    <!--------- daovocie功能👇 ------------------->
    
    <!--------- daovocie功能👆 ------------------->
    <!-- -----------------twikoo评论系统引入👇 --------->
    <script src="https://cdn.jsdelivr.net/npm/twikoo@1.3.1/dist/twikoo.all.min.js"></script>
    <!-- -----------------twikoo评论系统引入👆 --------->
    <!---------------- leancloud api 功能依赖引入👇 -------->
    <script src="https://cdn.jsdelivr.net/npm/leancloud-storage@3.13.0/dist/av-min.js"></script>
    <!---------------- leancloud api 功能依赖引入👆 -------->
    <!------------------ valine评论系统引入👇 ------------------>
    <script src="https://solahome.github.io/media/js/valine.min.js"></script>
    <!------------------ valine评论系统引入👆 ------------------>
    <!---------------- 今日诗词api功能依赖👇 ----------------------->
    <script src="https://sdk.jinrishici.com/v2/browser/jinrishici.js" charset="utf-8"></script>
    <!---------------- 今日诗词api功能依赖👆 ----------------------->
    <!---------------- 今日诗词api功能依赖👇 ----------------------->
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/ispeak-bber/ispeak-bber.min.js" charset="utf-8"></script>
    <!---------------- 哔哔功能依赖👆 ----------------------->


</head>

<style>
    body {
        overflow-x: hidden;
        height: 100vh;
        overflow-y: scroll;
        cursor: url("https://solahome.github.io/media/images/mouse_cursor.png"), auto
    }
</style>

<body>
    <!--=------ 页面加载动态画面👇 --------------->
    <div class="pjaxloading" onclick="pjaxloadingClose()">
        <div class="loadingbackground"></div>
        <div id="loading-aname">
            <div class="loading-aname">
            </div>
        </div>
    </div>
    <!--=------ 页面加载动态画面👆 --------------->

    <!--------------- 左侧菜单栏👇 ------------------------>
    
    <div id=side>
        <div class="r_m_div">
            <a onclick="showAaplayer()"><i style="font-size: 20px;"><img
                        src='https://solahome.github.io/media/images/music.png' alt="音乐播放器" class="musicbtn"></i></a>
        </div>
        <div class="avatar-border">
            <img src="https://solahome.github.io/images/avatar.png?v=1750254968881" class="avatar">
        </div>
        <div class="sitename">sola的小屋</div>
        <span class="description" data-text='["故事的结尾总写在开头"]'>&nbsp;</span>
        <div class="siteinfocontainer">
            <span class="siteinfo">文章</span>
            <span class="siteinfo">标签</span>
        </div>
        <div class="siteinfocontainer">
            <span class="siteinfo" id="siteinfo-postsnum"></span>
            <span class="siteinfo">20</span>
        </div>
        
        <div class="search">
            <!-- <input type="text" class="search-input" placeholder="标题搜索(●'◡'●)" /> -->
            <input type="text" class="search-input" placeholder="标题搜索 ⚆_⚆ つ♡">
            <div class="search-results"></div>
        </div>
        
    <div class="share-button">
        <span></span>
        
        <a href="https://github.com/solahome" target="_blank"><i style="font-size: 20px;"><img class="icon"
                    src="https://solahome.github.io/media/images/github.png" alt=""></i></a>
        
        
        
        <a href="https://www.zhihu.com/people/ovo-27-42-70" target="_blank"><i style="font-size: 20px;"><img class="icon"
                    src="https://solahome.github.io/media/images/zh.png" alt=""></i></a>
        
        
        
        
        
        
    </div>
    <div id="qq" style="display:none"></div>
    <!------------ 加载菜单按钮👇 ------------------------->
    <div class="menucontainer">
        
        
        <div class="mchocie description">
            <a data-pjax href="/" class="menubutton" style="margin-right: 15px;">
                <i><img class="icon menuicon" src="" alt="首页"></i>&nbsp;<span
                    data-text="首页">首页</span>
            </a>
        </div>
        
        
        
        <div class="mchocie description">
            <a data-pjax href="/archives" class="menubutton" style="margin-right: 15px;">
                <i><img class="icon menuicon" src="" alt="归档"></i>&nbsp;<span
                    data-text="归档">归档</span>
            </a>
        </div>
        
        
        
        <div class="mchocie description">
            <a data-pjax href="/tags" class="menubutton" style="margin-right: 15px;">
                <i><img class="icon menuicon" src="" alt="标签"></i>&nbsp;<span
                    data-text="标签">标签</span>
            </a>
        </div>
        
        
        
        <div class="mchocie description">
            <a data-pjax href="/post/about" class="menubutton" style="margin-right: 15px;">
                <i><img class="icon menuicon" src="" alt="关于"></i>&nbsp;<span
                    data-text="关于">关于</span>
            </a>
        </div>
        
        

        
        <div class="mchocie description">
            <a data-pjax href="https://solahome.github.io/talk" class="menubutton" style="margin-right: 15px;">
                <i><img class="icon menuicon" src="" alt="说说"></i>&nbsp;<span data-text="说说">说说</span>
            </a>
        </div>
        
        
        
        <div class="mchocie description">
            <a data-pjax href="https://solahome.github.io/friends" class="menubutton" style="margin-right: 15px;">
                <i><img class="icon menuicon" src="" alt="友人帐"></i>&nbsp;<span data-text="友人帐">友人帐</span>
            </a>
        </div>
        
        <!--  -->
    </div>
    <!------------ 加载菜单按钮👆 ------------------------->
    <hr>
    <style>
    .copyright {
        font-size: 15px;
        color: #d9d9d9;
    }


    @media screen and (min-width: 1200px) {
        .foot {
            width: 100%;
            height: 130px;
        }
    }

    @media screen and (max-width: 1200px) {
        .foot {
            display: none;
        }
    }
</style>

<body>
    <div class="foot">
        <div id="footinfo">Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | Theme: <a
                href="https://github.com/850552586/gridea-theme-fog">Fog</a>
            <div>
                
                <div id="sitegotimeDate">载入天数...</div>
                <div id="sitegotimes">载入时分秒...</div>
                <div id="cussitetime" style="display:none">07/20/2020</div>
                
            </div>
            <div>
                
                <script async
                    src="https://cdn.jsdelivr.net/gh/850552586/ericamcdn@0.1/js/busuanzi.pure.mini.js"></script>
                <div id="vistornum" style="margin:0">
                    <span id="busuanzi_container_site_uv">
                        <i class="fa fa-heart" aria-hidden="true" style="color: red;"></i> 总访问量:<span
                            id="busuanzi_value_site_pv"></span>&nbsp;&nbsp;|&nbsp;&nbsp;
                        <i class="fa fa-user-o" aria-hidden="true"></i> 访问人数:<span id="busuanzi_value_site_uv"></span>
                    </span>
                </div>
                
            </div>
            <div class="copyright" style="color:white;text-align: center;"> 
            </div>
        </div>



    </div>
</body>

<script>
    //----------------------站点运行时间
    var now = new Date();

    function createtime() {
        var sitegotime = document.getElementById("cussitetime").innerHTML + " 00:00:00";
        var grt = new Date(sitegotime); //此处修改你的建站时间或者网站上线时间 
        now.setTime(now.getTime() + 250);
        days = (now - grt) / 1000 / 60 / 60 / 24;
        dnum = Math.floor(days);
        hours = (now - grt) / 1000 / 60 / 60 - (24 * dnum);
        hnum = Math.floor(hours);
        if (String(hnum).length == 1) {
            hnum = "0" + hnum;
        }
        minutes = (now - grt) / 1000 / 60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes);
        if (String(mnum).length == 1) {
            mnum = "0" + mnum;
        }
        seconds = (now - grt) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds);
        if (String(snum).length == 1) {
            snum = "0" + snum;
        }
        document.getElementById("sitegotimeDate").innerHTML = "本站已安全运行 " + dnum + " 天 ";
        document.getElementById("sitegotimes").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }

    let intervalId = setInterval("createtime()", 250);
</script>

    </div>

    <img class="moon" src="https://solahome.github.io/media/images/moon.png" onclick="moonlight()">
    <script src="https://solahome.github.io/media/js/wordshow.js"></script>
    
    <!--------------- 左侧菜单栏👆 ------------------------>

    <!----------------- 顶部菜单栏👇 ------------------------>
    
    <!----------------- 顶部菜单栏👆 ------------------------>

<!--核心功能加载-->
<script src="https://solahome.github.io/media/js/mainfunc.js"></script>

<!------------- Fog1.0版本彩蛋👇 -------------->
    <!-- <section id="terrestrial" class="popup">
        <a href="#" class="back">&lt; back</a>
    </section>

    <div class="flycontainer">
        <div class="flyword">御风飞行中...</div>
    </div>

    <script>
        $("#fog_go").click(function(){fog_go()});
        async function fog_go(){
            $(".bars").hide()
            $("#live2d").hide()
            await sleep(3000);
            $(".flycontainer").css("display","flex");
            $(".popup").fadeOut(500)
            await sleep(2000);
            var sitename = "sola的小屋";
            getFogUser(sitename);
        }

        function fog_goregis(){
            var sitename = "sola的小屋";
            var siteurl = "https://solahome.github.io";
            regisFogUser(sitename,siteurl);
        }
        fog_goregis();
    </script> -->
<!------------- Fog1.0版本彩蛋👆 -------------->
<script>
    function change_topmenu(){
        tmcolor = "antiquewhite"
        tsnamecolor = "antiquewhite"
        $(".navbar-inverse .navbar-nav>li>a").css("color",tmcolor)
        $(".navbar-inverse .navbar-brand").css("color",tsnamecolor)
        $(".navbar-fixed-top").css("border-radius","0 0 10px 10px")
    }
</script>

<!------ 根据自定义配置声明变量👇 --------------------->
    
    <script>
        var postnumChoice = false
    </script>
    
    
    <script>
        var shareChoice = true
    </script>
    
    
    <script>
        var donateChoice = true
    </script>
    
    
    <script>
        var searchChoice = true
    </script>
    
    <script>let postsnum = 0;</script>
    <div id="side-posttitle" style="display: none;">
    
    
    [InferLLM大模型推理框架项目](27)kern中ARM优化模块的optimized代码分析(src/kern/optimized/arm/optimized.h)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](26)kern中ARM优化模块的kernel代码分析(src/kern/optimized/arm/kernel_gpu.h+.cpp)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](25)kern中ARM优化模块整体分析(src/kern/optimized/arm)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](24)kern中optimized模块整体分析(src/kern/optimized)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](23)kern模块中GPU内核的实现(src/kern/naive/gpu/kernel_gpu.h+.cpp)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](22)kern模块中量化的实现(src/kern/naive/quantize.h)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](21)kern模块中naive的实现(src/kern/naive/naive.h+.cpp)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](20)kern中naive模块的概述(src/kern/naive)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](19)kernel类的定义(src/kern/kernel.h)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](18)kernel计算的基础定义实现(src/kern/kernel_define.h)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](17)InferLLM模块kern的概述(src/kern)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](16)LlamaLike模型代码实现(src/graph/llama_like.h+.cpp)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](15)Llama系列模型代码实现(src/graph/ggml_llama.h+.cpp)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](14)ChatGLM系列模型代码实现(src/graph/...)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](13)计算图的工厂方法实现(src/graph/graph_imp.cpp)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](12)graph模块的功能和具体实现(src/graph)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](11)Op类的实现(src/core/op.h+.cpp)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](10)ThreadPool类的实现(src/core/thread_pool.h+.cpp)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](09)ModelImp类的实现(src/core/model_imp.h+.cpp)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](08)Model类的实现(src/core/model.h+.cpp)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](07)KvStorage类的实现(src/core/kvstorage.h+.cpp)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](06)Graph类的实现(src/core/graph.h+.cpp)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](05)Device类的实现(src/core/device.h+.cpp)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](04)Tensor类的实现(src/core/tensor.h+.cpp)
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](03)InferLLM核心模块core(src/core)综合介绍
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](02)InferLLM的src模块
    <script>
        postsnum += 1
    </script>
    
    
    
    [InferLLM大模型推理框架项目](01)项目概括
    <script>
        postsnum += 1
    </script>
    
    
    
    数字设计与计算机架构课程的第三讲笔记
    <script>
        postsnum += 1
    </script>
    
    
    
    数字设计与计算机架构课程的第二讲笔记（3）
    <script>
        postsnum += 1
    </script>
    
    
    
    数字设计与计算机架构课程的第二讲笔记（2）
    <script>
        postsnum += 1
    </script>
    
    
    
    数字设计与计算机架构课程的第二讲笔记（1）
    <script>
        postsnum += 1
    </script>
    
    
    
    Leetcode每日一题：9. 回文数
    <script>
        postsnum += 1
    </script>
    
    
    
    Leetcode每日一题：11. 盛最多水的容器
    <script>
        postsnum += 1
    </script>
    
    
    
    Leetcode每日一题：893. 特殊等价字符串组
    <script>
        postsnum += 1
    </script>
    
    
    
    Leetcode每日一题：7. 整数反转
    <script>
        postsnum += 1
    </script>
    
    
    
    每日一题：1006 换个格式输出整数
    <script>
        postsnum += 1
    </script>
    
    
    
    每日一题：1005 继续(3n+1)猜想
    <script>
        postsnum += 1
    </script>
    
    
    
    每日一题：1004 成绩排名
    <script>
        postsnum += 1
    </script>
    
    
    
    wsl的相关命令
    <script>
        postsnum += 1
    </script>
    
    
    
    读书笔记(一) 人工智能导论 绪论
    <script>
        postsnum += 1
    </script>
    
    
    
    计算机发展历程
    <script>
        postsnum += 1
    </script>
    
    
    
    什么是因特网
    <script>
        postsnum += 1
    </script>
    
    
    
    每日一题：1002 写出这个数
    <script>
        postsnum += 1
    </script>
    
    
    
    每日一题：1001 害死人不偿命的(3n+1)猜想
    <script>
        postsnum += 1
    </script>
    
    
    
    冯•诺依曼结构的要点
    <script>
        postsnum += 1
    </script>
    
    
    
    浅谈C和C++中的头文件
    <script>
        postsnum += 1
    </script>
    
    
    
    Python中Tab键与空格键在处理代码缩进时的问题
    <script>
        postsnum += 1
    </script>
    
    
    
    关于scanf/printf及cin/cout在刷题时的选择
    <script>
        postsnum += 1
    </script>
    
    
    
    C++运行中的错误：error: stray &#39;\302&#39;(或&#39;\240&#39;) in program
    <script>
        postsnum += 1
    </script>
    
    
    
    Python编辑器的选择
    <script>
        postsnum += 1
    </script>
    
    
    
    关于Python的包管理器Anaconda的一些常见问题
    <script>
        postsnum += 1
    </script>
    
    
    
    写给刚刚开始学习Python的初学者
    <script>
        postsnum += 1
    </script>
    
    
    
    漫漫长路一相逢，便胜却人间无数
    <script>
        postsnum += 1
    </script>
    
    
    </div>
    
        <script>
            var menupos = 'left';
            $("#siteinfo-postsnum").html(postsnum);
        </script>
    
    <script>
        let commentsHide = false
    </script>
    <!------ 根据自定义配置声明变量👆 --------------------->


    <!----------根据自定义配置设置图标信息👇---------->
    <script>
        var icondict = new Array();
    </script>
    
        
    
    <!----------根据自定义配置设置图标信息👆---------->
</body>

<script src="https://solahome.github.io/media/share/dist/Share.js"></script>
<!-- 页面pjax测试 -->
<script src="https://cdn.bootcdn.net/ajax/libs/jquery.pjax/2.0.1/jquery.pjax.js"></script>
<!--确保jQuery已经在此之前加载-->
<!-- <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script> -->
<script src="https://solahome.github.io/media/js/all-pjax.js"></script>
<script>
    //-------------------------------说说功能👇--------------------------------------
    let talksall = [];
    let talks = [];
    let talkindex = 0;
    get();

    function get() {
        var t = JSON.parse('[{"author":"sola","img":"https://i.postimg.cc/Bbrc9BvD/shuoshuo202009110956.jpg","text":"最近忙里偷闲挤出了一点时间读了三秋缒老师的《不哭不哭，痛痛飞走吧》，虽然很早就知道到了三秋缒的作品了，比如说《三日间的幸福》等等，不过实际这本书算是实际上第一次读的三秋缒老师的书。“爱也许就是人类最伟大的发明，或者是全世界最温柔的谎言”。看完这本书给我了一种淡淡的忧伤的感觉，等过一段时间有空了给大家分享一下我读后的感受（好像好久都没有写过这种东西了 哈哈）","time":"2020-09-11"},{"author":"sola","img":"https://i.postimg.cc/D09rW5ny/shuoshuo202009201030.jpg","text":"最近有在零零星星的时间中连着读了五十岚雄策老师的好几本书——《恋爱的死神，与我被遗忘的夏天》、《七日间的幽灵，第八日的女友》、《与你相约在未来的七月雪》。五十岚雄策老师的故事一般最后都有一个挺幸福的结局，相比于那种淡淡的忧伤，五十岚雄策老师的作品最后一般都是一种淡淡的甜味。读多了令人潸然泪下的故事之后，不妨试试，转换一下心情吧。","time":"2020-09-20"},{"author":"sola","img":"https://cdn.colorhub.me/yZoSKZ7KzjI/rs:auto:0:500:0/g:ce/fn:colorhub/bG9jYWw6Ly8vOWQvMDYvMjcwOWZmNzczYTA1MmVjYmM3NzJiMDk2YTQzZWIyYjUxMmE3OWQwNi5qcGc.webp","text":"最近发现vscode更新后越来越好用了，大家可以尝试一下哟o(^▽^)o","time":"2020-11-15"},{"author":"sola","img":"https://cdn.colorhub.me/Iw6z_han0oU/rs:auto:0:500:0/g:ce/fn:colorhub/bG9jYWw6Ly8vMWQvMjUvYjdiZjcxMWFhYThlMWIyY2UxYmYzMjU2MjBlNjJmNzZjNDcwMWQyNS5qcGVn.webp","text":"开始了一个新的内容“每日一题”，应该会记录一些我现在刷的题，争取做到每天更新(ง •_•)ง","time":"2020-11-19"},{"author":"sola","img":"https://cdn.colorhub.me/ECllLb4fnWk/rs:auto:0:500:0/g:ce/fn:colorhub/bG9jYWw6Ly8vYTYvY2UvZGZiOGYzNGU4NmVhZGYwODUxYTEyZTdhYzExZmMyYTZmMzAyYTZjZS5qcGVn.webp","text":"最近一连串考试终于是暂且告一段落了，之后大概就会恢复正常更新速度吧o(*≧▽≦)ツ","time":"2020-12-04"},{"author":"sola","img":"https://cdn.colorhub.me/B8N749UXK9k/rs:auto:0:500:0/g:ce/fn:colorhub/bG9jYWw6Ly8vMDYvYjAvNTViMGRmYjdjYTc0Nzc0NGMyZTFiY2Q1NzlhMmYzYjc1MWZiMDZiMC5qcGc.webp","text":"这几天一直再虚拟机里玩各种Linux的发行版本，尝试各种各样的美化，有空的话，找个机会给大家分享一下。","time":"2020-12-14"},{"author":"sola","img":"https://cdn.colorhub.me/0NuwkwkenXs/rs:auto:0:500:0/g:ce/fn:colorhub/bG9jYWw6Ly8vNzMvNjYvMDdlOGNkMzIxNjFlMzYyNzE3ZDhjOGM5MzRkMmMzNThkYTYwNzM2Ni5qcGVn.webp","text":"大家元旦快乐o(*≧▽≦)ツ","time":"2021-01-01"},{"author":"sola","img":"https://cdn.colorhub.me/LytaQWs3YRo/rs:auto:0:500:0/g:ce/fn:colorhub/bG9jYWw6Ly8vY2IvMmEvYmVlMTYxNjgwNjlmNjJhODRjNmVkMWYwNWViYTYyMzA1YzNiY2IyYS5qcGc.webp","text":"有好一阵子没更新啦，最近准备新开一个栏目，leetcode刷起来，这次要好好坚持下去啦！","time":"2023-03-07"}]');
        talkindex = t.length - 1;
        for (var i = 0; i <= t.length - 1; i++) {
            var d = {};
            d["time"] = t[i].time;
            d["author"] = t[i].author;
            d["text"] = t[i].text;
            d["img"] = t[i].img;
            talksall.push(d);
        }
    }

    function add() {
        for (var i = talkindex; i >= 0 && i > talkindex - 3; i--) {
            talks.push(talksall[i]);
        }
        talkindex = (talkindex - 3) >= 0 ? (talkindex - 3) : -1;
        if (talkindex < 0) $("#getmorebt").hide();
        show();
    }

    function show() {
        var talkT = document.getElementById("talkContainer");
        talkT.innerHTML = "";
        for (var i = 0; i < talks.length; i++) {
            if (talks[i].img != '') {
                talkT.innerHTML += '\
            <div class="talk" v-for="t in talks"> \
                <span data-fancybox="images" href="' + talks[i].img + '"><img src="' + talks[i].img + '" class="talkimg"></span> \
                <div class="ainfo">\
                    <div class="ainfo-intro">\
                        <div class="author">' + talks[i].author + '</div>·\
                        <div class="time">' + talks[i].time + '</div>\
                    </div>\
                </div>\
                <div class="talkcontent">' + talks[i].text + '\
                </div\>\
            ';
            } else {
                talkT.innerHTML += '\
            <div class="talk" v-for="t in talks"> \
                <div class="ainfo">\
                    <div class="ainfo-intro">\
                        <div class="author">' + talks[i].author + '</div>·\
                        <div class="time">' + talks[i].time + '</div>\
                    </div>\
                </div>\
                <div class="talkcontent">' + talks[i].text + '\
                </div\>\
            ';
            }
        }
    }
    //-------------------------------说说功能👆--------------------------------------
</script>

<!---->
<script>
    function replaceAvatar() {
        twikoo_avatar = $(".tk-avatar-img");
        for (var i = 0; i < twikoo_avatar.length; i++) {
            var asrc = twikoo_avatar.eq(i).attr('src');
            if (asrc.search('cn.gravatar.com') != -1) {
                twikoo_avatar.eq(i).attr('src', 'https://solahome.github.io/media/images/comavatar.png');
            }
        }
    }

    function showWechat() {
        var wechat = '';
        alert("博主微信号：" + wechat);
        return false;
    }
</script>

<script>
    /**
     * https://github.com/tangkaichuan/gridea-search
     */
    // 检查缓存是否最新
    function checkCache() {
        var caches_postsnum = localStorage.getItem("postsnum");
        var pt = document.getElementById("side-posttitle").innerHTML;
        pt = pt.replace(/\s+/g, '').replace(/<script>.*?<\/script>/gi, "")
        var ptlen = pt.length;
        var ptlen_storage = localStorage.getItem("poststitlelen");
        if (caches_postsnum != postsnum || ptlen != ptlen_storage) {
            console.log("更新缓存");
            return false
        }
        return true;
    }

    (function () {
        var CACHES = checkCache()
        if (!CACHES) {
            var NOW = Date.now()
            var API_CONTENT = '../api-content/index.html' + '?_=' + NOW
            preload(API_CONTENT);
            getContents(show_getContentresult);
        }

        function show_getContentresult(data) {
            console.log("获取成功");
            var postslen = data["posts"].length;
            localStorage.setItem('postsnum', postslen);
            var poststitlelen = 0;
            var pt = document.getElementById("side-posttitle").innerHTML;
            pt = pt.replace(/\s+/g, '').replace(/<script>.*?<\/script>/gi, "");
            localStorage.setItem('poststitlelen', pt.length);
        }



        // preload
        function preload(url) {
            var preloadLink = document.createElement('link')
            preloadLink.href = url
            preloadLink.rel = 'preload'
            preloadLink.as = 'fetch'
            preloadLink.crossOrigin = 'anonymous'
            document.head.appendChild(preloadLink)
        }

        // 异步 GET 请求
        function get(obj) {
            var xhr = new XMLHttpRequest()
            xhr.open('get', obj.url, true)
            xhr.send(null)
            xhr.onreadystatechange = function () {
                // 异步请求：响应状态为4，数据加载完毕
                if (xhr.readyState === 4) {
                    if (xhr.status === 200) {
                        obj.success(xhr.responseText)
                    } else {
                        obj.error(xhr.status)
                    }
                }
            }
        }

        // 获取博客全文 api
        function getContents(callback) {
            if (CACHES) {
                callback(CACHES.contents)
            } else {
                get({
                    url: API_CONTENT,
                    success: function (data) {
                        callback(JSON.parse(data));
                        localStorage.setItem('ContentsCache', data);
                        getStorageContent();
                    }
                })
            }
        }
    })()
    //------------------------核心功能--pjax全局渲染👇-------------------
    let talkinit = false;
    $(document).pjax('a[data-pjax]', '#main', {
        fragment: '#main',
        timeout: 8000
    }).on('pjax:complete', function () {
        //代码块高亮渲染
        document.querySelectorAll('pre code').forEach((block) => {
            hljs.highlightBlock(block);
        });
        //文章热度
        if(postnumChoice){
            getHotnum();
        }
        //文章索引
        toginit();
        //清空搜索栏
        if (searchChoice) {
            searchInit();
            searchInput.value = "";
        }
        var pl = window.location.pathname;
        //说说更新
        if (pl.search("talk") != -1 && !talkinit) {
            add();
            talkinit = true;
        } else if (pl.search("talk") != -1) {
            show();
        }
        if (pl.search("post") != -1) {
            if (shareChoice)
                shareInit();
            if (donateChoice)
                donateInit();
            if ("default" == "default"&&!commentsHide) {
                $("#comment").hide()
                commentsHide = true
            }
            lazyload();
            codeinit();
            if ("default" == "twikoo") {
                //twikoo头像更换
                setTimeout(replaceAvatar, 3000);
                setTimeout(replaceAvatar, 8000); //部分博主站点速度加载过慢，再添加一个延迟
            }
        }
        if (btstate == true)
            dropdown();
    }).on('pjax:start', function () {
        $(".pjaxloading").fadeIn(50);
    }).on('pjax:end', function () {
        $(".pjaxloading").fadeOut(50);
    });
    //------------------------核心功能--pjax全局渲染👆-------------------

    //-------------------------------------------------搜索
    // 获取搜索框、搜索按钮、清空搜索、结果输出对应的元素
    if (searchChoice) {
        var searchInput = document.querySelector('.search-input');
        var searchResults = document.querySelector('.search-results');
    }
    // 申明保存文章的标题、链接、内容的数组变量
    var searchValue = '',
        arrItems = [],
        arrLinks = [],
        arrTitles = [],
        arrContents = [],
        arrResults = [],
        indexItem = [],
        itemLength = 0;
    var tmpDiv = document.createElement('div');
    tmpDiv.className = 'result-item';


    function getStorageContent() {
        var data = localStorage.getItem('ContentsCache');
        data = JSON.parse(data);
        posts = data["posts"];
        for (var i = 0; i < posts.length; i++) {
            arrLinks[i] = posts[i].link;
            arrTitles[i] = posts[i].title;
            arrContents[i] = posts[i].content;
            itemLength++;
        }
    }
    if (checkCache())
        getStorageContent();
    // 输入框内容变化后就开始匹配，可以不用点按钮
    // 经测试，onkeydown, onchange 等方法效果不太理想，
    // 存在输入延迟等问题，最后发现触发 input 事件最理想，
    // 并且可以处理中文输入法拼写的变化
    searchInput.oninput = function () {
        setTimeout(searchConfirm, 0);
    }
    searchInput.onfocus = function () {
        searchResults.style.display = 'block';
    }

    function searchConfirm() {
        if (searchInput.value == '') {
            searchResults.style.display = 'none';
        } else if (searchInput.value.search(/^\s+$/) >= 0) {
            // 检测输入值全是空白的情况
            searchInit();
            var itemDiv = tmpDiv.cloneNode(true);
            itemDiv.innerText = '请输入有效内容...';
            searchResults.appendChild(itemDiv);
        } else {
            // 合法输入值的情况
            searchInit();
            searchValue = searchInput.value;
            // 在标题、内容中查找
            searchMatching(arrTitles, arrContents, searchValue);
        }
    }

    // 每次搜索完成后的初始化
    function searchInit() {
        arrResults = [];
        indexItem = [];
        searchResults.innerHTML = '';
        searchResults.style.display = 'block';
    }

    function searchMatching(arr1, arr2, input) {
        // 忽略输入大小写
        input = new RegExp(input, 'i');
        var step = 10;
        // 在所有文章标题、内容中匹配查询值
        for (i = 0; i < itemLength; i++) {
            var indexContent = arr2[i].search(input);
            //标题匹配
            if (arr1[i].search(input) != -1) {
                indexItem.push(i); // 保存匹配值的索引
                if (indexContent != -1) {
                    startindex = (indexContent - step) >= 0 ? (indexContent - step) : 0;
                    var searchcontent = arr2[i].slice(indexContent - step, indexContent + 5)
                    arrResults.push("....." + searchcontent + ".....");
                } else
                    arrResults.push("");
            }
            //内容匹配
            else if (indexContent != -1) {
                indexItem.push(i); // 保存匹配值的索引

                startindex = (indexContent - step) >= 0 ? (indexContent - step) : 0;
                var searchcontent = arr2[i].slice(indexContent - step, indexContent + 5)
                arrResults.push("....." + searchcontent + ".....");
            }
        }

        // 输出总共匹配到的数目
        var totalDiv = tmpDiv.cloneNode(true);
        totalDiv.innerHTML = '<b>总匹配：' + indexItem.length + ' 项<hr></b>';
        searchResults.appendChild(totalDiv);

        // 未匹配到内容的情况
        if (indexItem.length == 0) {
            var itemDiv = tmpDiv.cloneNode(true);
            itemDiv.innerText = '未匹配到内容...';
            searchResults.appendChild(itemDiv);
        }

        // 将所有匹配内容进行组合
        for (i = 0; i < arrResults.length; i++) {
            var itemDiv = tmpDiv.cloneNode(true);
            itemDiv.innerHTML = '<a data-pjax class="searchresults" href="' + arrLinks[indexItem[i]] + '"><b>[' +
                arrTitles[indexItem[i]] +
                ']</b><p>' + arrResults[i] + '</p></a><hr />';
            searchResults.appendChild(itemDiv);
        }
    }
</script>



<!--------------------- 菜单栏图标加载功能👇 ----------------------->
<script>
var menulist = $(".menuicon");
var menulength = menulist.length;
for (var i = 0; i < menulength; i++) {
    var altname = menulist.eq(i).attr("alt");
    var iconurl = icondict[altname];
    if (iconurl == undefined || iconurl == '')
        continue;
    if (iconurl == 'Home') {
        menulist.eq(i).attr("src", "https://solahome.github.io/media/images/Home.png");
menulist.eq(i).show();
} else if (iconurl == 'Archive') {
menulist.eq(i).attr("src", "https://solahome.github.io/media/images/Archive.png");
menulist.eq(i).show();
} else if (iconurl == 'Tag') {
menulist.eq(i).attr("src", "https://solahome.github.io/media/images/Tag.png");
menulist.eq(i).show();
} else if (iconurl == 'About') {
menulist.eq(i).attr("src", "https://solahome.github.io/media/images/About.png");
menulist.eq(i).show();
} else if (iconurl == 'Talk') {
menulist.eq(i).attr("src", "https://solahome.github.io/media/images/Talk.png");
menulist.eq(i).show();
} else if (iconurl == 'Friends') {
menulist.eq(i).attr("src", "https://solahome.github.io/media/images/Friends.png");
menulist.eq(i).show();
} else if (iconurl == 'Fly') {
menulist.eq(i).attr("src", "https://solahome.github.io/media/images/wind.png");
menulist.eq(i).show();
} else {
menulist.eq(i).attr("src", iconurl);
menulist.eq(i).show();
}
}
</script>
<!--------------------- 菜单栏图标加载功能👆 ----------------------->

<script>
    //--------------------bber-onetalk加载👇--------------------------------
    function bberonetalkLoad(){
        bberjsonurl = '';
        if ( $("#bber-talk").length > 0 ) {
        jsonUrl = bberjsonurl
        $.getJSON(jsonUrl+"?t="+Date.parse( new Date()),function(res){
        var bberHtml = ''
        $.each(res.data, function(i, item){
            d = new Date(item.date)
            date = d.getFullYear()+'/'+(d.getMonth()+1)+'/'+d.getDate() +' '+d.getHours()+':'+d.getMinutes()+':'+d.getSeconds()
            dataTime = '<span class="datatime">'+date+'</span>'
            bberHtml += '<li class="item item-'+(i+1)+'">'+dataTime+'： <a data-pjax href="https://solahome.github.io/bber">'+item.content+'</a></li>'
        });
        $('#bber-talk').append('<span class="index-talk-icon"><svg viewBox="0 0 1024 1024" width="21" height="21"><path d="M184.32 891.667692c-12.603077 0-25.206154-2.363077-37.809231-7.876923-37.021538-14.966154-59.864615-49.624615-59.864615-89.009231v-275.692307c0-212.676923 173.292308-385.969231 385.969231-385.969231h78.76923c212.676923 0 385.969231 173.292308 385.969231 385.969231 0 169.353846-137.846154 307.2-307.2 307.2H289.083077l-37.021539 37.021538c-18.904615 18.116923-43.323077 28.356923-67.741538 28.356923zM472.615385 195.347692c-178.018462 0-322.953846 144.935385-322.953847 322.953846v275.692308c0 21.267692 15.753846 29.144615 20.48 31.507692 4.726154 2.363077 22.055385 7.876923 37.021539-7.08923l46.473846-46.473846c6.301538-6.301538 14.178462-9.452308 22.055385-9.452308h354.461538c134.695385 0 244.184615-109.489231 244.184616-244.184616 0-178.018462-144.935385-322.953846-322.953847-322.953846H472.615385z"></path><path d="M321.378462 512m-59.076924 0a59.076923 59.076923 0 1 0 118.153847 0 59.076923 59.076923 0 1 0-118.153847 0Z"></path><path d="M518.301538 512m-59.076923 0a59.076923 59.076923 0 1 0 118.153847 0 59.076923 59.076923 0 1 0-118.153847 0Z"></path><path d="M715.224615 512m-59.076923 0a59.076923 59.076923 0 1 0 118.153846 0 59.076923 59.076923 0 1 0-118.153846 0Z"></path></svg></span><ul class="talk-list">'+bberHtml+'</ul>')
        //Lately({ 'target': '#bber-talk .datatime' });
        });
        function Roll (){
        var list_li = $('.talk-list li'),cur_li = list_li.first(),last_li = list_li.last();
        last_li.after(cur_li);
        };
        setInterval(Roll,3000);
    }
    }
  //--------------------bber-onetalk加载👆--------------------------------
</script>

<script>
    var ClicksideState = 1
    var cur_readingmode = false
    function readingmode(){
        if (!cur_readingmode){
            if(menupos == 'left'){
                if(ClicksideState==1)
                menu_openclose1()
            }
            $("#tab1").fadeOut(50)
            $("#bg").fadeOut(50)
            $(".navbar").fadeOut(50)
            $(".markdownIt-TOC li a").css("color","black")
            cur_readingmode = true
        }else{
            if(menupos == 'left'){
                if(ClicksideState==0)
                    menu_openclose1()
            }
            $("#tab1").fadeIn(50)
            $("#bg").fadeIn(50)
            $(".navbar").fadeIn(50)
            $(".markdownIt-TOC li a").css("color","antiquewhite")
            cur_readingmode = false  
        }
    }
</script>


<!----夜间模式👇-->
<script>
    var moonlightState = false
    function moonlight(){
        if(!moonlightState){
            $("#bg").fadeOut(200)
            $('body').css("background","rgba(0,0,0,0.8)")
            $(".markdownIt-TOC li a").css("color","white")
            $(".text p, .text ul").css("color","white")
            $("#articlecontent").css("color","white")
            $(".post-copyright").css("background","rgba(10,10,10,0.5)")
            $('blockquote').css("background","rgba(10,10,10,0.6)")
            $("#articlecontent").css("background","rgba(10,10,10,0.5)")
            moonlightState = true
            $(".moon").attr("src","https://solahome.github.io/media/images/sun.png")
        }else{
            $("#bg").show()
            $(".markdownIt-TOC li a").css("color","antiquewhite")
            $(".text p, .text ul").css("color","rgba(0, 0, 0, 0.7)")
            $("#articlecontent").css("color","black")
            $(".post-copyright").css("background","#f9f9f9")
            $('blockquote').css("background","#eff8f0")
            $("#articlecontent").css("background","white")
            $(".moon").attr("src","https://solahome.github.io/media/images/moon.png")
            $('body').css("background","white")
            moonlightState = false
        }
    }
</script>
<!----夜间模式👆--></div>
            <!--------------- pc端菜单栏👆 ---------------->

            <!-- ----------------左侧菜单栏html 👇-------------------------------------->
            
            <div>
                <div id="tab1" class="tab">
                    <div class="bars">
                        <span></span>
                        <span></span>
                        <span></span>
                    </div>
                    <div class="close"></div>
                </div>
                <div class="col-md-1 col-lg-3"></div>
                <!-- main标签，pjax渲染时加载以下内容👇 -->
                <div id="main" class="col-xs-12 col-sm-12 col-md-12 col-lg-7" style="padding:0;">
                    <div class="readingmode" onclick="readingmode()">阅</div>
                    <link rel="stylesheet" href="https://solahome.github.io/media/css/font-awesome.css">

<body>
    <div class="allcontent" id="postdetail">
        <div class="postshow postpadding" style="padding-bottom: 0;">
            
            <div class="postdetailimg">
                <img src="https://img.colorhub.me/8EyMc0ZadTY/rs:auto:0:500:0/g:ce/fn:colorhub/bG9jYWw6Ly8vZGMvYzYvMTVhYjZlNDY3YTkxMmY4ZTA4NDZiNWM0M2IzOTYwNTgxZDg1ZGNjNi5qcGc.webp" class="postimage" style="cursor:auto">
            </div>
            <div class="postinfo-detail">
                <div class="postdate"><i class="fa fa-calendar"></i>2025-04-28</div>
                <div class="poststatus postdate"><i class="fa fa-clock-o"></i> 23 min read</div>
                <div class="posttag">
                    
                    <a data-pjax href="https://solahome.github.io/tag/QJycDK-Pnt/" class="postlink">
                        <i class="fa fa-tag"></i> 大语言模型
                    </a>
                    
                    <a data-pjax href="https://solahome.github.io/tag/nQrX5LWbHdP/" class="postlink">
                        <i class="fa fa-tag"></i> 大语言模型推理
                    </a>
                    
                    <a data-pjax href="https://solahome.github.io/tag/48Xg_1oAEhE/" class="postlink">
                        <i class="fa fa-tag"></i> AI-Infra
                    </a>
                    
                </div>
            </div>
            
        <div id="articlecontent">
            <div id="texttitle" style="text-align: center">
                <h2 id="ptitle">[InferLLM大模型推理框架项目](23)kern模块中GPU内核的实现(src/kern/naive/gpu/kernel_gpu.h+.cpp)</h2>
                <!-- id 将作为查询条件 -->
                <div id="pl" style="display:none">https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-23/</div>
                <div id="rootaddr" style="display:none">https://solahome.github.io</div>
                
            </div>
            <div class="text ">
                <h1 id="inferllm-框架中-gpu-内核实现分析">InferLLM 框架中 GPU 内核实现分析</h1>
<p>InferLLM 框架中的 GPU 内核实现主要由 <code>kernel_gpu.h</code> 和 <code>kernel_gpu.cu</code> 两个文件组成，它们提供了大语言模型在 GPU 上运行所需的各种计算操作的实现。</p>
<h2 id="1-整体架构">1. 整体架构</h2>
<h3 id="11-文件结构">1.1 文件结构</h3>
<ul>
<li><code>kernel_gpu.h</code>：声明 GPU 内核函数和注册内核</li>
<li><code>kernel_gpu.cu</code>：实现 GPU 内核函数</li>
</ul>
<h3 id="12-核心组件">1.2 核心组件</h3>
<ol>
<li>
<p><strong>cudaHandle 结构体</strong>：管理 CUDA 资源</p>
<pre><code class="language-cpp">struct cudaHandle {
    cudaStream_t stream{nullptr};
    cublasHandle_t cublas_handle{nullptr};
};
</code></pre>
</li>
<li>
<p><strong>内核函数</strong>：实现各种计算操作</p>
<ul>
<li>嵌入查找</li>
<li>元素级操作</li>
<li>归一化</li>
<li>Softmax</li>
<li>矩阵乘法</li>
<li>注意力计算</li>
<li>位置编码</li>
<li>掩码操作</li>
</ul>
</li>
<li>
<p><strong>内核注册机制</strong>：使用宏注册内核函数</p>
<pre><code class="language-cpp">#define PartialImplementKernel(kernel_id, fun)               \
    template &lt;typename... Args&gt;                              \
    struct Comp&lt;KernelID::kernel_id, Args...&gt; {              \
        static void exec(Args... args, cudaHandle* handle) { \
            return fun(std::forward&lt;Args&gt;(args)..., handle); \
        }                                                    \
    };
</code></pre>
</li>
</ol>
<h2 id="2-主要功能模块分析">2. 主要功能模块分析</h2>
<h3 id="21-嵌入查找embedding-lookup">2.1 嵌入查找（Embedding Lookup）</h3>
<pre><code class="language-cpp">void llm_embedding_get_int4_float(
        const void* weights, const uint32_t* index, float* dst, uint32_t len_seq,
        uint32_t embd, cudaHandle* handle);
void llm_embedding_get_float_float(
        const float* weights, const uint32_t* index, float* dst, uint32_t len_seq,
        uint32_t embd, cudaHandle* handle);
</code></pre>
<p>这些函数在 GPU 上实现嵌入查找操作：</p>
<ul>
<li><code>llm_embedding_get_int4_float</code>：从 4 位整数量化的嵌入表中查找</li>
<li><code>llm_embedding_get_float_float</code>：从浮点数嵌入表中查找</li>
</ul>
<p>实现中使用 CUDA 内核函数并行处理多个序列位置：</p>
<pre><code class="language-cpp">__global__ void llm_embedding_get_int4_float_gpu(
        const void* weights, const uint32_t* index, float* dst, uint32_t len_seq,
        uint32_t embd, const int weight_stride) {
    int seq_id = blockIdx.y;
    int thread_id = threadIdx.x + blockIdx.x * blockDim.x;
    if (thread_id &lt; embd / 2) {
        uint32_t row = index[seq_id];
        dst = dst + seq_id * embd;
        const void* src = (static_cast&lt;const char*&gt;(weights) + row * weight_stride);
        int q40_block_id = thread_id * 2 / QK40;
        int block_offset = thread_id % (QK40 / 2);
        BlockQ40* q40_block = (BlockQ40*)src + q40_block_id;
        float scale = q40_block-&gt;d;
        uint8_t value = q40_block-&gt;qs[block_offset];
        const int8_t v1 = value &amp; 0xf;
        const int8_t v2 = value &gt;&gt; 4;
        dst[thread_id * 2] = (v1 - 8) * scale;
        dst[thread_id * 2 + 1] = (v2 - 8) * scale;
    }
}
</code></pre>
<h3 id="22-元素级操作elementwise-operations">2.2 元素级操作（Elementwise Operations）</h3>
<pre><code class="language-cpp">void llm_elemwise_compute_float(
        InData&lt;float&gt; srcs, float* dst, size_t len, ElemMode mode, cudaHandle* handle);
void llm_elemwise_compute_float_scale(
        float* src, float* dst, size_t len, float scale, cudaHandle* handle);
void llm_elemwise_broadcast_dim0_src1_compute_float(
        const float* src0, const float* src1, float* dst, uint32_t len0, uint32_t len1,
        ElemMode mode, cudaHandle* handle);
</code></pre>
<p>这些函数实现了各种元素级操作：</p>
<ul>
<li>加法、乘法</li>
<li>Silu、Gelu 激活函数</li>
<li>缩放操作</li>
<li>广播操作</li>
</ul>
<p>实现中使用函数对象（Functor）和模板元编程简化代码：</p>
<pre><code class="language-cpp">struct SiluFunctor {
    __device__ float operator()(uint32_t i, const float* input) const {
        float src = input[i];
        return src / (1.0 + exp(-src));
    }
};

struct GeluFunctor {
    __device__ float operator()(uint32_t i, const float* input) const {
        float src = input[i];
        return 0.5 * src * (1 + tanh(sqrt(2.0 / PI) * (src + PGELU * src * src * src)));
    }
};

template &lt;typename Function, typename... Args&gt;
__global__ void ApplyFunction(Function functor, int64_t n, float* ret, Args... args) {
    const int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid &lt; n) {
        ret[tid] = functor(tid, args...);
    }
}
</code></pre>
<h3 id="23-归一化normalization">2.3 归一化（Normalization）</h3>
<pre><code class="language-cpp">void llm_norm_compute_float(
        const float* src, float* dst, uint32_t seq_len, uint32_t embd, float eps,
        cudaHandle* handle);
void llm_rms_norm_compute_float(
        const float* src, float* dst, uint32_t seq_len, uint32_t embd, float eps,
        cudaHandle* handle);
</code></pre>
<p>这些函数实现了层归一化和 RMS 归一化：</p>
<ul>
<li><code>llm_norm_compute_float</code>：层归一化，计算均值和方差</li>
<li><code>llm_rms_norm_compute_float</code>：RMS 归一化，只计算均方根</li>
</ul>
<p>实现中使用 warp-level 归约优化性能：</p>
<pre><code class="language-cpp">__global__ void rms_norm_f32(const float* x, float* dst, const int ncols, float eps) {
    const int row = blockIdx.x;
    const int tid = threadIdx.x;
    const int WARP_SIZE = blockDim.x;

    float tmp = 0.0f;  // partial sum for thread in warp
    for (int i = 0; i &lt; ncols; i += WARP_SIZE) {
        const int col = i + tid;
        const float xi = x[row * ncols + col];
        tmp += xi * xi;
    }

    // sum up partial sums
    __syncthreads();
#pragma unroll
    for (int mask = 16; mask &gt; 0; mask &gt;&gt;= 1) {
        tmp += __shfl_xor_sync(0xffffffff, tmp, mask, 32);
    }

    const float mean = tmp / ncols;
    const float scale = 1.0f / sqrtf(mean + eps);

    for (int i = 0; i &lt; ncols; i += WARP_SIZE) {
        const int col = i + tid;
        dst[row * ncols + col] = scale * x[row * ncols + col];
    }
}
</code></pre>
<h3 id="24-softmax">2.4 Softmax</h3>
<pre><code class="language-cpp">void llm_softmax_compute_float(
        const float* src, float* dst, uint32_t len_row, uint32_t col,
        cudaHandle* handle);
</code></pre>
<p>实现 Softmax 函数，使用 warp-level 归约优化性能：</p>
<pre><code class="language-cpp">__global__ void softmax_f32_cuda(const float* x, float* dst, const int cols) {
    const int row = blockDim.y * blockIdx.y + threadIdx.y;
    const int block_size = blockDim.x;
    const int tid = threadIdx.x;
    const float* src = x + row * cols;
    dst = dst + row * cols;

    float max = -INFINITY;
    for (int col = tid; col &lt; cols; col += block_size) {
        const float val = src[col];
        max = val &gt; max ? val : max;
    }

    // sum up partial sums
    __syncthreads();
#pragma unroll
    for (int mask = 16; mask &gt; 0; mask &gt;&gt;= 1) {
        float temp = __shfl_xor_sync(0xffffffff, max, mask);
        max = max &gt; temp ? max : temp;
    }

    float sum = 0.0;
    for (int col = tid; col &lt; cols; col += block_size) {
        const float val = expf(src[col] - max);
        sum += val;
        dst[col] = val;
    }

    // sum up partial sums
    __syncthreads();
#pragma unroll
    for (int mask = 16; mask &gt; 0; mask &gt;&gt;= 1) {
        sum += __shfl_xor_sync(0xffffffff, sum, mask, 32);
    }

    for (int col = tid; col &lt; cols; col += block_size) {
        dst[col] /= sum;
    }
}
</code></pre>
<h3 id="25-矩阵乘法matrix-multiplication">2.5 矩阵乘法（Matrix Multiplication）</h3>
<pre><code class="language-cpp">void llm_matmul_compute_int4_float(
        float* dst, const void* src0, const float* bias, const float* src1, uint32_t M,
        uint32_t N, uint32_t K, void* workspace, uint32_t size, cudaHandle* handle);
void llm_matmul_compute_float_float(
        float* dst, const float* src0, const float* bias, const float* src1, uint32_t M,
        uint32_t N, uint32_t K, void* workspace, uint32_t size, cudaHandle* handle);
</code></pre>
<p>这些函数实现了不同精度的矩阵乘法：</p>
<ul>
<li><code>llm_matmul_compute_int4_float</code>：4 位整数权重与浮点数激活值的矩阵乘法</li>
<li><code>llm_matmul_compute_float_float</code>：浮点数矩阵乘法</li>
</ul>
<p>浮点数矩阵乘法使用 cuBLAS 库优化性能：</p>
<pre><code class="language-cpp">void llm_matmul_compute_float_float(
        float* dst, const float* src0, const float* bias, const float* src1, uint32_t M,
        uint32_t N, uint32_t K, void* workspace, uint32_t size, cudaHandle* handle) {
    cudaStream_t stream = handle-&gt;stream;
    cublasHandle_t cublas_handle = handle-&gt;cublas_handle;
    float alpha = 1.f;
    float beta = 0.f;
    CUBLAS_CHECK(cublasSetStream(cublas_handle, stream));
    CUBLAS_CHECK(cublasSgemm(
            cublas_handle, CUBLAS_OP_T, CUBLAS_OP_N, N, M, K,
            &amp;alpha, src0, K, src1, K, &amp;beta, dst, N));
    if (bias != nullptr) {
        llm_elemwise_broadcast_dim0_src1_compute_float(
                dst, bias, dst, M, N, ElemMode::Add, handle);
    }
}
</code></pre>
<h3 id="26-注意力计算attention-computation">2.6 注意力计算（Attention Computation）</h3>
<pre><code class="language-cpp">void llm_matmul_compute_with_head_stride_float(
        float* dst, const float* srck, const float* srcq, uint32_t seqlen,
        uint32_t embd, uint32_t head, uint32_t nr_past, cudaHandle* handle);
void llm_matmul_compute_with_head_strideq_broadcastk_float(
        float* dst, const float* srck, const float* srcq, uint32_t seqlen,
        uint32_t embd, uint32_t head, uint32_t query_group_num, uint32_t nr_past,
        cudaHandle* handle);
void llm_head_batched_matmul_compute_float(
        float* dst, const float* v, const float* qk, uint32_t seqlen, uint32_t embd,
        uint32_t head, uint32_t nr_past, cudaHandle* handle);
void llm_head_batched_matmul_broadcastv_float(
        float* dst, const float* v, const float* qk, uint32_t seqlen, uint32_t embd,
        uint32_t head, uint32_t query_group_num, uint32_t nr_past, cudaHandle* handle);
</code></pre>
<p>这些函数实现了自注意力机制所需的矩阵运算：</p>
<ul>
<li>多头注意力</li>
<li>多查询注意力（MQA）</li>
</ul>
<p>实现中使用 cuBLAS 的批处理矩阵乘法优化性能：</p>
<pre><code class="language-cpp">void llm_matmul_compute_with_head_stride_float(
        float* dst, const float* srck, const float* srcq, uint32_t seqlen,
        uint32_t embd, uint32_t head, uint32_t nr_past, cudaHandle* handle) {
    uint32_t head_embd = embd / head;
    uint32_t M = seqlen;
    uint32_t N = seqlen + nr_past;
    uint32_t K = head_embd;
    cudaStream_t stream = handle-&gt;stream;
    cublasHandle_t cublas_handle = handle-&gt;cublas_handle;
    float alpha = 1.f;
    float beta = 0.f;
    CUBLAS_CHECK(cublasSetStream(cublas_handle, stream));
    CUBLAS_CHECK(cublasSgemmStridedBatched(
            cublas_handle, CUBLAS_OP_T, CUBLAS_OP_N, N, M, K, &amp;alpha, srck, embd,
            head_embd, srcq, embd, head_embd, &amp;beta, dst, N, M * N, head));
}
</code></pre>
<h2 id="27-位置编码position-encoding">2.7 位置编码（Position Encoding）</h2>
<pre><code class="language-cpp">void llm_rope_compute_float(
        float* dst, const float* src0, uint32_t n_past, uint32_t n_rot, RotMode m,
        uint32_t N, uint32_t head, uint32_t embd, cudaHandle* handle);
void llm_glm_rope_compute_float(
        float* dst, const float* src0, uint32_t n_past, uint32_t gmask_positon,
        uint32_t seqlen, uint32_t head, uint32_t embd, cudaHandle* handle);
</code></pre>
<p>这些函数实现了旋转位置编码（RoPE）和 GLM 模型特定的旋转位置编码：</p>
<pre><code class="language-cpp">template &lt;bool halfmode&gt;
__global__ void rope_compute_float(
        float* dst, const float* src, float theta_scale, int position_offset,
        int n_rot, int seqlen, int head, int head_embd) {
    const int seq = blockIdx.y;
    const int h = blockIdx.x;
    int p = threadIdx.x;

    if (seq &gt;= seqlen || h &gt;= head || p &gt;= head_embd / 2)
        return;

    const int position = seq + position_offset;
    const int half_embd = head_embd / 2;
    const int offset = h * head_embd + seq * head * head_embd;

    if (p &lt; n_rot) {
        const float theta = powf(theta_scale, p);
        const float sin_theta = sinf(position * theta);
        const float cos_theta = cosf(position * theta);

        if (halfmode) {
            const float x0 = src[offset + p];
            const float x1 = src[offset + p + half_embd];
            dst[offset + p] = x0 * cos_theta - x1 * sin_theta;
            dst[offset + p + half_embd] = x0 * sin_theta + x1 * cos_theta;
        } else {
            const float x0 = src[offset + 2 * p];
            const float x1 = src[offset + 2 * p + 1];
            dst[offset + 2 * p] = x0 * cos_theta - x1 * sin_theta;
            dst[offset + 2 * p + 1] = x0 * sin_theta + x1 * cos_theta;
        }
    } else {
        if (halfmode) {
            dst[offset + p] = src[offset + p];
            dst[offset + p + half_embd] = src[offset + p + half_embd];
        } else {
            dst[offset + 2 * p] = src[offset + 2 * p];
            dst[offset + 2 * p + 1] = src[offset + 2 * p + 1];
        }
    }
}
</code></pre>
<p>GLM 模型的位置编码实现了特殊的位置计算逻辑：</p>
<pre><code class="language-cpp">__global__ void glm_rope_compute_float(
        float* dst, const float* src, int32_t n_past, int32_t gmask_positon,
        int32_t seqlen, int32_t head, int32_t embd) {
    const int seq = blockIdx.y;
    const int h = blockIdx.x;
    int p = threadIdx.x;

    if (seq &gt;= seqlen || h &gt;= head || p &gt;= embd / 2)
        return;

    int quart_embd = embd / 4;
    int half_embd = embd / 2;

    int position_id = MIN(seq + n_past, gmask_positon);
    int block_position_id = MAX((n_past + seq) - gmask_positon, 0);

    bool is_second_half = p &gt;= quart_embd;

    position_id = is_second_half ? block_position_id : position_id;

    p = is_second_half ? p - quart_embd : p;

    const float theta = powf(10000.0f, -2.0f * p / quart_embd);
    const float sin_theta = sinf(position_id * theta);
    const float cos_theta = cosf(position_id * theta);

    const int offset = h * embd + seq * head * embd;
    const int rot_offset = is_second_half ? quart_embd : 0;

    const float x0 = src[offset + rot_offset + p];
    const float x1 = src[offset + rot_offset + p + half_embd];
    dst[offset + rot_offset + p] = x0 * cos_theta - x1 * sin_theta;
    dst[offset + rot_offset + p + half_embd] = x0 * sin_theta + x1 * cos_theta;
}
</code></pre>
<h3 id="28-掩码操作masking-operations">2.8 掩码操作（Masking Operations）</h3>
<pre><code class="language-cpp">void llm_diag_mask_inf_float(
        float* dst, const float* src0, uint32_t n_past, uint32_t N, uint32_t head,
        cudaHandle* handle);
void llm_glm_gmask_inf_float(
        float* dst, uint32_t n_past, uint32_t seqlen, uint32_t head,
        cudaHandle* handle);
void llm_scale_diag_mask_inf_float(
        float* dst, const float* src0, float scale, uint32_t n_past, uint32_t seqlen,
        uint32_t head, cudaHandle* handle);
</code></pre>
<p>这些函数实现了注意力掩码操作：</p>
<ul>
<li><code>llm_diag_mask_inf_float</code>：自回归掩码，将上三角部分设为负无穷</li>
<li><code>llm_glm_gmask_inf_float</code>：GLM 模型特定的掩码</li>
<li><code>llm_scale_diag_mask_inf_float</code>：先缩放再掩码</li>
</ul>
<pre><code class="language-cpp">__global__ void diag_mask_inf_f32(
        const float* src, float* dst, const int past, const int len,
        const int head_dim) {
    const int head = blockIdx.z;
    const int row = blockIdx.y * blockDim.y + threadIdx.y;
    const int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row &gt;= len || col &gt;= past + len || head &gt;= head_dim)
        return;

    const int offset = head * len * (past + len) + row * (past + len) + col;
    if (col &gt;= past &amp;&amp; col - past &gt; row) {
        dst[offset] = -INFINITY;
    } else {
        dst[offset] = src[offset];
    }
}
</code></pre>
<p>GLM 模型的掩码实现了特殊的掩码逻辑：</p>
<pre><code class="language-cpp">__global__ void glm_gmask_inf_f32(
        float* dst, const int past, const int seqlen, const int head) {
    const int index = blockIdx.x * blockDim.x + threadIdx.x;
    const int head_id = index / seqlen;
    const int row_id = index % seqlen;

    // the laxt row not set -inf
    if (row_id &gt;= seqlen - 1 || head_id &gt;= head)
        return;

    int total_seq = seqlen + past;
    int offset = head_id * seqlen * total_seq + row_id * total_seq + total_seq - 1;
    dst[offset] = -INFINITY;
}
</code></pre>
<h2 id="3-内核注册机制">3. 内核注册机制</h2>
<p>GPU 内核使用模板和宏实现内核注册机制，将函数与内核 ID 关联起来：</p>
<pre><code class="language-cpp">#define PartialImplementKernel(kernel_id, fun)               \
    template &lt;typename... Args&gt;                              \
    struct Comp&lt;KernelID::kernel_id, Args...&gt; {              \
        static void exec(Args... args, cudaHandle* handle) { \
            return fun(std::forward&lt;Args&gt;(args)..., handle); \
        }                                                    \
    };

#define PartialImplementSpace(kernel_id, fun)        \
    template &lt;typename... Args&gt;                      \
    struct Space&lt;KernelID::kernel_id, Args...&gt; {     \
        static size_t get(Args... args) {            \
            return fun(std::forward&lt;Args&gt;(args)...); \
        }                                            \
    };

#define NOImplementKernel(kernel_id)                         \
    template &lt;typename... Args&gt;                              \
    struct Comp&lt;KernelID::kernel_id, Args...&gt; {              \
        static void exec(Args... args, cudaHandle* handle) { \
            INFER_ASSERT(0, &quot;kernel not implement&quot;);         \
        }                                                    \
    };
</code></pre>
<p>这些宏用于注册已实现的内核和标记未实现的内核：</p>
<pre><code class="language-cpp">PartialImplementKernel(ElemwiseFloat, llm_elemwise_compute_float);
PartialImplementKernel(ElemwiseFloatScale, llm_elemwise_compute_float_scale);
// ... 其他已实现的内核

NOImplementKernel(MatmulInt4FloatPacked);
NOImplementKernel(MatmulInt4WeightReorder);
NOImplementKernel(MatmulInt8Float);
NOImplementKernel(EmbeddingGetInt8Float);
</code></pre>
<h2 id="4-cuda-优化技术">4. CUDA 优化技术</h2>
<h3 id="41-线程块和网格配置">4.1 线程块和网格配置</h3>
<p>代码中根据不同的计算需求，使用不同的线程块和网格配置：</p>
<pre><code class="language-cpp">// 一维网格和线程块
const dim3 block_dims(CUDA_NUM_THREADS, 1, 1);
const dim3 block_nums((len + CUDA_NUM_THREADS - 1) / CUDA_NUM_THREADS, 1, 1);

// 二维网格
const dim3 block_dims(512, 1, 1);
const dim3 block_nums((ncols + 511) / 512, rows, 1);

// 三维网格
const dim3 block_dims(kBlockSize, kBlockSize, 1);
const dim3 block_nums(block_x, block_y, head);
</code></pre>
<h3 id="42-warp-level-归约">4.2 Warp-level 归约</h3>
<p>代码中使用 warp-level 归约优化归一化和 Softmax 等操作：</p>
<pre><code class="language-cpp">// 使用 __shfl_xor_sync 进行 warp 内归约
#pragma unroll
for (int mask = 16; mask &gt; 0; mask &gt;&gt;= 1) {
    tmp += __shfl_xor_sync(0xffffffff, tmp, mask, 32);
}
</code></pre>
<h3 id="43-内存访问优化">4.3 内存访问优化</h3>
<p>代码中使用合适的内存访问模式，减少内存访问开销：</p>
<pre><code class="language-cpp">// 连续访问内存
for (int i = 0; i &lt; ncols; i += WARP_SIZE) {
    const int col = i + tid;
    const float xi = x[row * ncols + col];
    tmp += xi * xi;
}
</code></pre>
<h3 id="44-使用-cublas-库">4.4 使用 cuBLAS 库</h3>
<p>代码中使用 cuBLAS 库优化矩阵乘法和批处理矩阵乘法：</p>
<pre><code class="language-cpp">CUBLAS_CHECK(cublasSgemm(
        cublas_handle, CUBLAS_OP_T, CUBLAS_OP_N, N, M, K,
        &amp;alpha, src0, K, src1, K, &amp;beta, dst, N));

CUBLAS_CHECK(cublasSgemmStridedBatched(
        cublas_handle, CUBLAS_OP_T, CUBLAS_OP_N, N, M, K, &amp;alpha, srck, embd,
        head_embd, srcq, embd, head_embd, &amp;beta, dst, N, M * N, head));
</code></pre>
<h2 id="5-与-cpu-实现的比较">5. 与 CPU 实现的比较</h2>
<p>GPU 实现与 CPU 实现（naive 模块）的主要区别：</p>
<h3 id="51-并行度">5.1 并行度</h3>
<ul>
<li>CPU 实现：使用 TaskSet 和多线程并行</li>
<li>GPU 实现：使用 CUDA 内核函数和数千个线程并行</li>
</ul>
<h3 id="52-内存管理">5.2 内存管理</h3>
<ul>
<li>CPU 实现：直接访问主内存</li>
<li>GPU 实现：需要在 GPU 内存和主内存之间传输数据</li>
</ul>
<h3 id="53-优化方法">5.3 优化方法</h3>
<ul>
<li>CPU 实现：使用 SIMD 指令和缓存优化</li>
<li>GPU 实现：使用 CUDA 内核函数、warp-level 归约和 cuBLAS 库</li>
</ul>
<h3 id="54-功能覆盖">5.4 功能覆盖</h3>
<ul>
<li>CPU 实现：完整实现所有功能</li>
<li>GPU 实现：部分功能未实现（如 <code>MatmulInt4FloatPacked</code>、<code>MatmulInt8Float</code> 等）</li>
</ul>
<h2 id="6-性能考虑">6. 性能考虑</h2>
<h3 id="61-内存传输开销">6.1 内存传输开销</h3>
<p>GPU 计算需要在 CPU 和 GPU 之间传输数据，这可能成为性能瓶颈。代码中使用 CUDA 流和异步操作减少这种开销：</p>
<pre><code class="language-cpp">cudaStream_t stream = handle-&gt;stream;
</code></pre>
<h3 id="62-内核启动开销">6.2 内核启动开销</h3>
<p>CUDA 内核启动有一定开销，代码中尽量减少内核启动次数，将多个小操作合并为一个大操作：</p>
<pre><code class="language-cpp">// 一次内核启动处理多个元素
__global__ void llm_elemwise_compute_float_scale_gpu(
        float* src, float* dst, size_t len, float scale) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index &lt; len) {
        dst[index] = src[index] * scale;
    }
}
</code></pre>
<h3 id="63-线程分配和负载均衡">6.3 线程分配和负载均衡</h3>
<p>代码中根据计算需求，合理分配线程和线程块，确保负载均衡：</p>
<pre><code class="language-cpp">// 根据计算需求调整线程块大小
const dim3 block_dims(kBlockSize, kBlockSize, 1);
const dim3 block_nums(block_x, block_y, head);
</code></pre>
<h3 id="64-内存访问模式">6.4 内存访问模式</h3>
<p>代码中优化内存访问模式，减少内存访问开销：</p>
<pre><code class="language-cpp">// 连续访问内存
for (int col = tid; col &lt; cols; col += block_size) {
    const float val = src[col];
    max = val &gt; max ? val : max;
}
</code></pre>
<h2 id="7-未实现的功能">7. 未实现的功能</h2>
<p>代码中使用 <code>NOImplementKernel</code> 宏标记了未实现的功能：</p>
<pre><code class="language-cpp">NOImplementKernel(MatmulInt4FloatPacked);
NOImplementKernel(MatmulInt4WeightReorder);
NOImplementKernel(MatmulInt8Float);
NOImplementKernel(EmbeddingGetInt8Float);
</code></pre>
<p>这些功能在 CPU 实现中已经实现，但在 GPU 实现中尚未实现。这可能是因为：</p>
<ol>
<li>这些功能在 GPU 上实现复杂度高</li>
<li>这些功能在 GPU 上性能提升有限</li>
<li>开发资源有限，优先实现更重要</li>
</ol>
<h2 id="8-内核注册与调用机制">8. 内核注册与调用机制</h2>
<p>GPU 内核的注册与调用机制是通过模板特化和宏定义实现的，这种设计使得框架可以在运行时根据内核 ID 选择合适的实现。</p>
<h3 id="81-内核注册宏">8.1 内核注册宏</h3>
<pre><code class="language-cpp">#define PartialImplementKernel(kernel_id, fun)               \
    template &lt;typename... Args&gt;                              \
    struct Comp&lt;KernelID::kernel_id, Args...&gt; {              \
        static void exec(Args... args, cudaHandle* handle) { \
            return fun(std::forward&lt;Args&gt;(args)..., handle); \
        }                                                    \
    };

#define PartialImplementSpace(kernel_id, fun)        \
    template &lt;typename... Args&gt;                      \
    struct Space&lt;KernelID::kernel_id, Args...&gt; {     \
        static size_t get(Args... args) {            \
            return fun(std::forward&lt;Args&gt;(args)...); \
        }                                            \
    };

#define NOImplementKernel(kernel_id)                         \
    template &lt;typename... Args&gt;                              \
    struct Comp&lt;KernelID::kernel_id, Args...&gt; {              \
        static void exec(Args... args, cudaHandle* handle) { \
            INFER_ASSERT(0, &quot;kernel not implement&quot;);         \
        }                                                    \
    };
</code></pre>
<p>这些宏定义了三种类型的内核注册：</p>
<ol>
<li><code>PartialImplementKernel</code>：注册已实现的计算内核</li>
<li><code>PartialImplementSpace</code>：注册工作空间计算函数</li>
<li><code>NOImplementKernel</code>：标记未实现的内核</li>
</ol>
<h3 id="82-内核注册列表">8.2 内核注册列表</h3>
<pre><code class="language-cpp">PartialImplementKernel(ElemwiseFloat, llm_elemwise_compute_float);
PartialImplementKernel(ElemwiseFloatScale, llm_elemwise_compute_float_scale);
PartialImplementKernel(
        ElemwiseBroadcastDim0Src1Float, llm_elemwise_broadcast_dim0_src1_compute_float);
PartialImplementKernel(NormFloat, llm_norm_compute_float);
PartialImplementKernel(RmsNormFloat, llm_rms_norm_compute_float);
PartialImplementKernel(EmbeddingGetInt4Float, llm_embedding_get_int4_float);
PartialImplementKernel(EmbeddingGetFloatFloat, llm_embedding_get_float_float);
PartialImplementKernel(SoftmaxFloat, llm_softmax_compute_float);
PartialImplementKernel(MatmulInt4Float, llm_matmul_compute_int4_float);
PartialImplementKernel(MatmulFloatFloat, llm_matmul_compute_float_float);
PartialImplementKernel(
        MatmulWithHeadStrideFloat, llm_matmul_compute_with_head_stride_float);
PartialImplementKernel(HeadBatchedMatmulFloat, llm_head_batched_matmul_compute_float);
PartialImplementKernel(DiagMaskFloat, llm_diag_mask_inf_float);
PartialImplementKernel(RopeFloat, llm_rope_compute_float);
PartialImplementKernel(GlmRopeFloat, llm_glm_rope_compute_float);
PartialImplementKernel(ScaleDiagMaskFloat, llm_scale_diag_mask_inf_float);
PartialImplementKernel(GlmGmask, llm_glm_gmask_inf_float);
PartialImplementKernel(PermuteFloat, llm_permute_compute_float);

//! multi query attention
PartialImplementKernel(
        MatmulWithHeadStrideQBroadCastKFloat,
        llm_matmul_compute_with_head_strideq_broadcastk_float);
PartialImplementKernel(
        HeadBatchedMatmulBroadCastVFloat, llm_head_batched_matmul_broadcastv_float);

PartialImplementSpace(MatmulInt4Float, llm_matmul_get_workspace_float);
PartialImplementSpace(MatmulFloatFloat, llm_matmul_get_workspace_float_float);

NOImplementKernel(MatmulInt4FloatPacked);
NOImplementKernel(MatmulInt4WeightReorder);
NOImplementKernel(MatmulInt8Float);
NOImplementKernel(EmbeddingGetInt8Float);
</code></pre>
<p>这些注册语句将内核 ID 与具体的实现函数关联起来，使得框架可以在运行时根据内核 ID 选择合适的实现。</p>
<h3 id="83-内核调用机制">8.3 内核调用机制</h3>
<p>内核调用是通过 <code>Comp</code> 和 <code>Space</code> 模板类实现的：</p>
<pre><code class="language-cpp">template &lt;KernelID Id, typename... Args&gt;
struct Comp {
    static void exec(Args... args, cudaHandle* handle);
};

template &lt;KernelID Id, typename... Args&gt;
struct Space {
    static size_t get(Args... args);
};
</code></pre>
<p>这些模板类提供了统一的接口，而具体实现由模板特化提供。在运行时，框架可以根据内核 ID 选择合适的实现：</p>
<pre><code class="language-cpp">// 调用示例
gpu::Comp&lt;KernelID::MatmulFloatFloat&gt;::exec(
        dst, src0, bias, src1, M, N, K, workspace, size, handle);
</code></pre>
<h2 id="9-cuda-资源管理">9. CUDA 资源管理</h2>
<h3 id="91-cudahandle-结构体">9.1 cudaHandle 结构体</h3>
<pre><code class="language-cpp">struct cudaHandle {
    cudaStream_t stream{nullptr};
    cublasHandle_t cublas_handle{nullptr};
};
</code></pre>
<p><code>cudaHandle</code> 结构体管理 CUDA 资源，包括：</p>
<ol>
<li><code>stream</code>：CUDA 流，用于异步执行 CUDA 操作</li>
<li><code>cublas_handle</code>：cuBLAS 句柄，用于调用 cuBLAS 库函数</li>
</ol>
<p>这种设计使得框架可以在多个 CUDA 设备和多个 CUDA 流上并行执行计算，提高计算效率。</p>
<h3 id="92-cuda-流使用">9.2 CUDA 流使用</h3>
<p>代码中使用 CUDA 流执行异步操作，减少 CPU 和 GPU 之间的同步开销：</p>
<pre><code class="language-cpp">cudaStream_t stream = handle-&gt;stream;
llm_embedding_get_int4_float_gpu&lt;&lt;&lt;grid, DequantizedBlockSize, 0, stream&gt;&gt;&gt;(
        weights, index, dst, len_seq, embd, weight_stride);
</code></pre>
<h3 id="93-cublas-库使用">9.3 cuBLAS 库使用</h3>
<p>代码中使用 cuBLAS 库优化矩阵乘法和批处理矩阵乘法：</p>
<pre><code class="language-cpp">cublasHandle_t cublas_handle = handle-&gt;cublas_handle;
CUBLAS_CHECK(cublasSetStream(cublas_handle, stream));
CUBLAS_CHECK(cublasSgemm(
        cublas_handle, CUBLAS_OP_T, CUBLAS_OP_N, N, M, K,
        &amp;alpha, src0, K, src1, K, &amp;beta, dst, N));
</code></pre>
<h2 id="10-条件编译">10. 条件编译</h2>
<p>代码使用条件编译确保 GPU 代码只在启用 GPU 支持时编译：</p>
<pre><code class="language-cpp">#if ENABLE_GPU
// GPU 代码
#endif
</code></pre>
<p>这种设计使得框架可以在不支持 GPU 的平台上编译和运行，只使用 CPU 实现。</p>
<h2 id="11-错误处理">11. 错误处理</h2>
<p>代码中使用宏和断言处理错误：</p>
<pre><code class="language-cpp">#define CUBLAS_CHECK(func)                                                               \
    do {                                                                                 \
        cublasStatus_t status = (func);                                                  \
        if (status != CUBLAS_STATUS_SUCCESS) {                                           \
            printf(&quot;CUBLAS API failed at line %d with error: %s (%d)\n&quot;, __LINE__,       \
                   _cudaGetErrorEnum(status), status);                                   \
            return;                                                                      \
        }                                                                                \
    } while (0)

#define CUDA_CHECK(func)                                                              \
    do {                                                                              \
        cudaError_t status = (func);                                                  \
        if (status != cudaSuccess) {                                                  \
            printf(&quot;CUDA API failed at line %d with error: %s (%d)\n&quot;, __LINE__,      \
                   cudaGetErrorString(status), status);                               \
            return;                                                                   \
        }                                                                             \
    } while (0)
</code></pre>
<p>这些宏检查 CUDA 和 cuBLAS 函数的返回值，如果发生错误，打印错误信息并返回。</p>
<h2 id="12-与其他模块的集成">12. 与其他模块的集成</h2>
<p>GPU 内核模块与其他模块的集成主要通过以下方式：</p>
<h3 id="121-与-kernelh-的集成">12.1 与 kernel.h 的集成</h3>
<p>kernel.h 定义了内核系统的接口，GPU 内核模块实现了这些接口，提供了 GPU 上的计算实现。</p>
<h3 id="122-与-tensorh-的集成">12.2 与 tensor.h 的集成</h3>
<p>tensor.h 定义了张量数据结构，GPU 内核模块使用这些张量作为输入和输出。</p>
<h3 id="123-与-naive-模块的集成">12.3 与 naive 模块的集成</h3>
<p>当 GPU 实现不可用时，系统会回退到 naive 模块的 CPU 实现。</p>
<h2 id="13-性能优化总结">13. 性能优化总结</h2>
<p>GPU 内核模块使用了多种性能优化技术：</p>
<h3 id="131-并行计算">13.1 并行计算</h3>
<ul>
<li>使用 CUDA 内核函数和数千个线程并行计算</li>
<li>使用 cuBLAS 库优化矩阵乘法和批处理矩阵乘法</li>
</ul>
<h3 id="132-内存优化">13.2 内存优化</h3>
<ul>
<li>使用合适的内存访问模式，减少内存访问开销</li>
<li>使用共享内存和寄存器优化内存访问</li>
</ul>
<h3 id="133-算法优化">13.3 算法优化</h3>
<ul>
<li>使用 warp-level 归约优化归一化和 Softmax 等操作</li>
<li>使用批处理矩阵乘法优化注意力计算</li>
</ul>
<h3 id="134-资源管理">13.4 资源管理</h3>
<ul>
<li>使用 CUDA 流执行异步操作，减少 CPU 和 GPU 之间的同步开销</li>
<li>使用 cuBLAS 库优化矩阵乘法和批处理矩阵乘法</li>
</ul>
<h2 id="14-未来优化方向">14. 未来优化方向</h2>
<p>基于当前实现，可以考虑以下优化方向：</p>
<h3 id="141-实现未实现的功能">14.1 实现未实现的功能</h3>
<p>实现当前标记为 <code>NOImplementKernel</code> 的功能：</p>
<ul>
<li><code>MatmulInt4FloatPacked</code></li>
<li><code>MatmulInt4WeightReorder</code></li>
<li><code>MatmulInt8Float</code></li>
<li><code>EmbeddingGetInt8Float</code></li>
</ul>
<h3 id="142-使用更高效的-gpu-算法">14.2 使用更高效的 GPU 算法</h3>
<ul>
<li>使用 Tensor Core 加速矩阵乘法</li>
<li>使用 Flash Attention 算法优化注意力计算</li>
<li>使用混合精度计算提高性能</li>
</ul>
<h3 id="143-优化内存使用">14.3 优化内存使用</h3>
<ul>
<li>使用内存池减少内存分配和释放的开销</li>
<li>使用流水线并行减少内存占用</li>
<li>使用张量并行和模型并行处理大型模型</li>
</ul>
<h3 id="144-支持更多-gpu-平台">14.4 支持更多 GPU 平台</h3>
<ul>
<li>支持 AMD GPU（使用 HIP）</li>
<li>支持移动 GPU（使用 OpenCL 或 Vulkan）</li>
<li>支持多 GPU 并行计算</li>
</ul>
<h2 id="总结">总结</h2>
<p>InferLLM 框架中的 GPU 内核实现提供了大语言模型在 GPU 上运行所需的各种计算操作的实现。通过使用 CUDA 内核函数、warp-level 归约、cuBLAS 库等技术，GPU 内核模块在 GPU 上实现了高效的计算。内核注册与调用机制使得框架可以在运行时根据内核 ID 选择合适的实现，而条件编译确保 GPU 代码只在启用 GPU 支持时编译。</p>
<p>虽然当前实现已经覆盖了大部分功能，但仍有一些功能未实现，如 <code>MatmulInt4FloatPacked</code>、<code>MatmulInt4WeightReorder</code> 等。未来可以考虑实现这些功能，并使用更高效的 GPU 算法、优化内存使用、支持更多 GPU 平台等方向进行优化。</p>

            </div>
            <hr style="width: 80%;">
            
            <div class="donateContainer">
                <span class="donatebtn">赏</span>
            </div>
            <div class="donateContainer">
                <img src="https://i.postimg.cc/XYDh2Nx5/donate.png" alt="" class="donate">
            </div>
            
            <div class="postfooter">
                <ul class="post-copyright">
                    <li><strong>文章作者：</strong>sola</li>
            <li><strong>原文链接：</strong>https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-23/</li>
            <li><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用<i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA
                许可协议。转载请注明出处！</li>
            </ul>
        </div>
        
            <div class="shareContainer">
                <div class="share" style="height:30px;">
                    <a class="qq" href=""><i><img class="shareicon"
                                src="https://solahome.github.io/media/images/qqs.png" alt=""></i></a>
                    <a class="wechat" href=""><i><img class="shareicon"
                                src="https://solahome.github.io/media/images/wechats.png" alt=""></i></a>
                    <a class="weibo" href=""><i><img class="shareicon"
                                src="https://solahome.github.io/media/images/weibos.png" alt=""></i></a>
                    <a class="qzone" href=""><i><img class="shareicon"
                                src="https://solahome.github.io/media/images/qzones.png" alt=""></i></a>
                    <a class="douban" href=""><i><img class="shareicon"
                                src="https://solahome.github.io/media/images/doubans.png" alt=""></i></a>
                    <a class="facebook" href=""><i><img class="shareicon"
                                src="https://solahome.github.io/media/images/facebooks.png" alt=""></i></a>
                    <a class="twitter" href=""><i><img class="shareicon"
                                src="https://solahome.github.io/media/images/twitter.png" alt=""></i></a>
                </div>
            </div>
            
        <div id="otherpost">
            
            <a  data-pjax class="prev-post-pc" href="https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-24/"  style="color: black;">
                <div id="ppcontainer">
                    
                    <img src="https://photo.colorhub.me/I86H-w3Q8UE/rs:auto:0:500:0/g:ce/fn:colorhub/bG9jYWw6Ly8vNjAvZTAvNWMwMmM4ODc2M2M4ZTkwZTg2YmJiMGQ4ODQ3MGVhZWI1NmIxNjBlMC5qcGVn.webp" style="width: 100%;" class="opimg">
                    
                    <h4 class="optitle">[InferLLM大模型推理框架项目](24)kern中optimized模块整体分析(src/kern/optimized)</h4>
                </div>
            </a>
            <div class="prev-post">
                上一篇 <a  data-pjax href="https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-24/">[InferLLM大模型推理框架项目](24)kern中optimized模块整体分析(src/kern/optimized)</a>
            </div>
            
            
            <a data-pjax class="next-post-pc" href="https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-22/"  style="color: black;">
                <div id="npcontainer">
                    
                    <img src="https://img.colorhub.me/nwrFG5pja5I/rs:auto:0:500:0/g:ce/fn:colorhub/bG9jYWw6Ly8vMDEvYTYvZjFlYTRhYmY3YjM5OGRlMzQ1MmZiOWIwY2UzODI4OGQzZjczMDFhNi5qcGVn.webp" style="width: 100%;" class="opimg">
                    
                    <h4 class="optitle">[InferLLM大模型推理框架项目](22)kern模块中量化的实现(src/kern/naive/quantize.h)</h4>
                </div>
            </a>
            <div class="next-post">
                下一篇 <a  data-pjax href="https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-22/">[InferLLM大模型推理框架项目](22)kern模块中量化的实现(src/kern/naive/quantize.h)</a>
            </div>
            
        </div>
    </div>
    </div>
    </div>
    <!-- 弹窗 -->
    <div id="myModal" class="modal">
        <img class="modal-content" id="img01">
    </div>
</body>
<script>
    //文章阅读热度
    var pl = $("#pl").html();
    var rootaddr = $("#rootaddr").html();
    pl = pl.replace(rootaddr, "");
    $("#hotnum").attr('id', pl);
</script>
                    <div class="toc-container">
                        <ul class="markdownIt-TOC">
<li><a href="#inferllm-%E6%A1%86%E6%9E%B6%E4%B8%AD-gpu-%E5%86%85%E6%A0%B8%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90">InferLLM 框架中 GPU 内核实现分析</a>
<ul>
<li><a href="#1-%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84">1. 整体架构</a>
<ul>
<li><a href="#11-%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84">1.1 文件结构</a></li>
<li><a href="#12-%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6">1.2 核心组件</a></li>
</ul>
</li>
<li><a href="#2-%E4%B8%BB%E8%A6%81%E5%8A%9F%E8%83%BD%E6%A8%A1%E5%9D%97%E5%88%86%E6%9E%90">2. 主要功能模块分析</a>
<ul>
<li><a href="#21-%E5%B5%8C%E5%85%A5%E6%9F%A5%E6%89%BEembedding-lookup">2.1 嵌入查找（Embedding Lookup）</a></li>
<li><a href="#22-%E5%85%83%E7%B4%A0%E7%BA%A7%E6%93%8D%E4%BD%9Celementwise-operations">2.2 元素级操作（Elementwise Operations）</a></li>
<li><a href="#23-%E5%BD%92%E4%B8%80%E5%8C%96normalization">2.3 归一化（Normalization）</a></li>
<li><a href="#24-softmax">2.4 Softmax</a></li>
<li><a href="#25-%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95matrix-multiplication">2.5 矩阵乘法（Matrix Multiplication）</a></li>
<li><a href="#26-%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%AE%A1%E7%AE%97attention-computation">2.6 注意力计算（Attention Computation）</a></li>
</ul>
</li>
<li><a href="#27-%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81position-encoding">2.7 位置编码（Position Encoding）</a>
<ul>
<li><a href="#28-%E6%8E%A9%E7%A0%81%E6%93%8D%E4%BD%9Cmasking-operations">2.8 掩码操作（Masking Operations）</a></li>
</ul>
</li>
<li><a href="#3-%E5%86%85%E6%A0%B8%E6%B3%A8%E5%86%8C%E6%9C%BA%E5%88%B6">3. 内核注册机制</a></li>
<li><a href="#4-cuda-%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF">4. CUDA 优化技术</a>
<ul>
<li><a href="#41-%E7%BA%BF%E7%A8%8B%E5%9D%97%E5%92%8C%E7%BD%91%E6%A0%BC%E9%85%8D%E7%BD%AE">4.1 线程块和网格配置</a></li>
<li><a href="#42-warp-level-%E5%BD%92%E7%BA%A6">4.2 Warp-level 归约</a></li>
<li><a href="#43-%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE%E4%BC%98%E5%8C%96">4.3 内存访问优化</a></li>
<li><a href="#44-%E4%BD%BF%E7%94%A8-cublas-%E5%BA%93">4.4 使用 cuBLAS 库</a></li>
</ul>
</li>
<li><a href="#5-%E4%B8%8E-cpu-%E5%AE%9E%E7%8E%B0%E7%9A%84%E6%AF%94%E8%BE%83">5. 与 CPU 实现的比较</a>
<ul>
<li><a href="#51-%E5%B9%B6%E8%A1%8C%E5%BA%A6">5.1 并行度</a></li>
<li><a href="#52-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86">5.2 内存管理</a></li>
<li><a href="#53-%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95">5.3 优化方法</a></li>
<li><a href="#54-%E5%8A%9F%E8%83%BD%E8%A6%86%E7%9B%96">5.4 功能覆盖</a></li>
</ul>
</li>
<li><a href="#6-%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91">6. 性能考虑</a>
<ul>
<li><a href="#61-%E5%86%85%E5%AD%98%E4%BC%A0%E8%BE%93%E5%BC%80%E9%94%80">6.1 内存传输开销</a></li>
<li><a href="#62-%E5%86%85%E6%A0%B8%E5%90%AF%E5%8A%A8%E5%BC%80%E9%94%80">6.2 内核启动开销</a></li>
<li><a href="#63-%E7%BA%BF%E7%A8%8B%E5%88%86%E9%85%8D%E5%92%8C%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1">6.3 线程分配和负载均衡</a></li>
<li><a href="#64-%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%BC%8F">6.4 内存访问模式</a></li>
</ul>
</li>
<li><a href="#7-%E6%9C%AA%E5%AE%9E%E7%8E%B0%E7%9A%84%E5%8A%9F%E8%83%BD">7. 未实现的功能</a></li>
<li><a href="#8-%E5%86%85%E6%A0%B8%E6%B3%A8%E5%86%8C%E4%B8%8E%E8%B0%83%E7%94%A8%E6%9C%BA%E5%88%B6">8. 内核注册与调用机制</a>
<ul>
<li><a href="#81-%E5%86%85%E6%A0%B8%E6%B3%A8%E5%86%8C%E5%AE%8F">8.1 内核注册宏</a></li>
<li><a href="#82-%E5%86%85%E6%A0%B8%E6%B3%A8%E5%86%8C%E5%88%97%E8%A1%A8">8.2 内核注册列表</a></li>
<li><a href="#83-%E5%86%85%E6%A0%B8%E8%B0%83%E7%94%A8%E6%9C%BA%E5%88%B6">8.3 内核调用机制</a></li>
</ul>
</li>
<li><a href="#9-cuda-%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86">9. CUDA 资源管理</a>
<ul>
<li><a href="#91-cudahandle-%E7%BB%93%E6%9E%84%E4%BD%93">9.1 cudaHandle 结构体</a></li>
<li><a href="#92-cuda-%E6%B5%81%E4%BD%BF%E7%94%A8">9.2 CUDA 流使用</a></li>
<li><a href="#93-cublas-%E5%BA%93%E4%BD%BF%E7%94%A8">9.3 cuBLAS 库使用</a></li>
</ul>
</li>
<li><a href="#10-%E6%9D%A1%E4%BB%B6%E7%BC%96%E8%AF%91">10. 条件编译</a></li>
<li><a href="#11-%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86">11. 错误处理</a></li>
<li><a href="#12-%E4%B8%8E%E5%85%B6%E4%BB%96%E6%A8%A1%E5%9D%97%E7%9A%84%E9%9B%86%E6%88%90">12. 与其他模块的集成</a>
<ul>
<li><a href="#121-%E4%B8%8E-kernelh-%E7%9A%84%E9%9B%86%E6%88%90">12.1 与 kernel.h 的集成</a></li>
<li><a href="#122-%E4%B8%8E-tensorh-%E7%9A%84%E9%9B%86%E6%88%90">12.2 与 tensor.h 的集成</a></li>
<li><a href="#123-%E4%B8%8E-naive-%E6%A8%A1%E5%9D%97%E7%9A%84%E9%9B%86%E6%88%90">12.3 与 naive 模块的集成</a></li>
</ul>
</li>
<li><a href="#13-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93">13. 性能优化总结</a>
<ul>
<li><a href="#131-%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97">13.1 并行计算</a></li>
<li><a href="#132-%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96">13.2 内存优化</a></li>
<li><a href="#133-%E7%AE%97%E6%B3%95%E4%BC%98%E5%8C%96">13.3 算法优化</a></li>
<li><a href="#134-%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86">13.4 资源管理</a></li>
</ul>
</li>
<li><a href="#14-%E6%9C%AA%E6%9D%A5%E4%BC%98%E5%8C%96%E6%96%B9%E5%90%91">14. 未来优化方向</a>
<ul>
<li><a href="#141-%E5%AE%9E%E7%8E%B0%E6%9C%AA%E5%AE%9E%E7%8E%B0%E7%9A%84%E5%8A%9F%E8%83%BD">14.1 实现未实现的功能</a></li>
<li><a href="#142-%E4%BD%BF%E7%94%A8%E6%9B%B4%E9%AB%98%E6%95%88%E7%9A%84-gpu-%E7%AE%97%E6%B3%95">14.2 使用更高效的 GPU 算法</a></li>
<li><a href="#143-%E4%BC%98%E5%8C%96%E5%86%85%E5%AD%98%E4%BD%BF%E7%94%A8">14.3 优化内存使用</a></li>
<li><a href="#144-%E6%94%AF%E6%8C%81%E6%9B%B4%E5%A4%9A-gpu-%E5%B9%B3%E5%8F%B0">14.4 支持更多 GPU 平台</a></li>
</ul>
</li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
</ul>
</li>
</ul>

                    </div>
                    <div class="tog">
                        <div class="toggleContainer">
                            <div class="toggle">
                                <i class="fas fa-angle-double-up"></i>
                            </div>
                        </div>
                    </div>
                    <!---------------------------- 评论系统👇 ------------------------->
                    <div id="comment" class="comment">
                        <div class="commentcontainer">
                            
                                
                            
                        </div>
                    </div>
                    <!---------------------------- 评论系统👆 ------------------------->
                </div>
                <!-- main标签，pjax渲染时加载以上内容👆 -->
                <div class="col-md-1 col-lg-2"></div>
                <div id="bg">
                </div>
            </div>
            <!-- ----------------左侧菜单栏html 👆-------------------------------------->
            
        </div>
        <!-- aplayer播放器加载👇 -->
            
            <head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css">
</head>

<body>
    <div class="miniMode">
        <meting-js id="myaplayer" server="netease" type="playlist" fixed="true" order="default" muted="muted">
        </meting-js>
    </div>
</body>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js"></script>
<script>
    function aplayerInit() {
        var aplayerid = "486899256";
        var aplayerserver = "netease";
        var aplayerorder = "random";
        var aplayerautoplay = "false";
        var aplayer = $("#myaplayer");
        aplayer.attr("server", aplayerserver);
        aplayer.attr("id", aplayerid);
        aplayer.attr("order", aplayerorder);
        if (aplayerautoplay == "true")
            aplayer.attr("autoplay", "true");
    }
    aplayerInit();
</script>

<script src="https://cdn.jsdelivr.net/gh/850552586/ericamcdn@0.1/js/Aplayer.min.js"></script>
<!-- require MetingJS -->
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

<script>
    function aplayerbtnInit() {
        var player = document.querySelectorAll('meting-js')[0].aplayer;
        if(player==undefined)return false;
        player.lrc.hide();
        var mbabtn = $("#mbaplayer");
        if (player.paused == false) {
            mbabtn.removeClass("fa-play-circle-o");
            mbabtn.addClass("fa-pause-circle");
        } else {
            mbabtn.addClass("fa-play-circle-o");
            mbabtn.removeClass("fa-pause-circle");
        }
    }

    function aplayerPlay() {
        var player = document.querySelectorAll('meting-js')[0].aplayer;
        var mbabtn = $("#mbaplayer");
        if (player.paused == true) {
            player.play();
            mbabtn.removeClass("fa-play-circle-o");
            mbabtn.addClass("fa-pause-circle");
        } else {
            player.pause();
            mbabtn.addClass("fa-play-circle-o");
            mbabtn.removeClass("fa-pause-circle");
        }
    }
</script>
            
        <!-- aplayer播放器加载👆 -->
    </div>
    <!-- 背景图加载方式👇 -->
    <div id="bgchoice" style="display: none">link
                </div>
                
                <div id="bgurl" style="display:none">https://cdn.colorhub.me/sU3CVeqJayE/rs:auto:0:500:0/g:ce/fn:colorhub/bG9jYWw6Ly8vZDgvZTQvNTlkMmI4MTNjYmU5ZWE2ZDRlNDE1OGQ5YjdkMzMxZjBmZDEwZDhlNC5qcGc.webp</div>
                
    <!-- 背景图加载方式👆 -->
</body>

<script>
    //初始化
    //更换twikoo头像
    setTimeout(replaceAvatar,1000);
    setTimeout(replaceAvatar,2000);
    setTimeout(replaceAvatar,8000);
    // --------为每张图片添加懒加载（lazyload）标签-->懒加载功能 👇------------
    lazyload();
    //为文章内所有代码块添加代码复制功能 👇
    codeinit();
    // ------代码高亮显示功能-------------
    hljs.initHighlightingOnLoad();
    if (shareChoice)
        shareInit();
    if (donateChoice)
        donateInit();
</script>

<!------------------ 加载live2d模型功能（看板娘）👇 ---------------->

<script type="text/javascript">
    var message_Path = '/live2d/'
    var home_Path = document.getElementById("domainname").innerHTML + "/"; //此处修改为你的域名，必须带斜杠
</script>
<script type="text/javascript" src="https://solahome.github.io/media/js/live2d.js"></script>
<script type="text/javascript" src="https://solahome.github.io/media/live2d/js/message.js"></script>
<script type="text/javascript">
    loadlive2d("live2d", "https://solahome.github.io/media/live2d/assets/haru01.model.json");
</script>

<!------------------ 加载live2d模型功能（看板娘）👆 ---------------->


<!----------------------- 返回顶部按钮功能👇 --------------------->
<script>
    $(function () {
        $('.toggleContainer').click(function () {
            $('html,body').animate({
                scrollTop: '0px'
            }, 800);
        });
        $(window).scroll(function () {
            var st = $(window).scrollTop();
            if (st > 30) {
                $(".toggleContainer").fadeIn(400);
            } else {
                $(".toggleContainer").fadeOut(100);
            }
        });
    });
</script>
<!----------------------- 返回顶部按钮功能👆 --------------------->


<!------------ 加载全局背景图功能👇（开始时没有写好，这功能有点鸡肋/(ㄒoㄒ)/~~） -->
<script>
    var bgchoice = "link"
    var bg = $('#bg');
    var bgurl = document.getElementById("bgurl").innerHTML;
    if (bgchoice == 'default')
    {
        for (var i = 0; i < 3; i++)
            bgurl = bgurl.replace("\\", "/");
        bgurl = "https://solahome.github.io/" + bgurl;
    }
    bg.css("background", "url('" + bgurl + "')");

    window.onload = function () {
        aplayerbtnInit();
    }
</script>
<!------------ 加载全局背景图功能👆（开始时没有写好，这功能有点鸡肋/(ㄒoㄒ)/~~） -->




<script src="https://solahome.github.io/media/js/index.js"></script>


<script>
    //加载后的事件同步（阅读量、移动端评论等
    window.onload = function () {
    //代码块高亮渲染
    document.querySelectorAll('pre code').forEach((block) => {
            hljs.highlightBlock(block);
    });
        getHotnum();
        var sharepc = $(".share").eq(0).html();
        $(".share").eq(1).html(sharepc);   //插件只会寻找一个分享标签渲染，所以需要手动添加到移动端
        aplayerbtnInit();
        $(".hljs").css({
            "padding": "20px"
        })
    }
</script>

<!-- <script src="https://solahome.github.io/media/js/av-min.js"></script> -->
<!-- <script src="https://cdn.jsdelivr.net/npm/leancloud-storage@3.13.0/dist/av-min.js"></script> -->
<!-- <script src="https://cdn.jsdelivr.net/npm/leancloud-storage@4.6.1/dist/av-live-query-min.js"></script> -->