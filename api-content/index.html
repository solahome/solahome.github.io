{"posts":[{"title":"[InferLLM大模型推理框架项目](27)kern中ARM优化模块的optimized代码分析(src/kern/optimized/arm/optimized.h)","content":"InferLLM 框架中 ARM 优化模块的 optimized.h 分析 optimized.h 文件是 InferLLM 框架中 ARM 优化模块的核心部分，它实现了各种基础向量运算函数，这些函数被 kernel.cpp 中的高级计算函数调用。下面对其代码结构和功能实现进行详细分析。 1. 文件结构概述 该文件包含了必要的头文件，其中 arm_neon.h 是 ARM NEON 指令集的头文件，提供了 NEON 指令集的内联函数。所有函数都定义在 inferllm::opt 命名空间中。 2. 基础元素级运算函数 2.1 向量加法 这个函数实现了向量加法，将两个向量 x 和 y 相加，结果存储在向量 z 中。使用 __restrict 关键字告诉编译器这些指针不会重叠，有助于编译器生成更优化的代码。 2.2 向量乘法 这个函数实现了向量乘法，将两个向量 x 和 y 对应元素相乘，结果存储在向量 z 中。 2.3 SiLU 激活函数 这个函数实现了 SiLU 激活函数，也称为 Swish 激活函数，公式为 f(x) = x * sigmoid(x)，这里使用等价形式 f(x) = x / (1 + exp(-x))。 2.4 GELU 激活函数 这个函数实现了 GELU 激活函数，使用近似公式 f(x) = 0.5 * x * (1 + tanh(sqrt(2/π) * (x + 0.044715 * x^3)))，其中 PGELU 是常数 0.044715。 2.5 向量缩放 这个函数实现了向量缩放，将向量 x 的每个元素乘以缩放因子 scale，结果存储在向量 z 中。 3. 归约运算函数 3.1 平方和归约 这个函数计算向量 x 的平方和，用于 RMS 归一化。 3.2 最大值归约 这个函数找到向量 x 的最大值，用于 Softmax 计算。 3.3 减去最大值并计算指数和 这个函数将向量 x 的每个元素减去最大值 max，然后计算指数，结果存储在向量 y 中，并返回指数和。这是 Softmax 计算的一部分，减去最大值可以提高数值稳定性。 4. 矩阵乘法函数 4.1 带偏移的矩阵乘法 这个函数实现了带偏移的矩阵乘法，用于多头注意力中的 Q 和 K 的矩阵乘法。它使用分块处理策略，每次处理 4 列，提高计算效率。 4.2 带不连续目标的矩阵乘法 这个函数实现了带不连续目标的矩阵乘法，用于多头注意力中的 QK 和 V 的矩阵乘法。目标矩阵 dst 的元素不是连续存储的，而是按照 offset_dst 的偏移存储。 5. 量化点积计算 这个函数实现了 4 位整数量化与 8 位整数量化的点积计算，是矩阵乘法中的核心计算部分。它使用 NEON 指令集进行向量化计算，并根据处理器是否支持 SDOT 指令选择不同的实现。 主要优化点包括： 条件编译：根据 __ARM_FEATURE_DOTPROD 宏判断处理器是否支持 SDOT 指令，选择不同的实现 NEON 指令集优化：使用 NEON 指令集进行向量化计算，每次处理多个数据元素 分块处理：每次处理两个块，减少循环开销 并行计算：使用多个向量寄存器并行计算，提高指令级并行性 内存访问优化：使用连续的内存访问模式，提高缓存命中率 6. NEON 指令集优化分析 ARM 优化模块使用 NEON 指令集进行向量化计算，提高计算效率。NEON 指令集是 ARM 处理器的 SIMD（单指令多数据）扩展，可以同时处理多个数据元素。 6.1 NEON 指令集的基本操作 数据加载：使用 vld1q_u8、vld1q_s8 等指令加载数据到向量寄存器 数据存储：使用 vst1q_f32 等指令将向量寄存器中的数据存储到内存 算术运算：使用 vaddq_f32、vmulq_f32 等指令进行向量加法、乘法等运算 位运算：使用 vandq_u8、vshrq_n_u8 等指令进行位运算 类型转换：使用 vreinterpretq_s8_u8、vcvtq_f32_s32 等指令进行类型转换 归约运算：使用 vaddvq_f32 等指令进行水平求和 6.2 NEON 指令集在 optimized.h 中的应用 向量加法：可以使用 vaddq_f32 指令同时处理 4 个浮点数 向量乘法：可以使用 vmulq_f32 指令同时处理 4 个浮点数 点积计算：可以使用 vdotq_s32 指令（如果支持）或模拟实现计算点积 归约运算：可以使用 vaddvq_f32 指令计算向量的水平求和 6.3 NEON 指令集的优化效果 使用 NEON 指令集可以显著提高计算效率，特别是对于大型矩阵乘法和向量运算。在支持 SDOT 指令的处理器上，点积计算的性能更高。 7. 优化策略分析 7.1 向量化计算 ARM 优化模块使用 NEON 指令集进行向量化计算，每次处理多个数据元素，提高计算效率。例如，在点积计算中，每次处理 32 个 4 位整数和 32 个 8 位整数。 7.2 分块处理 ARM 优化模块使用分块处理策略，将数据分成多个块进行处理，减少循环开销。例如，在矩阵乘法中，每次处理 4 列，在点积计算中，每次处理两个块。 7.3 条件编译 ARM 优化模块使用条件编译，根据处理器支持的指令集选择不同的实现。例如，在点积计算中，根据处理器是否支持 SDOT 指令选择不同的实现。 7.4 内存访问优化 ARM 优化模块优化了内存访问模式，使用连续的内存访问模式，提高缓存命中率。例如，在矩阵乘法中，一次加载输入数据，然后计算多个输出元素。 8. 与 naive 实现的比较 ARM 优化模块中的函数与 naive 模块中的函数相比，主要区别在于： 向量化实现：ARM 优化模块使用 NEON 指令集进行向量化计算，naive 模块使用标量实现 分块处理：ARM 优化模块使用分块处理策略，naive 模块使用简单的循环 条件编译：ARM 优化模块使用条件编译，根据处理器支持的指令集选择不同的实现，naive 模块没有这种优化 内存访问优化：ARM 优化模块优化了内存访问模式，naive 模块没有这种优化 在实际应用中，ARM 优化模块的性能明显优于 naive 模块，特别是在支持 NEON 指令集的 ARM 处理器上。 9. 未来优化方向 基于当前实现，可以考虑以下优化方向： 9.1 更多 NEON 指令集优化 使用 NEON 指令集优化基础向量运算函数：目前 elemwise_vector_add、elemwise_vector_mul 等函数没有使用 NEON 指令集优化，可以添加 NEON 实现 使用 NEON 指令集优化激活函数：目前 elemwise_vector_silu、elemwise_vector_gelu 等函数没有使用 NEON 指令集优化，可以添加 NEON 实现 使用 NEON 指令集优化归约运算：目前 reduce_square_sum、reduce_max 等函数没有使用 NEON 指令集优化，可以添加 NEON 实现 9.2 更多 ARM 指令集支持 支持 ARMv8.2-A 的 FP16 指令：使用半精度浮点数进行计算，减少内存占用和计算量 支持 ARMv8.2-A 的 DotProd 指令：加速点积计算，提高矩阵乘法性能 支持 ARMv8.6-A 的 BFloat16 指令：使用 BFloat16 进行计算，减少内存占用和计算量 9.3 更高效的算法 使用 Winograd 算法优化矩阵乘法：减少乘法次数，提高计算效率 使用 Flash Attention 算法优化注意力计算：减少内存占用和计算量 使用混合精度计算提高性能：在不同的计算阶段使用不同的精度，平衡精度和性能 总结 InferLLM 框架中的 ARM 优化模块通过 optimized.h 文件实现了各种基础向量运算函数，这些函数被 kernel.cpp 中的高级计算函数调用。optimized.h 文件中的函数使用 NEON 指令集进行向量化计算，使用分块处理策略和条件编译等优化技术，提高大语言模型推理的性能。 与 naive 模块相比，ARM 优化模块的性能明显更高，特别是在支持 NEON 指令集的 ARM 处理器上。未来可以考虑添加更多 NEON 指令集优化、支持更多 ARM 指令集和使用更高效的算法等方向进行优化，进一步提高大语言模型在 ARM 平台上的推理性能。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-27/"},{"title":"[InferLLM大模型推理框架项目](26)kern中ARM优化模块的kernel代码分析(src/kern/optimized/arm/kernel_gpu.h+.cpp)","content":"InferLLM 框架中 ARM 优化模块的 kernel.h 和 kernel.cpp 分析 InferLLM 框架中的 ARM 优化模块主要通过 kernel.h 和 kernel.cpp 文件实现了针对 ARM 架构的优化计算函数。这两个文件共同构成了 ARM 平台上大语言模型推理的核心计算部分。 1. kernel.h 文件分析 kernel.h 文件主要声明了各种计算函数和注册内核的宏。 1.1 头文件包含 这里包含了必要的头文件，其中 &quot;kern/naive/naive.h&quot; 包含了朴素实现的函数，用于在 ARM 优化不可用时作为回退方案。 1.2 函数声明 kernel.h 文件声明了一系列计算函数，这些函数都返回 TaskSet 类型，用于多线程并行计算： 1.3 内核注册 kernel.h 文件使用 PartialImplementKernel 和 PartialImplementSpace 宏注册内核： 这些宏将内核 ID 与具体的实现函数关联起来，使得框架可以在运行时根据内核 ID 选择合适的实现。 2. kernel.cpp 文件分析 kernel.cpp 文件实现了 kernel.h 中声明的各种计算函数。 2.1 头文件包含 这里包含了必要的头文件，其中 &quot;optimized.h&quot; 包含了基础向量运算函数，&quot;quantize.h&quot; 包含了量化和反量化操作。 2.2 嵌入层计算 这个函数实现了嵌入层的计算，将4位整数量化的权重反量化为浮点数。每个任务处理一个或多个序列位置，通过 dequantize_row_q4_0 函数将量化权重反量化为浮点数。 2.3 元素级计算 这个函数实现了元素级计算，支持加法、乘法、SiLU 激活函数和 GELU 激活函数。每个任务处理一段连续的数据，通过 elemwise_vector_add、elemwise_vector_mul、elemwise_vector_silu 和 elemwise_vector_gelu 函数实现具体的计算。 2.4 广播计算 这个函数实现了广播计算，将 src1 广播到 src0 的第0维，然后进行元素级计算。每个任务处理 src0 的一个或多个行，通过 elemwise_vector_add 和 elemwise_vector_mul 函数实现具体的计算。 2.5 RMS 归一化 这个函数实现了 RMS 归一化，每个任务处理一个或多个序列位置。首先计算平方和的均值，然后计算缩放因子，最后将输入向量乘以缩放因子得到归一化结果。 2.6 Softmax 计算 这个函数实现了 Softmax 计算，每个任务处理一个或多个行。首先找到最大值，然后减去最大值并计算指数和，最后将每个元素除以指数和得到 Softmax 结果。 2.7 量化矩阵乘法 这个函数实现了4位整数权重与浮点数激活值的矩阵乘法。它分为两个阶段：第一阶段将输入量化为8位整数，第二阶段计算矩阵乘法。每个阶段都使用多线程并行计算，第一阶段按行分解，第二阶段按列分解。 2.8 打包的量化矩阵乘法 这个函数实现了打包的量化矩阵乘法，针对打包的权重进行了优化，以提高缓存命中率。 2.9 获取矩阵乘法所需的工作空间大小 这个函数计算矩阵乘法所需的工作空间大小，工作空间用于存储量化的输入。 2.10 多头注意力计算 这两个函数实现了多头注意力计算中的矩阵乘法。llm_matmul_compute_with_head_stride_float 计算 Q 和 K 的矩阵乘法，llm_head_batched_matmul_compute_float 计算 QK 和 V 的矩阵乘法。每个任务处理一个注意力头，通过 compute_src_offset_embd_matmul 和 comput_matmul_with_dst_uncontinue 函数实现具体的计算。 3. 内核注册机制分析 InferLLM 框架使用内核注册机制，将函数与内核 ID 关联起来。在 kernel.h 文件中，使用 PartialImplementKernel 和 PartialImplementSpace 宏注册内核： 这些宏的定义可能在其他头文件中，它们的作用是将内核 ID 与具体的实现函数关联起来，使得框架可以在运行时根据内核 ID 选择合适的实现。 与 ImplementKernel 和 ImplementSpace 宏不同，PartialImplementKernel 和 PartialImplementSpace 宏可能表示这些实现是部分实现，可能只支持某些特定的参数组合或者只在某些特定的条件下可用。 4. 多线程并行策略分析 InferLLM 框架使用 TaskSet 实现多线程并行，每个计算函数都返回一个 TaskSet，包含一个或多个任务及其子任务数量： 不同的计算函数使用不同的任务分解策略： 按序列长度分解：如 llm_rms_norm_compute_float，每个任务处理一个或多个序列位置 按头数分解：如 llm_matmul_compute_with_head_stride_float，每个任务处理一个或多个注意力头 按矩阵行列分解：如 llm_matmul_compute_int4_float，使用两个任务集，一个按行分解，一个按列分解 这种设计使得计算任务可以在多线程环境中高效执行，通过将大型计算任务分解为多个子任务，并分配给不同的线程处理。 5. 优化策略分析 5.1 NEON 指令集优化 ARM 优化模块使用 NEON 指令集进行向量化计算，提高计算效率。这些优化主要在 optimized.h 和 quantize.h 文件中实现，kernel.cpp 文件中的函数调用这些优化的基础向量运算函数。 5.2 分块处理策略 ARM 优化模块使用分块处理策略，将数据分成多个块进行处理： 这种策略可以最大化利用 NEON 指令集的并行性，同时处理所有数据。 5.3 量化计算优化 ARM 优化模块使用量化计算减少内存占用和计算量： 这种设计减少了内存占用和内存带宽需求，提高了计算效率。特别是对于大型矩阵乘法，量化计算可以显著提高性能。 5.4 内存访问优化 ARM 优化模块优化了内存访问模式，减少内存访问开销： 这种设计提高了缓存命中率，减少了内存访问开销。通过一次加载输入数据，然后计算多个输出元素，可以减少内存带宽需求。 6. 与 naive 实现的比较 ARM 优化模块与 naive 模块的主要区别： 向量化实现：ARM 优化模块使用 NEON 指令集进行向量化计算，naive 模块使用标量实现 分块处理：ARM 优化模块使用分块处理策略，naive 模块使用简单的循环 量化计算：ARM 优化模块使用量化计算减少内存占用和计算量，naive 模块使用浮点计算 多线程并行：ARM 优化模块使用 TaskSet 实现多线程并行，naive 模块也使用 TaskSet，但任务分解策略不同 在实际应用中，ARM 优化模块的性能明显优于 naive 模块，特别是在支持 NEON 指令集的 ARM 处理器上。 7. 内核注册机制的实现 InferLLM 框架使用 PartialImplementKernel 和 PartialImplementSpace 宏注册内核。这些宏的定义可能如下： 这些宏将内核 ID 与具体的实现函数关联起来，使得框架可以在运行时根据内核 ID 选择合适的实现。与 ImplementKernel 和 ImplementSpace 宏不同，PartialImplementKernel 和 PartialImplementSpace 宏表示这些实现是部分实现，可能只支持某些特定的参数组合或者只在某些特定的条件下可用。 在运行时，框架会首先检查是否有完整实现，如果没有，则检查是否有部分实现，如果都没有，则使用 naive 实现作为回退方案。 8. TaskSet 的实现 TaskSet 是 InferLLM 框架中的一个重要概念，用于多线程并行计算。它的定义可能如下： 每个 TaskSet 包含一个或多个 TaskItem，每个 TaskItem 包含一个任务函数和子任务数量。在运行时，框架会将每个 TaskItem 分解为多个子任务，并分配给不同的线程处理。 9. 未来优化方向 基于当前实现，可以考虑以下优化方向： 9.1 更多 ARM 指令集支持 支持 ARMv8.2-A 的 FP16 指令，使用半精度浮点数进行计算 支持 ARMv8.2-A 的 DotProd 指令，加速点积计算 支持 ARMv8.6-A 的 BFloat16 指令，使用 BFloat16 进行计算 9.2 更高效的算法 使用 Winograd 算法优化矩阵乘法 使用 Flash Attention 算法优化注意力计算 使用混合精度计算提高性能 9.3 更多量化方法 支持 3 位、2 位甚至 1 位量化 支持非对称量化 支持组量化 9.4 更高级的并行策略 使用流水线并行减少内存占用 使用张量并行和模型并行处理大型模型 使用异步计算提高计算效率 总结 InferLLM 框架中的 ARM 优化模块通过 kernel.h 和 kernel.cpp 文件实现了针对 ARM 架构的优化计算函数。这些函数使用 NEON 指令集进行向量化计算，使用分块处理策略和量化计算减少内存占用和计算量，使用 TaskSet 实现多线程并行，提高大语言模型推理的性能。 与 naive 模块相比，ARM 优化模块的性能明显更高，特别是在支持 NEON 指令集的 ARM 处理器上。未来可以考虑支持更多 ARM 指令集、使用更高效的算法、支持更多量化方法和使用更高级的并行策略等方向进行优化，进一步提高大语言模型在 ARM 平台上的推理性能。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-26/"},{"title":"[InferLLM大模型推理框架项目](25)kern中ARM优化模块整体分析(src/kern/optimized/arm)","content":"InferLLM 框架中 ARM 优化模块分析 InferLLM 框架中的 ARM 优化模块是针对 ARM 架构处理器的优化实现，主要利用 ARM NEON 指令集进行向量化计算，提高大语言模型推理的性能。 1. 目录结构 ARM 优化模块包含以下文件： 2. 核心功能模块 2.1 基础向量运算 (optimized.h) optimized.h 文件实现了一系列基础向量运算函数： 2.1.1 元素级操作 2.1.2 激活函数 2.1.3 归约操作 2.1.4 矩阵运算 2.2 量化计算 (quantize.h) quantize.h 文件实现了量化和反量化操作： 2.2.1 4位整数量化 2.2.2 4位整数反量化 2.2.3 量化点积计算 量化点积计算是 ARM 优化模块中的关键优化技术之一，主要用于加速矩阵乘法计算。在 quantize.h 文件中，vec_vec_dot_q40_with_q80 函数实现了 4 位整数量化与 8 位整数量化的点积计算： 这个函数的主要优化点包括： NEON 指令集优化：使用 NEON 指令集进行向量化计算，每次处理多个数据元素 条件编译：根据 ARM 处理器是否支持 SDOT 指令（ARMv8.2-A 及以上），使用不同的实现 分块处理：每次处理两个块，减少循环开销 并行计算：使用多个向量寄存器并行计算，提高指令级并行性 内存访问优化：使用连续的内存访问模式，提高缓存命中率 这个函数是矩阵乘法中的核心计算部分，通过优化点积计算，可以显著提高矩阵乘法的性能。在 llm_matmul_compute_int4_float 函数中，使用这个函数计算矩阵乘法： 通过使用量化点积计算，可以减少内存占用和内存带宽需求，提高计算效率，特别是对于大型矩阵乘法，这种优化可以显著提高性能。 与其他平台的量化点积计算比较 与 x86 平台相比，ARM 平台的量化点积计算有以下特点： 指令集差异：ARM 使用 NEON 和 SDOT 指令，x86 使用 SSE/AVX/AVX2 和 VNNI 指令 向量宽度：ARM NEON 的向量宽度为 128 位，x86 AVX/AVX2 的向量宽度为 256 位 专用指令：ARMv8.2-A 引入了 SDOT 指令，专门用于加速点积计算；x86 引入了 VNNI 指令，也是专门用于加速点积计算 总体而言，ARM 平台的量化点积计算实现充分利用了 NEON 指令集的特性，在支持 SDOT 指令的处理器上可以获得更好的性能。 3. 计算函数实现 (kernel.cpp) kernel.cpp 文件实现了各种计算函数，这些函数使用 TaskSet 实现多线程并行计算。 3.1 嵌入层计算 这个函数实现了嵌入层的计算，将4位整数量化的权重反量化为浮点数。每个任务处理一个或多个序列位置，通过 dequantize_row_q4_0 函数将量化权重反量化为浮点数。 3.2 元素级计算 这个函数实现了元素级计算，支持加法、乘法、SiLU 激活函数和 GELU 激活函数。每个任务处理一段连续的数据，通过 elemwise_vector_add、elemwise_vector_mul、elemwise_vector_silu 和 elemwise_vector_gelu 函数实现具体的计算。 3.3 广播计算 这个函数实现了广播计算，将 src1 广播到 src0 的第0维，然后进行元素级计算。每个任务处理 src0 的一个或多个行，通过 elemwise_vector_add 和 elemwise_vector_mul 函数实现具体的计算。 3.4 RMS 归一化 这个函数实现了 RMS 归一化，每个任务处理一个或多个序列位置。首先计算平方和的均值，然后计算缩放因子，最后将输入向量乘以缩放因子得到归一化结果。 3.5 Softmax 计算 这个函数实现了 Softmax 计算，每个任务处理一个或多个行。首先找到最大值，然后减去最大值并计算指数和，最后将每个元素除以指数和得到 Softmax 结果。 3.6 量化矩阵乘法 这个函数实现了4位整数权重与浮点数激活值的矩阵乘法。它分为两个阶段：第一阶段将输入量化为8位整数，第二阶段计算矩阵乘法。每个阶段都使用多线程并行计算，第一阶段按行分解，第二阶段按列分解。 3.7 多头注意力计算 这两个函数实现了多头注意力计算中的矩阵乘法。llm_matmul_compute_with_head_stride_float 计算 Q 和 K 的矩阵乘法，llm_head_batched_matmul_compute_float 计算 QK 和 V 的矩阵乘法。每个任务处理一个注意力头，通过 compute_src_offset_embd_matmul 和 comput_matmul_with_dst_uncontinue 函数实现具体的计算。 4. 优化策略分析 4.1 NEON 指令集优化 ARM 优化模块使用 NEON 指令集进行向量化计算，提高计算效率： NEON 指令集允许同时处理多个数据元素，大大提高了计算效率。上面的代码每次处理 16 个浮点数，使用 4 个 float32x4_t 寄存器，每个寄存器可以同时处理 4 个浮点数。 4.2 分块处理策略 ARM 优化模块使用分块处理策略，将数据分成多个块进行处理： 这种策略可以最大化利用 NEON 指令集的并行性，同时处理所有数据。 4.3 多线程并行策略 ARM 优化模块使用 TaskSet 实现多线程并行，每个计算函数都返回一个 TaskSet，包含一个或多个任务及其子任务数量： 不同的计算函数使用不同的任务分解策略： 按序列长度分解：如 llm_rms_norm_compute_float，每个任务处理一个或多个序列位置 按头数分解：如 llm_matmul_compute_with_head_stride_float，每个任务处理一个或多个注意力头 按矩阵行列分解：如 llm_matmul_compute_int4_float，使用两个任务集，一个按行分解，一个按列分解 4.4 量化计算优化 ARM 优化模块使用量化计算减少内存占用和计算量： 这种设计减少了内存占用和内存带宽需求，提高了计算效率。特别是对于大型矩阵乘法，量化计算可以显著提高性能。 5. 内核注册机制 ARM 优化模块使用内核注册机制，将函数与内核 ID 关联起来： 这些宏和注册语句将内核 ID 与具体的实现函数关联起来，使得框架可以在运行时根据内核 ID 选择合适的实现。 6. 与 naive 实现的比较 ARM 优化模块与 naive 模块的主要区别： 向量化实现：ARM 优化模块使用 NEON 指令集进行向量化计算，naive 模块使用标量实现 分块处理：ARM 优化模块使用分块处理策略，naive 模块使用简单的循环 量化计算：ARM 优化模块使用量化计算减少内存占用和计算量，naive 模块使用浮点计算 多线程并行：ARM 优化模块使用 TaskSet 实现多线程并行，naive 模块也使用 TaskSet，但任务分解策略不同 在实际应用中，ARM 优化模块的性能明显优于 naive 模块，特别是在支持 NEON 指令集的 ARM 处理器上。 7. 与 x86 和 RVV 优化的比较 ARM 优化模块与 x86 和 RVV 优化模块的主要区别： 指令集：ARM 优化模块使用 NEON 指令集，x86 优化模块使用 SSE/AVX/AVX2 指令集，RVV 优化模块使用 RISC-V 向量扩展指令集 向量宽度：NEON 指令集的向量宽度为 128 位，可以同时处理 4 个单精度浮点数；AVX/AVX2 指令集的向量宽度为 256 位，可以同时处理 8 个单精度浮点数；RVV 指令集的向量宽度可变，取决于硬件实现 优化程度：x86 优化模块的优化程度最高，实现了更多的优化技术；ARM 优化模块次之；RVV 优化模块的优化程度最低，主要依赖 RVV 指令集的基本向量操作 8. 未来优化方向 基于当前实现，可以考虑以下优化方向： 8.1 更多 ARM 指令集支持 支持 ARMv8.2-A 的 FP16 指令，使用半精度浮点数进行计算 支持 ARMv8.2-A 的 DotProd 指令，加速点积计算 支持 ARMv8.6-A 的 BFloat16 指令，使用 BFloat16 进行计算 8.2 更高效的算法 使用 Winograd 算法优化矩阵乘法 使用 Flash Attention 算法优化注意力计算 使用混合精度计算提高性能 8.3 更多量化方法 支持 3 位、2 位甚至 1 位量化 支持非对称量化 支持组量化 8.4 更高级的并行策略 使用流水线并行减少内存占用 使用张量并行和模型并行处理大型模型 使用异步计算提高计算效率 总结 InferLLM 框架中的 ARM 优化模块提供了针对 ARM 架构处理器的优化实现，主要利用 NEON 指令集进行向量化计算，提高大语言模型推理的性能。通过使用 NEON 指令集、分块处理策略、多线程并行和量化计算等技术，ARM 优化模块实现了高效的计算。 与 naive 模块相比，ARM 优化模块的性能明显更高，特别是在支持 NEON 指令集的 ARM 处理器上。与 x86 和 RVV 优化模块相比，ARM 优化模块使用了不同的指令集和优化技术，适用于不同的硬件平台。 未来可以考虑支持更多 ARM 指令集、使用更高效的算法、支持更多量化方法和使用更高级的并行策略等方向进行优化，进一步提高大语言模型在 ARM 平台上的推理性能。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-25/"},{"title":"[InferLLM大模型推理框架项目](24)kern中optimized模块整体分析(src/kern/optimized)","content":"InferLLM 框架中 optimized 模块分析 InferLLM 框架中的 optimized 模块是针对不同硬件平台的优化实现，主要包括 ARM、RISC-V 向量扩展(RVV)和 x86 平台的优化。这些优化实现利用了各平台特有的指令集和特性，提高了大语言模型推理的性能。 1. 目录结构 optimized 模块按照硬件平台分为三个子目录： 2. 核心功能模块 2.1 基础向量运算 每个平台都实现了一系列基础向量运算函数，这些函数是高级操作的基础： 2.1.1 元素级操作 2.1.2 激活函数 2.1.3 归约操作 2.2 量化计算 每个平台都实现了量化和反量化操作，以及量化数据之间的点积计算： 2.3 矩阵运算 实现了矩阵乘法和注意力计算所需的特殊矩阵运算： 3. 平台特定优化 3.1 ARM 平台优化 ARM 平台的优化主要利用 NEON 指令集进行向量化计算： 3.2 RISC-V 向量扩展优化 RISC-V 向量扩展(RVV)的优化利用了 RVV 指令集进行向量化计算： RVV 模块还包含了一些辅助函数，如 vadd、vmul、vmax 等，这些函数封装了 RVV 指令集的使用。 3.3 x86 平台优化 x86 平台的优化主要利用 SSE、AVX 和 AVX2 指令集进行向量化计算： x86 模块还包含了一些辅助函数，如 exp256_ps 等，这些函数实现了数学函数的向量化版本。 4. 实现策略分析 4.1 分块处理 所有平台的优化实现都采用了分块处理策略，将数据分成多个块进行处理： 大块：使用最宽的向量指令处理（如 x86 上的 32 个元素一块） 中块：使用较窄的向量指令处理（如 x86 上的 8 个元素一块） 剩余元素：使用标量指令处理 这种策略可以最大化利用向量指令的并行性，同时处理所有数据。 4.2 内存访问优化 优化实现注重内存访问模式，尽量使用连续的内存访问： 4.3 计算优化 优化实现使用了各种计算优化技术： 向量化：使用 SIMD 指令并行处理多个数据 指令级并行：安排指令顺序，减少依赖 数学函数优化：使用近似计算或查表法加速数学函数 4.4 回退机制 所有优化实现都包含回退机制，当特定平台的指令集不可用时，使用标量实现： 5. 性能优化分析 5.1 ARM 平台性能优化 ARM 平台的性能优化主要体现在： 使用 NEON 指令集进行向量化计算 每次处理 16 个元素（4个 float32x4_t 寄存器） 针对不同数据大小使用不同的处理策略 ARM 平台的优化相对简单，主要依赖 NEON 指令集的基本向量操作。 5.2 RISC-V 向量扩展性能优化 RISC-V 向量扩展的性能优化主要体现在： 使用 RVV 指令集进行向量化计算 利用 RVV 的可变长度向量特性，适应不同硬件实现 封装底层 RVV 指令，提供简洁的接口 RVV 的优化利用了 RISC-V 向量扩展的灵活性，可以适应不同的硬件实现。 5.3 x86 平台性能优化 x86 平台的性能优化最为复杂和全面： 使用 AVX/AVX2 指令集进行向量化计算 每次处理 32 个元素（4个 __m256 寄存器） 实现了复杂数学函数的向量化版本（如 exp256_ps） 针对不同操作使用不同的优化策略 x86 平台的优化利用了 x86 丰富的 SIMD 指令集和寄存器资源，实现了最全面的优化。 6. 与 naive 实现的比较 optimized 模块与 naive 模块的主要区别： naive 模块使用标量实现，optimized 模块使用向量化实现 naive 模块适用于所有平台，optimized 模块针对特定平台优化 naive 模块实现简单直观，optimized 模块实现复杂但性能更高 naive 模块作为参考实现和后备方案，optimized 模块作为主要实现 在实际应用中，系统会优先使用 optimized 模块的实现，当特定平台的优化实现不可用时，会回退到 naive 模块的实现。 7. 多线程并行策略 optimized 模块使用 TaskSet 实现多线程并行，每个计算函数都返回一个 TaskSet，包含一个或多个任务及其子任务数量： 这种设计使得计算任务可以在多线程环境中高效执行，通过将大型计算任务分解为多个子任务，并分配给不同的线程处理。 7.1 任务分解策略 不同的计算函数使用不同的任务分解策略： 按序列长度分解：如 llm_rms_norm_compute_float，每个任务处理一个或多个序列位置 按头数分解：如 llm_matmul_compute_with_head_stride_float，每个任务处理一个或多个注意力头 按矩阵行列分解：如 llm_matmul_compute_int4_float，使用两个任务集，一个按行分解，一个按列分解 7.2 多阶段任务 某些计算函数使用多阶段任务，如 llm_matmul_compute_int4_float： 这种设计使得不同阶段的任务可以并行执行，提高计算效率。 8. 内存优化策略 8.1 工作空间管理 optimized 模块使用工作空间进行临时计算，减少内存分配和释放的开销： 工作空间由调用者分配和管理，计算函数只负责使用。 8.2 量化计算 optimized 模块使用量化计算减少内存占用和计算量： 这种设计减少了内存占用和内存带宽需求，提高了计算效率。 8.3 内存访问模式 optimized 模块优化了内存访问模式，减少内存访问开销： 这种设计提高了缓存命中率，减少了内存访问开销。 9. 注意力计算优化 optimized 模块对注意力计算进行了特殊优化： 9.1 多头注意力 这个函数实现了多头注意力中的 Q 和 K 的矩阵乘法，每个任务处理一个注意力头。 9.2 注意力输出计算 这个函数实现了多头注意力中的 QK 和 V 的矩阵乘法，每个任务处理一个注意力头。 10. 矩阵乘法优化 矩阵乘法是大语言模型中最耗时的操作，optimized 模块对矩阵乘法进行了特殊优化： 10.1 量化矩阵乘法 这个函数实现了 4 位整数权重与浮点数激活值的矩阵乘法，使用量化计算减少内存占用和计算量。 10.2 特殊矩阵乘法 这些函数实现了特殊的矩阵乘法，用于注意力计算中的特殊需求。 11. 未来优化方向 基于当前实现，可以考虑以下优化方向： 11.1 更多硬件平台支持 支持更多 ARM 架构（如 ARMv8.2-A 的 SVE 指令集） 支持更多 RISC-V 扩展（如 RISC-V P 扩展） 支持更多 x86 指令集（如 AVX-512） 11.2 更高效的算法 使用 Winograd 算法优化矩阵乘法 使用 Flash Attention 算法优化注意力计算 使用混合精度计算提高性能 11.3 更多量化方法 支持 3 位、2 位甚至 1 位量化 支持非对称量化 支持组量化 11.4 更高级的并行策略 使用流水线并行减少内存占用 使用张量并行和模型并行处理大型模型 使用异步计算提高计算效率 总结 InferLLM 框架中的 optimized 模块提供了针对不同硬件平台的优化实现，包括 ARM、RISC-V 向量扩展和 x86 平台。这些优化实现利用了各平台特有的指令集和特性，提高了大语言模型推理的性能。 optimized 模块使用多线程并行、量化计算、内存优化和特殊算法等技术，实现了高效的计算。与 naive 模块相比，optimized 模块的实现更复杂，但性能更高。在实际应用中，系统会优先使用 optimized 模块的实现，当特定平台的优化实现不可用时，会回退到 naive 模块的实现。 未来可以考虑支持更多硬件平台、使用更高效的算法、支持更多量化方法和使用更高级的并行策略等方向进行优化，进一步提高大语言模型推理的性能。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-24/"},{"title":"[InferLLM大模型推理框架项目](23)kern模块中GPU内核的实现(src/kern/naive/gpu/kernel_gpu.h+.cpp)","content":"InferLLM 框架中 GPU 内核实现分析 InferLLM 框架中的 GPU 内核实现主要由 kernel_gpu.h 和 kernel_gpu.cu 两个文件组成，它们提供了大语言模型在 GPU 上运行所需的各种计算操作的实现。 1. 整体架构 1.1 文件结构 kernel_gpu.h：声明 GPU 内核函数和注册内核 kernel_gpu.cu：实现 GPU 内核函数 1.2 核心组件 cudaHandle 结构体：管理 CUDA 资源 内核函数：实现各种计算操作 嵌入查找 元素级操作 归一化 Softmax 矩阵乘法 注意力计算 位置编码 掩码操作 内核注册机制：使用宏注册内核函数 2. 主要功能模块分析 2.1 嵌入查找（Embedding Lookup） 这些函数在 GPU 上实现嵌入查找操作： llm_embedding_get_int4_float：从 4 位整数量化的嵌入表中查找 llm_embedding_get_float_float：从浮点数嵌入表中查找 实现中使用 CUDA 内核函数并行处理多个序列位置： 2.2 元素级操作（Elementwise Operations） 这些函数实现了各种元素级操作： 加法、乘法 Silu、Gelu 激活函数 缩放操作 广播操作 实现中使用函数对象（Functor）和模板元编程简化代码： 2.3 归一化（Normalization） 这些函数实现了层归一化和 RMS 归一化： llm_norm_compute_float：层归一化，计算均值和方差 llm_rms_norm_compute_float：RMS 归一化，只计算均方根 实现中使用 warp-level 归约优化性能： 2.4 Softmax 实现 Softmax 函数，使用 warp-level 归约优化性能： 2.5 矩阵乘法（Matrix Multiplication） 这些函数实现了不同精度的矩阵乘法： llm_matmul_compute_int4_float：4 位整数权重与浮点数激活值的矩阵乘法 llm_matmul_compute_float_float：浮点数矩阵乘法 浮点数矩阵乘法使用 cuBLAS 库优化性能： 2.6 注意力计算（Attention Computation） 这些函数实现了自注意力机制所需的矩阵运算： 多头注意力 多查询注意力（MQA） 实现中使用 cuBLAS 的批处理矩阵乘法优化性能： 2.7 位置编码（Position Encoding） 这些函数实现了旋转位置编码（RoPE）和 GLM 模型特定的旋转位置编码： GLM 模型的位置编码实现了特殊的位置计算逻辑： 2.8 掩码操作（Masking Operations） 这些函数实现了注意力掩码操作： llm_diag_mask_inf_float：自回归掩码，将上三角部分设为负无穷 llm_glm_gmask_inf_float：GLM 模型特定的掩码 llm_scale_diag_mask_inf_float：先缩放再掩码 GLM 模型的掩码实现了特殊的掩码逻辑： 3. 内核注册机制 GPU 内核使用模板和宏实现内核注册机制，将函数与内核 ID 关联起来： 这些宏用于注册已实现的内核和标记未实现的内核： 4. CUDA 优化技术 4.1 线程块和网格配置 代码中根据不同的计算需求，使用不同的线程块和网格配置： 4.2 Warp-level 归约 代码中使用 warp-level 归约优化归一化和 Softmax 等操作： 4.3 内存访问优化 代码中使用合适的内存访问模式，减少内存访问开销： 4.4 使用 cuBLAS 库 代码中使用 cuBLAS 库优化矩阵乘法和批处理矩阵乘法： 5. 与 CPU 实现的比较 GPU 实现与 CPU 实现（naive 模块）的主要区别： 5.1 并行度 CPU 实现：使用 TaskSet 和多线程并行 GPU 实现：使用 CUDA 内核函数和数千个线程并行 5.2 内存管理 CPU 实现：直接访问主内存 GPU 实现：需要在 GPU 内存和主内存之间传输数据 5.3 优化方法 CPU 实现：使用 SIMD 指令和缓存优化 GPU 实现：使用 CUDA 内核函数、warp-level 归约和 cuBLAS 库 5.4 功能覆盖 CPU 实现：完整实现所有功能 GPU 实现：部分功能未实现（如 MatmulInt4FloatPacked、MatmulInt8Float 等） 6. 性能考虑 6.1 内存传输开销 GPU 计算需要在 CPU 和 GPU 之间传输数据，这可能成为性能瓶颈。代码中使用 CUDA 流和异步操作减少这种开销： 6.2 内核启动开销 CUDA 内核启动有一定开销，代码中尽量减少内核启动次数，将多个小操作合并为一个大操作： 6.3 线程分配和负载均衡 代码中根据计算需求，合理分配线程和线程块，确保负载均衡： 6.4 内存访问模式 代码中优化内存访问模式，减少内存访问开销： 7. 未实现的功能 代码中使用 NOImplementKernel 宏标记了未实现的功能： 这些功能在 CPU 实现中已经实现，但在 GPU 实现中尚未实现。这可能是因为： 这些功能在 GPU 上实现复杂度高 这些功能在 GPU 上性能提升有限 开发资源有限，优先实现更重要 8. 内核注册与调用机制 GPU 内核的注册与调用机制是通过模板特化和宏定义实现的，这种设计使得框架可以在运行时根据内核 ID 选择合适的实现。 8.1 内核注册宏 这些宏定义了三种类型的内核注册： PartialImplementKernel：注册已实现的计算内核 PartialImplementSpace：注册工作空间计算函数 NOImplementKernel：标记未实现的内核 8.2 内核注册列表 这些注册语句将内核 ID 与具体的实现函数关联起来，使得框架可以在运行时根据内核 ID 选择合适的实现。 8.3 内核调用机制 内核调用是通过 Comp 和 Space 模板类实现的： 这些模板类提供了统一的接口，而具体实现由模板特化提供。在运行时，框架可以根据内核 ID 选择合适的实现： 9. CUDA 资源管理 9.1 cudaHandle 结构体 cudaHandle 结构体管理 CUDA 资源，包括： stream：CUDA 流，用于异步执行 CUDA 操作 cublas_handle：cuBLAS 句柄，用于调用 cuBLAS 库函数 这种设计使得框架可以在多个 CUDA 设备和多个 CUDA 流上并行执行计算，提高计算效率。 9.2 CUDA 流使用 代码中使用 CUDA 流执行异步操作，减少 CPU 和 GPU 之间的同步开销： 9.3 cuBLAS 库使用 代码中使用 cuBLAS 库优化矩阵乘法和批处理矩阵乘法： 10. 条件编译 代码使用条件编译确保 GPU 代码只在启用 GPU 支持时编译： 这种设计使得框架可以在不支持 GPU 的平台上编译和运行，只使用 CPU 实现。 11. 错误处理 代码中使用宏和断言处理错误： 这些宏检查 CUDA 和 cuBLAS 函数的返回值，如果发生错误，打印错误信息并返回。 12. 与其他模块的集成 GPU 内核模块与其他模块的集成主要通过以下方式： 12.1 与 kernel.h 的集成 kernel.h 定义了内核系统的接口，GPU 内核模块实现了这些接口，提供了 GPU 上的计算实现。 12.2 与 tensor.h 的集成 tensor.h 定义了张量数据结构，GPU 内核模块使用这些张量作为输入和输出。 12.3 与 naive 模块的集成 当 GPU 实现不可用时，系统会回退到 naive 模块的 CPU 实现。 13. 性能优化总结 GPU 内核模块使用了多种性能优化技术： 13.1 并行计算 使用 CUDA 内核函数和数千个线程并行计算 使用 cuBLAS 库优化矩阵乘法和批处理矩阵乘法 13.2 内存优化 使用合适的内存访问模式，减少内存访问开销 使用共享内存和寄存器优化内存访问 13.3 算法优化 使用 warp-level 归约优化归一化和 Softmax 等操作 使用批处理矩阵乘法优化注意力计算 13.4 资源管理 使用 CUDA 流执行异步操作，减少 CPU 和 GPU 之间的同步开销 使用 cuBLAS 库优化矩阵乘法和批处理矩阵乘法 14. 未来优化方向 基于当前实现，可以考虑以下优化方向： 14.1 实现未实现的功能 实现当前标记为 NOImplementKernel 的功能： MatmulInt4FloatPacked MatmulInt4WeightReorder MatmulInt8Float EmbeddingGetInt8Float 14.2 使用更高效的 GPU 算法 使用 Tensor Core 加速矩阵乘法 使用 Flash Attention 算法优化注意力计算 使用混合精度计算提高性能 14.3 优化内存使用 使用内存池减少内存分配和释放的开销 使用流水线并行减少内存占用 使用张量并行和模型并行处理大型模型 14.4 支持更多 GPU 平台 支持 AMD GPU（使用 HIP） 支持移动 GPU（使用 OpenCL 或 Vulkan） 支持多 GPU 并行计算 总结 InferLLM 框架中的 GPU 内核实现提供了大语言模型在 GPU 上运行所需的各种计算操作的实现。通过使用 CUDA 内核函数、warp-level 归约、cuBLAS 库等技术，GPU 内核模块在 GPU 上实现了高效的计算。内核注册与调用机制使得框架可以在运行时根据内核 ID 选择合适的实现，而条件编译确保 GPU 代码只在启用 GPU 支持时编译。 虽然当前实现已经覆盖了大部分功能，但仍有一些功能未实现，如 MatmulInt4FloatPacked、MatmulInt4WeightReorder 等。未来可以考虑实现这些功能，并使用更高效的 GPU 算法、优化内存使用、支持更多 GPU 平台等方向进行优化。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-23/"},{"title":"[InferLLM大模型推理框架项目](22)kern模块中量化的实现(src/kern/naive/quantize.h)","content":"InferLLM 框架中 quantize.h 的代码结构与功能分析 quantize.h 是 InferLLM 框架中实现量化和反量化操作的核心头文件，位于 kern/naive 目录下，提供了基础的量化计算实现。这个文件主要实现了模型权重和激活值的量化与反量化操作，以及量化数据之间的点积计算，是实现低精度推理的关键组件。 1. 文件结构概览 文件结构可以分为以下几个部分： 头文件引入 反量化函数（dequantize） 量化函数（quantize） 量化数据点积计算函数 2. 反量化函数 2.1 4位整数反量化 这个函数将4位整数量化的数据反量化为浮点数： 将输入数据分为多个块（每块32个元素） 每个块有一个缩放因子 d 每个字节存储两个4位整数（高4位和低4位） 将4位整数解码为 -8 到 7 的范围，然后乘以缩放因子得到浮点数 2.2 8位整数反量化 这个函数将8位整数量化的数据反量化为浮点数： 将输入数据分为多个块（每块32个元素） 每个块有一个缩放因子 d 直接将8位整数乘以缩放因子得到浮点数 3. 量化函数 3.1 浮点数量化为4位整数 这个函数将浮点数量化为4位整数： 将输入数据分为多个块（每块32个元素） 计算每个块中的最大绝对值 amax 计算缩放因子 d = amax / 7（7 = 2^3 - 1） 将浮点数除以缩放因子，四舍五入后加上偏移量8，得到0到15的整数 将两个4位整数打包到一个字节中（低4位和高4位） 3.2 浮点数量化为8位整数 这个函数将浮点数量化为8位整数： 将输入数据分为多个块（每块32个元素） 计算每个块中的最大绝对值 amax 计算缩放因子 d = amax / 127（127 = 2^7 - 1） 将浮点数除以缩放因子，四舍五入后得到-127到127的整数 4. 量化数据点积计算函数 4.1 4位整数与8位整数点积 这个函数计算4位整数向量与8位整数向量的点积： 将输入数据分为多个块 对于每个块，解码4位整数和8位整数 计算整数之间的点积 乘以两个缩放因子得到最终结果 4.2 打包的4位整数与8位整数点积 这个函数计算8组打包的4位整数向量与一个8位整数向量的点积： 将输入数据分为多个块 对于每个块，解码8组4位整数和1组8位整数 计算8组点积 乘以相应的缩放因子得到最终结果 如果有偏置，加上偏置值 4.3 8位整数与8位整数点积 这个函数计算两个8位整数向量的点积： 将输入数据分为多个块 对于每个块，计算8位整数之间的点积 乘以两个缩放因子得到最终结果 4.4 浮点数与浮点数点积 这个函数计算两个浮点数向量的点积，直接进行浮点数乘法和累加。 5. 功能分析 通过分析 quantize.h 文件，可以看出它实现了以下主要功能： 5.1 数据量化 将浮点数量化为低精度整数（4位或8位），减少内存占用和计算量： 4位整数量化：每个浮点数用4位表示，每个字节存储两个值 8位整数量化：每个浮点数用8位表示 量化过程中使用块量化方法，每个块（通常是32个元素）共享一个缩放因子，这样可以在保持计算精度的同时减少内存占用。 5.2 数据反量化 将低精度整数（4位或8位）反量化为浮点数，用于模型的输出或中间结果： 4位整数反量化：将4位整数解码为-8到7的范围，然后乘以缩放因子 8位整数反量化：将8位整数乘以缩放因子 5.3 量化数据点积计算 实现不同精度数据之间的点积计算，是矩阵乘法的基础操作： 4位整数与8位整数点积 打包的4位整数与8位整数点积（一次计算8组） 8位整数与8位整数点积 浮点数与浮点数点积 这些点积计算函数是实现低精度矩阵乘法的核心，通过整数乘法和累加，再乘以缩放因子，可以在保持计算精度的同时提高计算效率。 6. 实现细节分析 6.1 块量化策略 quantize.h 采用块量化策略，将数据分为固定大小的块（通常是32个元素），每个块共享一个缩放因子。这种策略有以下优点： 减少存储开销：只需要为每个块存储一个缩放因子，而不是每个元素一个 保持局部精度：每个块独立计算缩放因子，可以适应数据的局部分布 计算效率高：可以批量处理一个块内的所有元素 6.2 对称量化 代码中使用的是对称量化方法，具体表现为： 4位整数量化：值域为 -8 到 7（通过减去偏移量8实现） 8位整数量化：值域为 -127 到 127 对称量化相比非对称量化更简单，计算效率更高，但可能在处理非对称分布的数据时精度略低。 6.3 内存布局优化 代码中的内存布局经过优化，以提高访问效率： BlockQ40 结构：先存储缩放因子，再存储量化值 BlockQ40X8 结构：将8组4位整数量化值连续存储，然后是8个缩放因子 打包存储：每个字节存储两个4位整数，减少内存占用 6.4 SIMD 友好设计 虽然 quantize.h 中的实现是标量版本，但其内存布局和计算模式设计考虑了 SIMD 指令集的优化可能： 固定大小的块（32个元素）适合 SIMD 寄存器宽度 连续的内存访问模式有利于 SIMD 加载指令 整数乘法和累加操作适合 SIMD 指令优化 7. 性能优化分析 7.1 计算复杂度优化 通过量化，将浮点数运算转换为整数运算，显著降低了计算复杂度： 浮点数乘法 → 整数乘法（更快） 浮点数加法 → 整数加法（更快） 最后只需要少量的浮点数乘法（乘以缩放因子） 7.2 内存占用优化 量化显著减少了内存占用： 4位量化：比32位浮点数减少87.5%的内存占用 8位量化：比32位浮点数减少75%的内存占用 减少内存占用不仅节省存储空间，还能提高缓存命中率，进一步提升性能。 7.3 批量处理优化 vec_vec_dot_q40_with_q80_packed_reference 函数实现了批量处理优化，一次计算8组点积，减少了函数调用开销和循环控制开销。 7.4 编译器优化友好 代码中使用了 __restrict 关键字，告诉编译器指针之间没有别名，有助于编译器进行更激进的优化。 8. 与其他优化实现的关系 quantize.h 中的函数都带有 _reference 后缀，表明这些是参考实现，主要用于： 功能验证：验证量化和反量化算法的正确性 后备实现：当特定平台的优化实现不可用时作为后备 基准比较：用于与优化实现进行性能比较 实际应用中，会根据目标平台选择更高效的实现，如： x86平台：使用 AVX/AVX2 指令集优化 ARM平台：使用 NEON 指令集优化 GPU平台：使用 CUDA 优化 9. 应用场景分析 9.1 模型权重量化 大型语言模型的权重通常占用大量内存，通过量化可以显著减少内存占用： 原始 LLaMA 7B 模型（FP32）：约28GB 4位量化后：约3.5GB 这使得在资源受限的设备上运行大型模型成为可能。 9.2 激活值量化 在模型推理过程中，中间激活值也可以量化，减少内存占用和计算量： 将注意力矩阵量化为8位整数 将前馈网络的中间结果量化为8位整数 9.3 矩阵乘法加速 矩阵乘法是大型语言模型中最耗时的操作，通过量化可以显著加速： 权重量化为4位整数 激活值量化为8位整数 使用整数矩阵乘法（比浮点数矩阵乘法快） 10. 未来优化方向 基于 quantize.h 的实现，可以考虑以下优化方向： 10.1 更高效的量化方法 非对称量化：处理非对称分布的数据 组量化：不同组使用不同的缩放因子，提高精度 混合精度量化：重要权重使用更高精度，不重要权重使用更低精度 10.2 更多硬件优化 ARM SVE：支持可变长度向量的 ARM 指令集 RISC-V V 扩展：RISC-V 的向量扩展 特定加速器：针对 NPU、TPU 等专用加速器的优化 10.3 算法优化 稀疏量化：结合稀疏性和量化，进一步减少计算量 量化感知训练：在训练阶段考虑量化误差，提高量化模型精度 自适应量化：根据数据分布动态调整量化参数 总结 quantize.h 实现了大语言模型推理中的核心量化操作，包括4位和8位整数量化、反量化以及量化数据之间的点积计算。通过块量化、对称量化、内存布局优化和批量处理等技术，在保持计算精度的同时显著减少了内存占用和计算量。这些实现为 InferLLM 框架提供了高效的低精度推理能力，使得大型语言模型可以在资源受限的设备上运行。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-22/"},{"title":"[InferLLM大模型推理框架项目](21)kern模块中naive的实现(src/kern/naive/naive.h+.cpp)","content":"InferLLM 框架中 naive.h 和 naive.cpp 的代码结构与功能分析 naive.h 和 naive.cpp 是 InferLLM 框架中朴素实现的核心文件，提供了大语言模型推理所需的各种计算操作的基础实现。这些实现不依赖特定硬件优化，确保在任何平台上都能正常运行。 1. 文件结构概览 1.1 naive.h naive.h 主要包含以下内容： 函数声明：声明各种计算函数 模板类定义：定义 Comp 和 Space 模板类 内核注册：使用 PartialImplementKernel 和 PartialImplementSpace 宏注册函数 1.2 naive.cpp naive.cpp 主要包含各种计算函数的具体实现，每个函数都返回一个 TaskSet，用于多线程执行。 2. 核心功能分类 通过分析代码，可以将 naive 模块的功能分为以下几类： 2.1 嵌入查找（Embedding Lookup） 这些函数从嵌入表中查找 token 的嵌入向量，支持不同的数据类型（int4、int8、float）。实现过程： 根据索引找到对应的行 计算权重的步长 对于量化数据，调用反量化函数 对于浮点数据，直接复制 2.2 元素级操作（Elementwise Operations） 这些函数实现了元素级操作，如加法、乘法、Silu 和 Gelu 激活函数，以及带有广播的元素级操作。实现过程： 根据操作模式选择不同的 lambda 函数 对每个元素执行相应的操作 对于广播操作，处理不同维度的广播逻辑 2.3 归一化（Normalization） 这些函数实现了层归一化和 RMS 归一化，用于稳定网络训练和推理。实现过程： 计算均值（对于层归一化）或均方根（对于 RMS 归一化） 计算缩放因子 应用归一化 2.4 Softmax 实现了 Softmax 函数，用于将输出转换为概率分布。实现过程： 找到每行的最大值 减去最大值并计算指数 计算和并归一化 2.5 矩阵乘法（Matrix Multiplication） 这些函数实现了不同精度的矩阵乘法，支持 int4、int8 和 float 数据类型，以及带有打包优化的版本。实现过程： 对于量化版本，先将输入量化为 int8 计算矩阵乘法（点积） 加上偏置（如果有） 2.6 注意力计算（Attention Computation） 这些函数实现了自注意力机制所需的矩阵运算，包括多头注意力和多查询注意力。实现过程： 计算 Q 和 K 的点积 对于多查询注意力，处理广播逻辑 计算注意力权重和 V 的加权和 2.7 位置编码（Position Encoding） 这些函数实现了旋转位置编码（RoPE）和 GLM 模型特定的旋转位置编码。实现过程： 计算旋转角度 应用旋转变换 对于 GLM，处理特殊的位置编码逻辑 2.8 掩码操作（Masking Operations） 这些函数实现了注意力掩码操作，用于控制注意力的范围。实现过程： 对于自回归掩码，将上三角部分设为负无穷 对于 GLM 掩码，处理特殊的掩码逻辑 对于缩放掩码，先缩放再掩码 2.9 排列操作（Permutation） 实现了张量的维度排列操作。实现过程： 根据排列参数重新排列维度 复制数据到新的位置 2.10 权重重排（Weight Reordering） 实现了 int4 权重的重排操作，用于优化矩阵乘法。实现过程： 将权重按照特定的布局重新排列 优化内存访问模式 3. 实现细节分析 3.1 任务分解与多线程 每个计算函数都返回一个 TaskSet，包含一个或多个任务及其子任务数量。每个任务都是一个 lambda 函数，接受一个 TaskId 参数，包含任务的起始索引、结束索引和线程 ID。 这种设计使得计算任务可以在多线程环境中高效执行，通过将大型计算任务分解为多个子任务，并分配给不同的线程处理。 3.2 量化计算 naive 模块支持 int4 和 int8 量化计算，通过 quantize.h 中定义的函数进行量化和反量化操作： 量化计算可以减少内存占用和计算量，同时保持计算精度。 3.3 内存布局优化 代码中的内存布局经过优化，以提高访问效率： 对于矩阵乘法，使用行主序存储 对于注意力计算，使用特定的步长访问内存 对于位置编码，使用特定的内存布局 3.4 数学函数优化 代码中使用了各种数学函数，如 exp、tanh、sqrt 等，这些函数在不同平台上可能有不同的实现，但 naive 模块使用标准库函数，确保在任何平台上都能正常运行。 4. 内核注册机制 naive.h 中使用 PartialImplementKernel 和 PartialImplementSpace 宏注册函数，这些宏定义在 kernel_define.h 中，用于将函数注册到内核系统中： 这种设计使得不同平台的优化实现可以共享相同的接口，而具体实现由不同平台的优化代码提供。 5. 回退机制 naive 模块还提供了回退机制，当特定平台的优化实现不可用时，会回退到 naive 实现： 这种设计确保在任何平台上都能正常运行，即使特定平台的优化实现不可用。 6. 性能考虑 虽然 naive 模块没有使用特定硬件的优化指令，但它仍然考虑了性能优化： 6.1 多线程并行 通过任务分解和 TaskSet，支持多线程并行计算，提高计算效率。 6.2 量化计算 支持 int4 和 int8 量化，减少内存占用和计算量，提高计算效率。 6.3 内存布局优化 使用合适的内存布局，减少内存访问开销，提高缓存命中率。 6.4 算法优化 使用优化的算法实现各种计算操作，如矩阵乘法、注意力计算等。 7. 与其他模块的关系 naive 模块是 InferLLM 框架中的基础实现，与其他模块的关系如下： 7.1 与 kernel.h 的关系 kernel.h 定义了内核系统的接口，naive 模块实现了这些接口，提供了基础的计算实现。 7.2 与 quantize.h 的关系 quantize.h 定义了量化和反量化函数，naive 模块使用这些函数进行量化计算。 7.3 与优化实现的关系 naive 模块是优化实现的基础和后备方案，当特定平台的优化实现不可用时，会回退到 naive 实现。 8. 应用场景 naive 模块适用于以下场景： 8.1 跨平台推理 由于不依赖特定硬件优化，naive 模块可以在任何平台上运行，适用于跨平台推理。 8.2 功能验证 naive 模块提供了参考实现，可以用于验证优化实现的正确性。 8.3 后备方案 当特定平台的优化实现不可用时，naive 模块可以作为后备方案，确保系统正常运行。 9. 设计模式分析 naive 模块采用了多种设计模式： 9.1 策略模式 通过 Comp 和 Space 模板类，可以根据内核 ID 选择不同的计算策略。 9.2 模板方法模式 通过 PartialImplementKernel 和 PartialImplementSpace 宏定义的模板类，提供了统一的接口，而具体实现由不同的函数提供。 9.3 命令模式 通过 lambda 函数和 TaskSet，将计算任务封装为命令对象，由线程池执行。 9.4 回退模式 通过 opt 命名空间中的模板类，如果没有优化实现则回退到 naive 实现，确保在任何平台上都能正常运行。 总结 naive.h 和 naive.cpp 是 InferLLM 框架中的基础计算实现，提供了大语言模型推理所需的各种计算操作的朴素实现。这些实现不依赖特定硬件优化，确保在任何平台上都能正常运行。通过任务分解和多线程、量化计算、内存布局优化和算法优化等技术，naive 模块在不依赖特定硬件优化的情况下，仍然能够提供相对高效的计算性能。同时，它作为其他优化实现的基础和后备方案，确保在任何平台上都能正常运行。 naive 模块的设计体现了 InferLLM 框架的可移植性和可扩展性，通过提供统一的接口和基础实现，使得框架可以在不同平台上运行，并且可以根据需要添加特定平台的优化实现。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-21/"},{"title":"[InferLLM大模型推理框架项目](20)kern中naive模块的概述(src/kern/naive)","content":"InferLLM 框架中 naive 模块的代码结构与功能分析 naive 模块是 InferLLM 框架中的基础计算实现，提供了不依赖特定硬件优化的计算内核。这个模块作为其他优化实现的基础和后备方案，确保在任何平台上都能正常运行。 1. 目录结构 naive 模块包含以下文件： naive.h：声明各种计算函数和注册内核 naive.cpp：实现各种计算函数 quantize.h：实现量化和反量化相关的函数 2. 核心文件分析 2.1 naive.h naive.h 文件定义了各种计算函数的声明，并使用 PartialImplementKernel 和 PartialImplementSpace 宏注册这些函数到内核系统中： 2.2 naive.cpp naive.cpp 文件实现了各种计算函数，每个函数都返回一个 TaskSet，用于多线程执行： 3. 功能分析 通过分析代码，可以看出 naive 模块实现了以下主要功能： 3.1 嵌入查找 这些函数从嵌入表中查找 token 的嵌入向量，支持不同的数据类型（int4、int8、float）。 3.2 元素级操作 这些函数实现了元素级操作，如加法、乘法、Silu 和 Gelu 激活函数，以及带有广播的元素级操作。 3.3 归一化 这些函数实现了层归一化和 RMS 归一化，用于稳定网络训练和推理。 3.4 Softmax 实现了 Softmax 函数，用于将输出转换为概率分布。 3.5 矩阵乘法 这些函数实现了不同精度的矩阵乘法，支持 int4、int8 和 float 数据类型，以及带有打包优化的版本。 3.6 注意力计算 这些函数实现了自注意力机制所需的矩阵运算，包括多头注意力和多查询注意力。 3.7 位置编码 这些函数实现了旋转位置编码（RoPE）和 GLM 模型特定的旋转位置编码。 3.8 掩码操作 这些函数实现了注意力掩码操作，用于控制注意力的范围。 3.9 排列操作 实现了张量的维度排列操作。 3.10 权重重排 实现了 int4 权重的重排操作，用于优化矩阵乘法。 4. 实现细节分析 4.1 任务分解与多线程 每个计算函数都返回一个 TaskSet，包含一个或多个任务及其子任务数量。每个任务都是一个 lambda 函数，接受一个 TaskId 参数，包含任务的起始索引、结束索引和线程 ID。 这种设计使得计算任务可以在多线程环境中高效执行，通过将大型计算任务分解为多个子任务，并分配给不同的线程处理。 4.2 量化计算 naive 模块支持 int4 和 int8 量化计算，通过 quantize.h 中定义的函数进行量化和反量化操作： 量化计算可以减少内存占用和计算量，同时保持计算精度。 4.3 矩阵乘法优化 矩阵乘法是深度学习中最耗时的操作之一，naive 模块通过以下方式优化矩阵乘法： 量化计算：使用 int4 和 int8 量化减少内存占用和计算量 打包优化：llm_matmul_compute_int4_float_packed 函数实现了打包优化，一次处理多个权重 工作空间优化：使用工作空间进行中间计算，减少内存分配和释放的开销 4.4 注意力计算优化 注意力计算是 Transformer 模型的核心，naive 模块通过以下方式优化注意力计算： 多头并行：将不同的注意力头分配给不同的线程处理 内存布局优化：使用合适的内存布局减少内存访问开销 广播优化：对于多查询注意力，使用广播优化减少内存占用和计算量 5. 设计模式分析 naive 模块采用了多种设计模式： 5.1 策略模式 通过 Comp 和 Space 模板类，可以根据内核 ID 选择不同的计算策略。 5.2 模板方法模式 通过 PartialImplementKernel 和 PartialImplementSpace 宏定义的模板类，提供了统一的接口，而具体实现由不同的函数提供。 5.3 命令模式 通过 lambda 函数和 TaskSet，将计算任务封装为命令对象，由线程池执行。 5.4 回退模式 通过 opt 命名空间中的模板类，如果没有优化实现则回退到 naive 实现，确保在任何平台上都能正常运行。 6. 性能考虑 虽然 naive 模块没有使用特定硬件的优化指令，但它仍然考虑了性能优化： 多线程并行：通过任务分解和 TaskSet，支持多线程并行计算 量化计算：支持 int4 和 int8 量化，减少内存占用和计算量 内存布局优化：使用合适的内存布局减少内存访问开销 算法优化：使用优化的算法实现各种计算操作 总结 naive 模块是 InferLLM 框架中的基础计算实现，提供了不依赖特定硬件优化的计算内核。它实现了大语言模型推理所需的各种计算操作，包括嵌入查找、元素级操作、归一化、Softmax、矩阵乘法、注意力计算、位置编码、掩码操作、排列操作和权重重排等。 通过任务分解和多线程、量化计算、矩阵乘法优化和注意力计算优化等技术，naive 模块在不依赖特定硬件优化的情况下，仍然能够提供相对高效的计算性能。同时，它作为其他优化实现的基础和后备方案，确保在任何平台上都能正常运行。 naive 模块的设计体现了 InferLLM 框架的可移植性和可扩展性，通过提供统一的接口和基础实现，使得框架可以在不同平台上运行，并且可以根据需要添加特定平台的优化实现。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-20/"},{"title":"[InferLLM大模型推理框架项目](19)kernel类的定义(src/kern/kernel.h)","content":"kernel.h 代码结构与功能实现分析 kernel.h 是 InferLLM 框架中的核心头文件，定义了 Kernel 类，该类作为不同平台内核实现的统一接口，负责根据硬件平台选择合适的计算内核实现，并管理多线程任务调度。 1. 文件结构概览 文件结构可以分为以下几个部分： 头文件引入和条件编译 Kernel 类定义 构造函数和初始化 线程和优化相关方法 计算操作接口 成员变量 2. 头文件引入和条件编译 这部分代码通过条件编译引入不同平台的优化实现： 如果定义了 INFER_X86，则引入 x86 平台的优化实现 如果定义了 INFER_ARM，则引入 ARM 平台的优化实现 如果定义了 INFER_RVV，则引入 RISC-V 向量扩展的优化实现 否则，引入朴素实现 此外，如果定义了 ENABLE_GPU，则引入 GPU 平台的优化实现。 这种设计使得编译时可以根据目标平台选择合适的优化实现，而不需要修改源代码。 3. Kernel 类定义 Kernel 类是整个内核系统的核心，它提供了统一的接口，根据硬件平台选择合适的计算内核实现，并管理多线程任务调度。 4. 构造函数和初始化 Kernel 类提供了两个构造函数： 第一个构造函数只接受内核类型参数，不使用线程池 第二个构造函数接受内核类型和线程池指针，可以进行多线程计算 如果定义了 INFER_RVV，则在构造函数中调用 opt::init() 进行 RISC-V 向量扩展的初始化。 5. 线程和优化相关方法 这部分代码提供了两个方法： nr_thread()：返回线程池中的线程数量，如果没有线程池则返回 1 supported_optimization()：检查是否支持特定的优化方法，目前只检查 ARM 平台是否支持 MatmulInt4Reorder 优化 supported_optimization() 方法使用条件编译检查 ARM 平台是否支持点积指令（__ARM_FEATURE_DOTPROD），如果支持则返回 true，否则返回 false。 6. 计算操作接口 这部分代码提供了两个模板方法： operator()：执行计算操作，根据内核类型选择不同的实现 get_workspace()：获取计算操作所需的工作空间大小 operator() 方法是 Kernel 类的核心，它根据内核类型选择不同的实现： 如果内核类型是 GPU，则调用 GPU 实现的 exec 方法 否则，调用优化实现的 get_all_task 方法获取任务集，然后将任务添加到线程池中执行 这种设计使得不同平台的优化实现可以共享相同的接口，而具体实现由不同平台的优化代码提供。 7. 成员变量 Kernel 类有两个主要成员变量： m_thread_pool：线程池指针，用于多线程计算 m_kernel_type：内核类型，用于选择合适的计算内核实现 如果定义了 ENABLE_GPU，则还有一个 m_handle 成员变量，用于 GPU 计算。 8. 功能分析 通过分析 kernel.h 文件，可以看出它实现了以下功能： 8.1 统一接口 通过 Kernel 类提供统一的接口，使得不同平台的优化实现可以共享相同的接口，而具体实现由不同平台的优化代码提供。 8.2 平台选择 通过条件编译和 KernelType 枚举，可以在编译时和运行时选择合适的优化实现，从而在不同硬件平台上获得最佳性能。 8.3 多线程计算 通过线程池和任务集，可以将计算任务分解为多个子任务，并分配给不同的线程处理，从而提高计算效率。 8.4 GPU 加速 通过条件编译和 KernelType::GPU，可以在支持 GPU 的平台上使用 GPU 加速计算。 9. 设计模式分析 kernel.h 文件采用了多种设计模式： 9.1 策略模式 通过 KernelType 枚举和不同平台的优化实现，可以在运行时选择不同的计算策略。 9.2 模板方法模式 通过模板方法 operator() 和 get_workspace()，提供了统一的接口，而具体实现由不同平台的优化代码提供。 9.3 工厂模式 通过 Kernel 类的构造函数和 KernelType 枚举，可以创建不同类型的内核实例。 总结 kernel.h 是 InferLLM 框架中的核心头文件，定义了 Kernel 类，该类作为不同平台内核实现的统一接口，负责根据硬件平台选择合适的计算内核实现，并管理多线程任务调度。通过条件编译、模板方法和策略模式等技术，实现了在不同硬件平台上的高效计算，同时保持了代码的可维护性和扩展性。这种设计使得 InferLLM 框架能够在不同硬件平台上高效运行，而不需要修改核心代码。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-19/"},{"title":"[InferLLM大模型推理框架项目](18)kernel计算的基础定义实现(src/kern/kernel_define.h)","content":"kernel_define.h 代码结构与功能实现分析 kernel_define.h 是 InferLLM 框架中的核心头文件，定义了内核计算所需的基本数据结构、枚举类型和宏定义。这个文件为整个计算内核系统提供了基础设施，使得不同平台的优化实现可以共享相同的接口和数据结构。 1. 文件结构概览 文件结构可以分为以下几个部分： 头文件引入和宏定义 命名空间和类型定义 内核相关枚举类型 任务调度相关结构 量化计算相关结构 宏定义工具 2. 头文件引入和宏定义 这部分引入了基本的 C/C++ 标准库头文件，并定义了一些常量和宏： PI 和 PGELU：数学常量，其中 PGELU 是 GELU 激活函数中使用的常数 INFER_ATTRIBUTE_TARGET：用于指定函数应该使用特定的 SIMD 指令集编译 3. 命名空间和类型定义 所有代码都在 inferllm 命名空间中，InData 是一个模板类型别名，表示指向常量数据的指针数组，用于传递多个输入数据。 4. 内核相关枚举类型 这部分定义了几个重要的枚举类型： KernelID：标识不同类型的计算操作，包括： 嵌入查找操作（如 EmbeddingGetInt4Float） 元素级操作（如 ElemwiseFloat） 矩阵乘法操作（如 MatmulInt4Float） 注意力计算相关操作（如 HeadBatchedMatmulFloat） 位置编码操作（如 RopeFloat） KernelOptMethod：定义内核优化方法，目前只有 MatmulInt4Reorder ElemMode：定义元素级操作的模式，包括加法、乘法、Silu 和 Gelu 激活函数 RotMode：定义旋转位置编码的模式，用于实现 RoPE（Rotary Position Embedding） KernelType：定义内核类型，包括朴素实现、ARM 优化、X86 优化和 GPU 实现 5. 任务调度相关结构 这部分定义了多线程任务调度相关的结构： TaskId：表示任务的范围和线程 ID MultiThreadingTask：表示可以在多线程环境中执行的任务函数 TaskSet：表示一组任务及其子任务数量 这些结构使得内核计算可以在多线程环境中高效执行，通过将大型计算任务分解为多个子任务，并分配给不同的线程处理。 6. 量化计算相关结构 这部分定义了量化计算相关的数据结构： BlockQ40：4 位量化块，包含一个缩放因子 d 和 16 个字节的量化值（每个字节存储两个 4 位值） BlockQ40X8：8 组 4 位量化块，包含 8 个缩放因子和 128 个字节的量化值 BlockQ80：8 位量化块，包含一个缩放因子 d 和 32 个字节的量化值 这些结构使得模型可以使用低精度整数（4 位或 8 位）进行计算，从而减少内存占用和计算量，同时通过缩放因子保持计算精度。 7. 宏定义工具 这部分定义了两个宏，用于简化内核实现： PartialImplementKernel：为特定的内核 ID 实现 Comp 模板类，该类提供 get_all_task 静态方法，返回任务集 PartialImplementSpace：为特定的内核 ID 实现 Space 模板类，该类提供 get 静态方法，返回工作空间大小 这些宏使得不同平台的优化实现可以方便地注册到内核系统中，而不需要修改核心代码。 8. 功能分析 通过分析 kernel_define.h 文件，可以看出它实现了以下功能： 8.1 内核标识与分类 通过 KernelID 枚举，将不同类型的计算操作进行分类和标识，使得内核系统可以根据操作类型选择合适的实现。 8.2 多线程任务调度 通过 TaskId、MultiThreadingTask 和 TaskSet 结构，提供了多线程任务调度的基础设施，使得计算任务可以在多线程环境中高效执行。 8.3 量化计算支持 通过 BlockQ40、BlockQ40X8 和 BlockQ80 结构，提供了 4 位和 8 位量化计算的支持，使得模型可以使用低精度整数进行计算，从而减少内存占用和计算量。 8.4 内核实现注册 通过 PartialImplementKernel 和 PartialImplementSpace 宏，提供了内核实现注册的机制，使得不同平台的优化实现可以方便地注册到内核系统中。 9. 设计模式分析 kernel_define.h 文件采用了多种设计模式： 9.1 策略模式 通过 KernelType 枚举，可以在运行时选择不同的计算策略（朴素实现、ARM 优化、X86 优化、GPU 实现）。 9.2 模板方法模式 通过 PartialImplementKernel 和 PartialImplementSpace 宏定义的模板类，提供了统一的接口，而具体实现由不同平台的优化代码提供。 9.3 命令模式 通过 MultiThreadingTask 和 TaskSet，将计算任务封装为命令对象，由线程池执行。 总结 kernel_define.h 是 InferLLM 框架中的核心头文件，定义了内核计算所需的基本数据结构、枚举类型和宏定义。它为整个计算内核系统提供了基础设施，使得不同平台的优化实现可以共享相同的接口和数据结构。通过多线程任务调度、量化计算支持和内核实现注册等机制，使得 InferLLM 框架能够在不同硬件平台上高效运行，同时保持代码的可维护性和扩展性。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-18/"},{"title":"[InferLLM大模型推理框架项目](17)InferLLM模块kern的概述(src/kern)","content":"InferLLM 框架中 kern 模块的代码结构与功能分析 kern 模块是 InferLLM 框架中的核心计算模块，负责提供各种计算内核（kernel）的实现，以支持不同硬件平台上的高效计算。通过分析代码结构，可以看出该模块采用了分层设计，支持多种硬件平台的优化实现。 1. 目录结构 kern 模块的目录结构如下： 这种结构清晰地分离了不同平台的实现： naive/: 提供基础的、不依赖特定硬件优化的实现 optimized/: 包含针对不同硬件平台优化的实现 arm/: ARM 平台优化 rvv/: RISC-V 向量扩展优化 x86/: x86 平台优化 gpu/: GPU 平台优化实现 2. 核心文件分析 2.1 kernel_define.h kernel_define.h 定义了内核计算所需的基本数据结构和枚举类型： 此外，该文件还定义了量化计算所需的数据结构： 2.2 kernel.h kernel.h 定义了 Kernel 类，作为不同平台内核实现的统一接口： Kernel 类的设计采用了模板和函数对象模式，通过 operator() 重载实现了统一的计算接口，根据内核类型自动选择相应的实现。 3. 优化实现分析 3.1 x86 平台优化 optimized/x86/ 目录下的实现利用了 x86 平台的 SIMD 指令集（如 AVX、AVX2）进行优化： 通过 INFER_ATTRIBUTE_TARGET 宏，可以根据编译目标自动选择合适的实现。 3.2 ARM 平台优化 optimized/arm/ 目录下的实现利用了 ARM 平台的 NEON 指令集进行优化： 3.3 RISC-V 向量扩展优化 optimized/rvv/ 目录下的实现针对 RISC-V 向量扩展进行了优化，提供了与其他平台类似的接口。 3.4 GPU 优化 gpu/ 目录下的实现使用 CUDA 进行 GPU 加速： 4. 功能分析 通过分析代码，可以看出 kern 模块提供了以下主要功能： 4.1 基础数学运算 元素级操作：加法、乘法、Silu、Gelu 等激活函数 矩阵乘法：支持不同精度（float、int4、int8）的矩阵乘法 归一化：RMSNorm、LayerNorm 等 4.2 特定于 LLM 的操作 嵌入查找：从嵌入表中查找 token 的嵌入向量 注意力计算：实现自注意力机制所需的矩阵运算 旋转位置编码：实现 RoPE（Rotary Position Embedding） 4.3 量化支持 4位量化：支持 4 位整数量化，减少内存占用和计算量 8位量化：支持 8 位整数量化 量化/反量化：提供在不同精度之间转换的函数 4.4 多线程与并行计算 任务分解：将大型计算任务分解为多个子任务 线程池集成：利用线程池执行并行计算 GPU 加速：支持 GPU 上的并行计算 5. 设计模式分析 kern 模块采用了多种设计模式： 5.1 策略模式 通过 KernelType 枚举和不同目录下的实现，可以在运行时选择不同的计算策略（naive、arm、x86、gpu）。 5.2 模板方法模式 使用 C++ 模板和宏定义（如 PartialImplementKernel）来实现不同内核的统一接口。 5.3 工厂模式 Kernel 类根据 KernelID 和 KernelType 创建并执行相应的计算任务。 5.4 命令模式 通过 TaskSet 和 MultiThreadingTask 将计算任务封装为命令对象，由线程池执行。 6. 优化策略分析 kern 模块采用了多种优化策略： 6.1 SIMD 指令优化 利用 x86 的 AVX/AVX2 和 ARM 的 NEON 指令集进行向量化计算。 6.2 量化计算 通过 4 位和 8 位量化减少内存占用和计算量，同时保持计算精度。 6.3 多线程并行 将计算任务分解为多个子任务，利用线程池并行执行。 6.4 GPU 加速 支持在 GPU 上执行计算，充分利用 GPU 的并行计算能力。 6.5 条件编译 通过条件编译和宏定义，根据目标平台自动选择最优实现。 总结 InferLLM 框架的 kern 模块是一个高度优化的计算内核库，提供了大语言模型推理所需的各种计算操作。它采用分层设计，支持多种硬件平台（x86、ARM、RISC-V、GPU），并通过 SIMD 指令、量化计算、多线程并行和 GPU 加速等技术实现高效计算。这种设计使得 InferLLM 能够在不同硬件平台上高效运行，同时保持代码的可维护性和扩展性。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-17/"},{"title":"[InferLLM大模型推理框架项目](16)LlamaLike模型代码实现(src/graph/llama_like.h+.cpp)","content":"LlamaLike 模型代码结构与功能实现分析 llama_like.h 和 llama_like.cpp 文件实现了 InferLLM 框架中对类 Llama 架构模型（如 Baichuan 和 Llama2）的支持。这些文件定义了 LlamaLikeGraph 类，该类继承自基础的 Graph 类，并实现了特定于类 Llama 架构模型的加载和计算图构建逻辑。 1. llama_like.h 文件分析 1.1 文件结构 llama_like.h 文件结构简洁，主要包含以下几个部分： 头文件引入： 只引入了基础的图类头文件。 命名空间声明： 所有代码都在 inferllm 命名空间中。 LlamaLikeGraph 类定义： 定义了 LlamaLikeGraph 类，继承自 Graph 类，并重写了三个关键方法： set_weights_alias：设置权重别名映射 construct_llm：构建模型计算图 load_param：加载模型参数 2. llama_like.cpp 文件分析 2.1 头文件引入 引入了 llama_like.h 和 chatGLM.h 头文件，后者可能是为了使用其中定义的一些数据结构（如 Header）。 2.2 权重别名设置 这个方法建立了原始权重名称和内部使用的权重名称之间的映射关系。与 GGML Llama 和 ChatGLM 不同，LlamaLike 模型使用分离的 Q、K、V 权重，而不是融合的 QKV 权重。这反映了 Llama2 和 Baichuan 等模型的架构特点。 2.3 参数加载 这个方法从模型文件中加载参数和词汇表，包括： 读取模型头部信息： 读取参数、词汇表和张量的偏移量和长度 加载模型参数： 读取嵌入维度、头数、层数、前馈网络倍数和词汇表大小 加载词汇表： 根据词汇表偏移量读取词汇表 输出日志： 输出词汇表大小的日志信息 设置文件指针： 将文件指针移动到张量数据的起始位置 2.4 计算图构建 这个方法构建了 LlamaLike 模型的计算图，包括： 初始化参数： 从模型参数中获取嵌入维度、前馈网络大小、头数等参数 计算旋转位置编码的维度 rot = embd / head 创建输入张量和嵌入层： 创建输入张量 添加嵌入层，将 token ID 转换为嵌入向量 循环添加 Transformer 层： 每个 Transformer 层包括： 注意力层前的 RMSNorm（rms = true，bias = false，eps = 1e-6） 注意力层 (LlamaAttention)，使用 ModelRotHalf 旋转模式 残差连接 前馈网络前的 RMSNorm 前馈网络 (LlamaFFNModule) 残差连接 添加输出层： 添加 HeadModule，将隐藏状态转换为词汇表概率分布 3. LlamaLike 模型的特点 通过分析 llama_like.h 和 llama_like.cpp 文件，可以看出 LlamaLike 模型的以下特点： 3.1 模型架构 Pre-LayerNorm 架构： 在注意力层和前馈网络前进行层归一化 使用 RMSNorm（rms = true，没有偏置项 bias = false） 使用较小的 epsilon 值 (eps = 1e-6) 分离的 Q、K、V 权重： 使用分离的 Q、K、V 权重，而不是融合的 QKV 权重 权重别名映射反映了这一点： 旋转位置编码： 使用旋转位置编码 (RoPE)，通过 rot = embd / head 计算旋转维度 使用 ModelRotHalf 旋转模式：RotMode::ModelRotHalf SwiGLU 激活函数： 在前馈网络中使用 SwiGLU 激活函数，通过 LlamaFFNModule 实现 使用三个权重矩阵：w1、w2 和 w3 无偏置项： 层归一化不使用偏置项 (bias = false) 注意力层不使用偏置项 (false 参数) 3.2 与 GGML Llama 的比较 LlamaLike 模型与 GGML Llama 模型有以下区别： 权重结构： LlamaLike 使用分离的 Q、K、V 权重 GGML Llama 可能使用融合的 QKV 权重 权重命名： LlamaLike 使用 model.embed_tokens.weight 等命名 GGML Llama 使用不同的命名约定 旋转模式： LlamaLike 明确指定了 ModelRotHalf 旋转模式 GGML Llama 可能使用不同的旋转模式 模型格式： LlamaLike 使用自定义格式 GGML Llama 使用 GGML 格式 3.3 与 ChatGLM 系列的比较 LlamaLike 模型与 ChatGLM 系列模型有以下区别： 层归一化： LlamaLike 使用 RMSNorm（rms = true，bias = false） ChatGLM 使用传统的层归一化（rms = false，bias = true） 注意力机制： LlamaLike 使用 LlamaAttention ChatGLM 使用 GlmAttention 或 Glm2MultiQueryAttention 前馈网络： LlamaLike 使用 LlamaFFNModule ChatGLM 使用 GlmFFNModule 残差连接： LlamaLike 使用普通的残差连接 ChatGLM 使用带缩放因子的残差连接 特殊 token 处理： LlamaLike 模型没有实现 post_tokenize 方法，可能不需要特殊的 token 处理 ChatGLM 系列模型实现了 post_tokenize 方法，在输入序列的开头和结尾添加特殊 token 词汇表大小： LlamaLike 模型的词汇表大小由模型文件决定 ChatGLM 系列模型使用固定的词汇表大小（ChatGLM 为 130528，ChatGLM2/3 为 65024） 4. 模型加载流程 LlamaLike 模型的加载流程包括以下步骤： 读取模型头部信息： 加载模型参数： 加载词汇表： 构建计算图： 在 Graph::load 方法中调用 construct_llm 构建计算图 收集权重： 在 Graph::load 方法中调用 collect_weights 收集权重 设置权重别名： 在 Graph::load 方法中调用 set_weights_alias 设置权重别名 加载权重： 在 Graph::load 方法中循环读取权重数据，并将其与计算图中的权重关联 5. 计算图执行流程 LlamaLike 模型的计算图执行流程与其他模型类似，包括以下步骤： 准备输入： 将输入文本转换为 token ID 如果需要，进行特殊 token 处理 执行计算图： 从输入到输出执行计算图 每个 Transformer 层按顺序执行 采样： 根据输出概率分布采样下一个 token 生成文本： 将采样得到的 token ID 转换为文本 重复执行计算图和采样，直到生成结束 6. 优化策略 LlamaLike 模型实现了一些优化策略： RMSNorm： 使用 RMSNorm 代替传统的层归一化，减少计算量 不使用偏置项，减少参数量 旋转位置编码： 使用旋转位置编码 (RoPE)，避免了位置嵌入的额外参数 使用 ModelRotHalf 旋转模式，可能是为了优化性能 SwiGLU 激活函数： 在前馈网络中使用 SwiGLU 激活函数，提高模型性能 分离的 Q、K、V 权重： 使用分离的 Q、K、V 权重，可能有助于并行计算 7. 扩展性分析 LlamaLike 模型的设计具有良好的扩展性： 支持多种模型： 通过相同的接口支持 Baichuan 和 Llama2 等多种模型 只需修改权重别名映射，就可以支持新的模型变体 模块化设计： 使用模块化设计构建计算图，便于扩展和维护 可以方便地替换或修改特定模块 参数化配置： 模型参数（如嵌入维度、头数、层数等）由模型文件决定 不需要硬编码特定模型的参数 总结 llama_like.h 和 llama_like.cpp 文件实现了 InferLLM 框架中对类 Llama 架构模型（如 Baichuan 和 Llama2）的支持。它们定义了 LlamaLikeGraph 类，该类继承自基础的 Graph 类，并实现了特定于类 Llama 架构模型的加载和计算图构建逻辑。 通过分析这些文件，我们可以了解类 Llama 架构模型的结构特点和实现细节，以及 InferLLM 框架如何支持不同架构的模型。这种设计使得 InferLLM 可以灵活地支持多种模型架构，而不需要修改框架的核心代码。 LlamaLike 模型的实现展示了现代大型语言模型的一些关键技术，如 RMSNorm、旋转位置编码和 SwiGLU 激活函数等。这些技术的应用使得模型在保持高性能的同时，减少了参数量和计算量，提高了推理效率。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-16/"},{"title":"[InferLLM大模型推理框架项目](15)Llama系列模型代码实现(src/graph/ggml_llama.h+.cpp)","content":"GGML Llama 模型代码结构与功能实现分析 ggml_llama.h 和 ggml_llama.cpp 文件实现了 InferLLM 框架中对 GGML 格式的 Llama 模型的支持。这些文件定义了 GgmlLlamaGraph 类，该类继承自基础的 Graph 类，并实现了特定于 GGML Llama 模型的加载和计算图构建逻辑。 1. ggml_llama.h 文件分析 1.1 文件结构 ggml_llama.h 文件包含以下几个部分： 头文件引入： 引入了必要的头文件，包括图、KV存储、算子和张量等核心组件。 命名空间声明： 所有代码都在 inferllm 命名空间中。 模型类型枚举： 定义了三种 GGML 格式的 Llama 模型类型： LLAMA_FILE_VERSION_GGML：原始 GGML 格式 LLAMA_FILE_VERSION_GGMF_V1：添加了版本字段和词汇表分数的格式 LLAMA_FILE_VERSION_GGJT_V1：另一种扩展格式 GgmlLlamaGraph 类定义： 定义了 GgmlLlamaGraph 类，继承自 Graph 类，并重写了三个关键方法： set_weights_alias：设置权重别名映射 construct_llm：构建模型计算图 load：加载模型参数和权重 2. ggml_llama.cpp 文件分析 2.1 权重别名设置 这个方法建立了原始权重名称和内部使用的权重名称之间的映射关系，使得模型可以正确加载预训练权重。 2.2 模型加载 这个方法从模型文件中加载参数和权重，包括： 验证模型格式： 读取魔数和版本号 根据魔数和版本号确定模型类型 加载模型参数： 读取词汇表大小、嵌入维度、头数、层数等参数 输出参数信息 加载词汇表： 根据模型类型选择不同的词汇表加载方法 构建计算图： 调用 construct_llm 构建计算图 调用 collect_weights 收集权重 加载权重： 设置权重别名 循环读取权重数据 将权重数据与计算图中的权重关联 2.3 计算图构建 这个方法构建了 Llama 模型的计算图，包括： 初始化参数： 从模型参数中获取嵌入维度、头数、层数等参数 计算前馈网络的隐藏层大小 创建输入张量和嵌入层： 创建输入张量 添加嵌入层，将 token ID 转换为嵌入向量 循环添加 Transformer 层： 每个 Transformer 层包括： 注意力层前的层归一化 注意力层 (LlamaAttention) 残差连接 前馈网络前的层归一化 前馈网络 (LlamaFFNModule) 残差连接 添加输出层： 添加 HeadModule，将隐藏状态转换为词汇表概率分布 3. GGML Llama 模型的特点 通过分析 ggml_llama.h 和 ggml_llama.cpp 文件，可以看出 GGML Llama 模型的以下特点： 3.1 模型架构 Pre-LayerNorm 架构： 在注意力层和前馈网络前进行层归一化 使用 RMSNorm（没有偏置项） 旋转位置编码： 使用旋转位置编码 (RoPE)，通过 rot 参数指定旋转维度 SwiGLU 激活函数： 在前馈网络中使用 SwiGLU 激活函数，通过 LlamaFFNModule 实现 残差连接： 在注意力层和前馈网络后使用残差连接 3.2 模型格式 GGML Llama 模型支持三种格式： LLAMA_FILE_VERSION_GGML：原始 GGML 格式 LLAMA_FILE_VERSION_GGMF_V1：添加了版本字段和词汇表分数的格式 LLAMA_FILE_VERSION_GGJT_V1：另一种扩展格式，支持带分数的词汇表 3.3 权重加载 模型权重的加载过程包括： 读取权重的维度、名称和数据类型 通过权重别名映射找到对应的内部权重 将权重数据与内部权重关联 对于 GGJT_V1 格式，需要进行 32 字节对齐 4. 与其他模型的比较 与 ChatGLM 系列模型相比，GGML Llama 模型有以下区别： 模型格式： GGML Llama 使用 GGML 格式，而 ChatGLM 使用自定义格式 注意力机制： GGML Llama 使用 LlamaAttention，而 ChatGLM 使用 GlmAttention 前馈网络： GGML Llama 使用 LlamaFFNModule，而 ChatGLM 使用 GlmFFNModule 层归一化： GGML Llama 使用 RMSNorm（没有偏置项），而 ChatGLM 使用传统的层归一化（有偏置项） 残差连接： GGML Llama 使用普通的残差连接，而 ChatGLM 使用带缩放因子的残差连接 总结 ggml_llama.h 和 ggml_llama.cpp 文件实现了 InferLLM 框架中对 GGML 格式的 Llama 模型的支持。它们定义了 GgmlLlamaGraph 类，该类继承自基础的 Graph 类，并实现了特定于 GGML Llama 模型的加载和计算图构建逻辑。 通过分析这些文件，我们可以了解 GGML Llama 模型的结构特点和实现细节，以及 InferLLM 框架如何支持不同格式的模型。这种设计使得 InferLLM 可以灵活地支持多种模型格式，而不需要修改框架的核心代码。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-15/"},{"title":"[InferLLM大模型推理框架项目](14)ChatGLM系列模型代码实现(src/graph/...)","content":"ChatGLM 系列模型代码结构与功能实现分析 InferLLM 框架中的 ChatGLM 系列模型实现包括 ChatGLM、ChatGLM2 和 ChatGLM3 三个版本，它们都继承自基础的 Graph 类，并实现了特定的模型结构和加载逻辑。下面对这些文件的代码结构和功能实现进行详细分析。 1. chatGLM.h 文件分析 chatGLM.h 文件定义了 ChatGLM 系列模型的基础结构和接口： 1.1 基础数据结构 这个结构体用于存储模型文件的头信息，包括参数、词汇表和张量的偏移量和长度。 这个结构体用于存储模型的参数信息，包括隐藏层大小、注意力头数、层数等。 1.2 ChatGLM 系列类定义 ChatGLMGraph 类继承自 Graph 类，并重写了四个关键方法： set_weights_alias：设置权重别名映射 construct_llm：构建模型计算图 load_param：加载模型参数 post_tokenize：对输入 token 进行后处理 ChatGLMGraph2 和 ChatGLMGraph3 类的定义与 ChatGLMGraph 类似，也重写了相同的四个方法。 2. chatGLM.cpp 文件分析 chatGLM.cpp 文件实现了 ChatGLM 模型的具体功能： 2.1 权重别名设置 这个方法建立了原始权重名称和内部使用的权重名称之间的映射关系，使得模型可以正确加载预训练权重。 2.2 参数加载 这个方法从模型文件中加载参数和词汇表，包括： 读取模型头部信息 根据偏移量读取模型参数 加载词汇表 设置固定的词汇表大小（130528） 将文件指针移动到张量数据的起始位置 2.3 计算图构建 这个方法构建了 ChatGLM 模型的计算图，包括： 创建输入张量 添加嵌入层 循环添加多个 Transformer 层，每层包括： 注意力层前的层归一化 注意力层 (GlmAttention) 残差连接（带缩放因子） 前馈网络前的层归一化 前馈网络 (GlmFFNModule) 残差连接（带缩放因子） 添加输出层 (HeadModule) 2.4 Token 后处理 这个方法在 token 序列的开头和结尾添加特殊 token： 在开头添加 token ID 5（开始标记） 在结尾添加 token ID 130001 和 130004（结束标记） 3. chatGLM2.cpp 文件分析 chatGLM2.cpp 文件实现了 ChatGLM2 模型的具体功能： 3.1 权重别名设置 ChatGLM2 的权重别名与 ChatGLM 不同，反映了模型架构的变化。 3.2 参数加载 ChatGLM2 的参数加载与 ChatGLM 类似，但增加了多查询注意力相关参数的加载，并使用不同的词汇表大小（65024）。 3.3 计算图构建 ChatGLM2 的计算图构建与 ChatGLM 类似，但有以下区别： 使用 Glm2MultiQueryAttention 代替 GlmAttention 使用 RMSNorm 代替传统的层归一化（rms 参数为 true） 不使用偏置项（bias 参数为 false） 残差连接不使用缩放因子 3.4 Token 后处理 ChatGLM2 的 token 后处理与 ChatGLM 不同，在开头添加了两个特殊 token（64790 和 64792）。 4. chatGLM3.cpp 文件分析 chatGLM3.cpp 文件实现了 ChatGLM3 模型的具体功能，其实现与 ChatGLM2 非常相似： 4.1 权重别名设置 ChatGLM3 的权重别名与 ChatGLM2 相同，表明它们的模型架构相似。 4.2 参数加载 ChatGLM3 的参数加载与 ChatGLM2 相同，包括多查询注意力相关参数的加载和相同的词汇表大小（65024）。 4.3 计算图构建 ChatGLM3 的计算图构建与 ChatGLM2 相同，使用相同的注意力机制和层归一化方式。 4.4 Token 后处理 ChatGLM3 的 token 后处理与 ChatGLM2 相同，在开头添加相同的特殊 token。 5. ChatGLM 系列模型的演进 通过分析这三个版本的代码，可以看出 ChatGLM 系列模型的演进： 5.1 ChatGLM 到 ChatGLM2 的变化 架构变化： 词嵌入层路径从 transformer.word_embeddings 变为 transformer.embedding.word_embeddings Transformer 层路径从 transformer.layers 变为 transformer.encoder.layers 注意力机制： 从普通注意力 (GlmAttention) 变为多查询注意力 (Glm2MultiQueryAttention) 归一化方式： 从传统的层归一化变为 RMSNorm 残差连接： 去掉了残差连接中的缩放因子 词汇表大小： 从 130528 减少到 65024 特殊 token： 使用不同的特殊 token 5.2 ChatGLM2 到 ChatGLM3 的变化 ChatGLM3 与 ChatGLM2 的代码几乎相同，表明它们的基础架构非常相似。可能的区别在于预训练权重和训练方法，而不是模型架构本身。 6. 总结 ChatGLM 系列模型在 InferLLM 框架中的实现展示了大型语言模型的基本架构和演进过程。通过分析这些代码，我们可以看到： 模块化设计： 使用模块化设计构建计算图，便于扩展和维护 权重映射： 使用权重别名机制，适应不同的权重命名约定 架构演进： 从 ChatGLM 到 ChatGLM2/3，模型架构有明显改进 引入多查询注意力机制，提高效率 使用 RMSNorm 代替传统层归一化，提高稳定性 特殊 token 处理： 不同版本使用不同的特殊 token，反映了模型训练的变化 这些实现为我们理解大型语言模型的架构和推理过程提供了宝贵的参考。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-14/"},{"title":"[InferLLM大模型推理框架项目](13)计算图的工厂方法实现(src/graph/graph_imp.cpp)","content":"graph_imp.cpp 代码结构与功能实现分析 graph_imp.cpp 是 InferLLM 框架中的一个核心文件，主要实现了计算图的工厂方法，用于根据模型名称创建相应的计算图实例。这个文件虽然简短，但在整个框架中扮演着重要的角色。 1. 文件结构 graph_imp.cpp 文件包含以下几个部分： 头文件引入： 引入了三个头文件，分别对应不同的模型实现： chatGLM.h：包含 ChatGLM 系列模型的定义 ggml_llama.h：包含 GGML 格式的 Llama 模型定义 llama_like.h：包含类 Llama 架构的模型定义（如 Baichuan 和 Llama2） 命名空间声明： 使用 inferllm 命名空间，这是整个框架的主命名空间。 工厂方法实现： 实现了 Graph 类的静态方法 make_graph，用于创建计算图实例。 2. 功能实现 2.1 工厂方法 make_graph make_graph 方法是一个典型的工厂方法，根据传入的模型名称 name 创建相应的计算图实例： 该方法接收三个参数： model_config：用户配置，包含模型的各种配置参数 device：设备指针，指向执行计算的设备（如 CPU 或 GPU） name：模型名称，用于确定创建哪种类型的计算图 根据模型名称，该方法创建以下几种计算图： &quot;llama&quot;：创建 GgmlLlamaGraph 实例，用于处理 GGML 格式的 Llama 模型 &quot;chatglm&quot;：创建 ChatGLMGraph 实例，用于处理 ChatGLM 模型 &quot;chatglm2&quot;：创建 ChatGLMGraph2 实例，用于处理 ChatGLM2 模型 &quot;chatglm3&quot;：创建 ChatGLMGraph3 实例，用于处理 ChatGLM3 模型 &quot;baichuan&quot; 或 &quot;llama2&quot;：创建 LlamaLikeGraph 实例，用于处理类 Llama 架构的模型 如果传入的模型名称不在上述列表中，则通过 INFER_ASSERT 宏触发断言，提示不支持该模型。 3. 设计模式分析 3.1 工厂模式 graph_imp.cpp 文件实现了典型的工厂模式，具体来说是简单工厂模式（Simple Factory Pattern）： 工厂方法：Graph::make_graph 是一个静态工厂方法，负责创建具体的产品（计算图实例） 抽象产品：Graph 是抽象产品，定义了计算图的接口 具体产品：GgmlLlamaGraph、ChatGLMGraph、ChatGLMGraph2、ChatGLMGraph3 和 LlamaLikeGraph 是具体产品，实现了计算图的接口 这种设计有以下优点： 封装创建逻辑：客户端不需要知道如何创建具体的计算图，只需要提供模型名称 扩展性：可以方便地添加新的模型支持，只需在工厂方法中添加新的条件分支 统一接口：所有计算图都实现了相同的接口，客户端可以统一处理 3.2 策略模式 虽然不是直接在 graph_imp.cpp 中实现，但整个计算图系统也体现了策略模式（Strategy Pattern）： 上下文：使用计算图的客户端代码 策略接口：Graph 类定义的接口 具体策略：不同的计算图实现（如 ChatGLMGraph、GgmlLlamaGraph 等） 这种设计使得客户端可以在运行时选择不同的计算图实现，而不需要修改客户端代码。 4. 与其他组件的交互 graph_imp.cpp 文件通过工厂方法与框架的其他组件进行交互： 与模型实现的交互： 引入不同模型的头文件（chatGLM.h、ggml_llama.h、llama_like.h） 创建不同模型的计算图实例 与设备的交互： 接收 Device* 参数，将设备传递给计算图实例 计算图实例使用设备进行计算 与用户配置的交互： 接收 UserConfig 参数，将用户配置传递给计算图实例 计算图实例根据用户配置调整行为 5. 扩展性分析 graph_imp.cpp 文件的设计具有良好的扩展性： 添加新模型： 要添加新的模型支持，只需： 创建新的计算图类，继承自 Graph 在 make_graph 方法中添加新的条件分支 修改现有模型： 修改现有模型的实现不需要修改 graph_imp.cpp 文件，只需修改相应的计算图类。 总结 graph_imp.cpp 文件虽然简短，但在 InferLLM 框架中扮演着重要的角色，实现了计算图的工厂方法，使得框架可以支持多种不同的模型。它采用了工厂模式和策略模式的设计，具有良好的封装性和扩展性，为框架提供了灵活的模型支持机制。 通过这个文件，我们可以看到 InferLLM 框架的模块化设计思想，以及如何通过设计模式实现灵活的组件组合。这种设计使得框架可以方便地支持新的模型，而不需要大幅修改现有代码。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-13/"},{"title":"[InferLLM大模型推理框架项目](12)graph模块的功能和具体实现(src/graph)","content":"InferLLM 图结构代码分析 InferLLM 的 graph 目录包含了不同模型架构的计算图实现，主要负责构建和管理大型语言模型的计算流程。下面对其代码结构和功能进行详细分析。 1. 整体架构 graph 目录主要包含以下几个模型的实现： ChatGLM 系列：ChatGLM、ChatGLM2、ChatGLM3 Llama 系列：GgmlLlama、LlamaLike (包括 Baichuan 和 Llama2) 所有这些模型都继承自基础的 Graph 类，并实现了特定的模型结构和加载逻辑。 2. 基础组件 2.1 Graph 工厂方法 这个工厂方法根据模型名称创建相应的图实现。 2.2 模型参数结构 这些结构体用于存储模型文件的头信息和模型参数。 3. 模型实现分析 3.1 ChatGLM 系列 3.1.1 ChatGLMGraph ChatGLM 是最早的 ChatGLM 系列模型实现。 权重别名设置： 这个方法建立了原始权重名称和内部使用的权重名称之间的映射关系。 参数加载： 这个方法从模型文件中加载参数和词汇表。 构建计算图： 这个方法构建了 ChatGLM 模型的计算图，包括： 创建输入张量 添加嵌入层 循环添加多个 Transformer 层，每层包括： 注意力层前的层归一化 注意力层 (GlmAttention) 残差连接 前馈网络前的层归一化 前馈网络 (GlmFFNModule) 残差连接 添加输出层 (HeadModule) 后处理 token： 这个方法在 token 序列的开头和结尾添加特殊 token。 3.1.2 ChatGLMGraph2 和 ChatGLMGraph3 ChatGLM2 和 ChatGLM3 的实现与 ChatGLM 类似，但有一些区别： 权重别名不同：反映了模型架构的变化 参数加载增加了多查询注意力相关参数： 构建计算图使用不同的注意力机制： ChatGLM2 使用 Glm2MultiQueryAttention ChatGLM3 可能使用其他变体 3.2 Llama 系列 3.2.1 GgmlLlamaGraph GgmlLlamaGraph 实现了 GGML 格式的 Llama 模型。 加载方法： 这个方法处理了 GGML 格式的模型加载，包括识别不同的 GGML 版本。 构建计算图： 这个方法构建了 Llama 模型的计算图，结构与 ChatGLM 类似，但使用了 LlamaAttention 和 LlamaFFNModule。 3.2.2 LlamaLikeGraph LlamaLikeGraph 实现了类 Llama 架构的模型，如 Baichuan 和 Llama2。其实现与 GgmlLlamaGraph 类似，但加载方式和权重别名有所不同。 权重别名设置： LlamaLikeGraph 使用分离的 Q、K、V 权重，而不是融合的 QKV 权重。 参数加载： LlamaLikeGraph 的参数加载相对简单，直接读取各个参数值。 4. 模块组织结构 InferLLM 的图结构采用模块化设计，主要包括以下几种模块： 4.1 基础模块 EmbdModule：实现词嵌入功能，将 token ID 转换为嵌入向量 LayerNorm：实现层归一化，用于稳定网络训练 Elemwise：实现元素级操作，如加法、乘法等 HeadModule：实现输出层，将隐藏状态转换为词汇表概率分布 4.2 注意力模块 AttentionModule：注意力机制的通用模块，可以使用不同的注意力实现 GlmAttention：ChatGLM 系列的注意力实现 Glm2MultiQueryAttention：ChatGLM2 的多查询注意力实现 LlamaAttention：Llama 系列的注意力实现 4.3 前馈网络模块 GlmFFNModule：ChatGLM 系列的前馈网络实现 LlamaFFNModule：Llama 系列的前馈网络实现 5. 计算图构建流程 所有模型的计算图构建流程大致相同： 创建输入张量： 添加嵌入层： 循环添加 Transformer 层： 添加输出层： 6. 模型加载流程 模型加载流程通常包括以下步骤： 读取模型头部信息：获取参数、词汇表和张量的偏移量 加载模型参数：读取嵌入维度、头数、层数等参数 加载词汇表：读取词汇表信息 构建计算图：根据模型参数构建计算图 收集权重：收集计算图中的所有权重 设置权重别名：建立原始权重名称和内部使用的权重名称之间的映射关系 加载权重：从模型文件中读取权重数据 7. 模型推理流程 模型推理流程通常包括以下步骤： 准备输入：将输入文本转换为 token ID 前处理：添加特殊 token，如开始 token 执行计算图：从输入到输出执行计算图 采样：根据输出概率分布采样下一个 token 后处理：将 token ID 转换为文本 8. 不同模型的特点 8.1 ChatGLM 系列 Pre-LayerNorm 架构：先进行层归一化，再进行注意力计算 融合的 QKV 权重：使用一个权重矩阵计算 Q、K、V 残差连接带缩放：残差连接时使用缩放因子 scale = sqrt(2 * nr_layer) 带偏置的层归一化：层归一化使用偏置项 8.2 Llama 系列 RMSNorm：使用 RMSNorm 代替传统的层归一化 旋转位置编码：使用旋转位置编码 (RoPE) 代替位置嵌入 SwiGLU 激活函数：在前馈网络中使用 SwiGLU 激活函数 无偏置项：大多数层不使用偏置项 总结 InferLLM 的图结构代码实现了多种大型语言模型的计算图构建和执行。它采用模块化设计，支持不同的模型架构，如 ChatGLM 系列和 Llama 系列。通过分析这些代码，我们可以了解不同模型的结构特点和实现细节，以及 InferLLM 框架的设计思想和工作原理。 这种设计使得 InferLLM 可以灵活地支持新的模型架构，只需继承 Graph 类并实现相应的方法，就可以添加新的模型支持。同时，模块化的设计也使得代码更易于维护和扩展。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-12/"},{"title":"[InferLLM大模型推理框架项目](11)Op类的实现(src/core/op.h+.cpp)","content":"Op 类代码结构与功能实现分析 op.h 和 op.cpp 文件定义了 InferLLM 框架中的各种算子（Operator）类，这些算子是构建神经网络计算图的基本单元。下面对这些文件的代码结构和功能实现进行详细分析。 1. 基础定义 OpIOs 是一个类型别名，表示算子的输入、输出或权重，它是一个 Tensor 指针的向量。 PACK_SIZE 是一个常量，用于 Int4 量化权重的打包大小，设置为 8。 2. OpBase 类 OpBase 是所有算子的基类，定义了算子的基本接口和通用功能： 2.1 OpBase 的主要方法 构造函数：初始化设备、输入和名称，并增加输入张量的用户计数。 pre_execute：执行前准备，包括准备权重和输出张量的数据。 execute：执行算子的计算，这是一个虚函数，由子类实现。 end_execute：执行后清理，减少输入张量的当前用户计数。 deduce_output_shape：推导输出张量的形状，默认与第一个输入相同。 get_workspace_in_byte：获取执行时需要的工作空间大小。 load_weights：从文件加载权重。 init：初始化算子，设置输入、输出和工作空间。 2.2 OpBase 的辅助方法 device：获取设备指针。 get_kernel：获取内核指针。 set_weights：设置权重张量。 add_outputs：添加输出张量。 set_name：设置算子名称。 weights/inputs/outputs/name：获取权重、输入、输出和名称。 need_preprocess_weight：判断是否需要预处理权重。 preprocess_weight：预处理权重，使其更适合计算内核。 3. 具体算子类 3.1 LayerNorm 类 LayerNorm 实现了层归一化操作，支持 RMS 归一化和普通归一化，可选是否使用乘法和偏置。 3.1.1 LayerNorm::execute 实现 这个方法执行层归一化操作： 获取权重和偏置（如果有） 获取输入和输出张量 根据 m_rms 标志选择 RMS 归一化或普通归一化 如果有权重，执行元素级乘法 如果有偏置，执行元素级加法 3.2 MatMul 类 MatMul 实现了矩阵乘法操作，支持不同数据类型（Float32、Int8、Int4）的权重，以及可选的偏置。 3.2.1 MatMul::execute 实现 这个方法执行矩阵乘法操作： 获取矩阵的维度（M、N、K） 获取输入和权重的数据类型 根据权重的数据类型选择不同的矩阵乘法内核 如果权重是 Int4 类型且已打包，使用优化的打包版本 3.3 MatMulLast 类 MatMulLast 是 MatMul 的子类，专门用于计算序列中最后一个 token 的矩阵乘法，这在生成阶段很常见。 3.3.1 MatMulLast::execute 实现 与 MatMul::execute 类似，但只计算最后一个 token（src 指向最后一行）。 3.4 SoftMax 类 SoftMax 实现了 softmax 操作，通常用于注意力机制中。 3.4.1 SoftMax::execute 实现 这个方法执行 softmax 操作，对每一行（通常是注意力分数）应用 softmax 函数。 3.5 Reshape 类 Reshape 改变张量的形状，保持元素总数不变。 3.5.1 Reshape::deduce_output_shape 实现 这个方法推导输出张量的形状： 计算输入张量的元素总数 根据目标形状计算每个维度的大小 如果某个维度是 -1，则自动计算该维度的大小 3.6 Elemwise 类 Elemwise 实现了元素级操作，如加法、乘法、GELU 激活等。 这个方法执行元素级操作： 获取输入和输出张量 根据操作模式（m_mode）选择不同的元素级操作内核 如果只有一个输入，执行单输入操作（如 GELU、Silu、Scale） 如果有两个输入，执行双输入操作（如 Add、Mul） 3.7 SpliteHalfActiveMul 类 SpliteHalfActiveMul 实现了将输入张量分成两半，对一半应用激活函数，然后与另一半相乘的操作。这在一些模型架构中很常见，如 SwiGLU。 这个方法执行分半激活乘法操作： 获取输入和输出张量 计算每半部分的嵌入维度 根据激活模式选择不同的内核（SiluMul 或 GeluMul） 3.8 DiagMask 类 DiagMask 实现了对角线掩码操作，通常用于自回归模型中的注意力机制，防止模型看到未来的 token。 这个方法执行对角线掩码操作： 获取输入和输出张量 调用对角线掩码内核，将未来 token 的注意力分数设为负无穷大 3.9 AttentionBase 类 AttentionBase 是注意力机制的基类，提供了注意力机制的通用功能，如 KV 缓存管理。 AttentionBase 类提供了： 工作空间大小计算，用于分配足够的内存进行注意力计算 权重预处理，优化 Int4 量化权重的内存布局 3.10 LlamaAttention 类 LlamaAttention 实现了 Llama 模型的注意力机制，包括旋转位置编码（RoPE）。 这个方法执行 Llama 注意力机制： 获取工作空间指针，为 Q, K, V, QK 分配内存 计算 Q, K, V 矩阵 应用旋转位置编码（RoPE） 将 K, V 存储到缓存中 计算注意力分数 QK 应用掩码和 softmax 计算注意力输出 3.11 GlmAttention 类 GlmAttention 实现了 GLM 模型的注意力机制，与 Llama 注意力机制类似，但有一些特定于 GLM 的处理。 3.12 Glm2MultiQueryAttention 类 Glm2MultiQueryAttention 实现了 GLM-2 模型的多查询注意力机制，这是一种优化的注意力变体，减少了 KV 缓存的大小。 3.13 Embedding 类 Embedding 实现了嵌入层，将 token ID 转换为嵌入向量。 这个方法执行嵌入操作： 获取输入 token ID、输出嵌入向量和嵌入权重 根据权重的数据类型选择不同的嵌入内核 将 token ID 转换为嵌入向量 4. 算子执行流程 算子的典型执行流程如下： 创建算子： 推导输出形状： 执行前准备： 执行计算： 执行后清理： 5. 内存管理 算子的内存管理主要通过以下机制实现： 引用计数： add_user()：增加张量的用户计数 decrease_curr_user_count()：减少张量的当前用户计数 resume_user_count()：恢复张量的用户计数 数据准备和回收： prepare_data()：准备张量的数据，可能涉及内存分配或从文件加载 recall_data()：回收张量的数据，可能涉及内存释放 工作空间： get_workspace_in_byte()：获取执行时需要的工作空间大小 workspace-&gt;ptr()：获取工作空间的指针 6. 优化策略 算子实现了多种优化策略： 权重预处理： need_preprocess_weight()：判断是否需要预处理权重 preprocess_weight()：预处理权重，优化内存布局 KV 缓存： m_kstorage 和 m_vstorage：存储注意力机制的 K 和 V 矩阵 prepare_data_with_length()：准备指定长度的缓存数据 add_id()：更新缓存的 ID 数据类型优化： 支持多种数据类型（Float32、Float16、Int8、Int4） 为不同数据类型提供专门的计算内核 内存复用： 通过工作空间复用内存 通过张量共享减少内存分配 总结 op.h 和 op.cpp 文件定义了 InferLLM 框架中的各种算子类，这些算子是构建神经网络计算图的基本单元。它们提供了丰富的功能，包括层归一化、矩阵乘法、softmax、元素级操作和注意力机制等。这些算子的实现考虑了性能优化，如内存复用、数据类型优化和权重预处理等。 7. 算子的设计模式 InferLLM 框架中的算子设计采用了以下设计模式： 工厂模式： 通过 Graph::make_graph 创建计算图，然后在计算图中创建各种算子。 策略模式： 通过虚函数和多态实现不同算子的不同行为，如 execute、deduce_output_shape 等。 组合模式： 算子可以组合成更复杂的计算图，形成层次结构。 观察者模式： 通过引用计数机制，张量可以观察到它的用户（算子）何时不再需要它，从而释放资源。 8. 算子的扩展性 InferLLM 框架的算子设计具有良好的扩展性： 新算子添加： 只需继承 OpBase 类并实现相应的虚函数，即可添加新的算子。 新模型支持： 通过组合现有算子或添加新算子，可以支持新的模型架构，如 Llama、GLM 等。 新硬件支持： 通过在 Kernel 类中添加新的硬件后端实现，可以支持新的硬件平台。 9. 算子的性能优化 InferLLM 框架的算子实现了多种性能优化技术： 内存优化： 工作空间复用 张量共享 KV 缓存 计算优化： 权重预处理 融合算子（如 SpliteHalfActiveMul） 特化实现（如 MatMulLast） 数据类型优化： 支持多种数据类型（Float32、Float16、Int8、Int4） 为不同数据类型提供专门的计算内核 10. 算子的调试支持 InferLLM 框架的算子提供了一些调试支持： 命名机制： 每个算子和张量都有一个名称，便于调试和跟踪。 断言检查： 使用 INFER_ASSERT 宏进行断言检查，确保算子的正确性。 形状推导： 通过 deduce_output_shape 方法推导输出张量的形状，便于检查形状是否正确。 11. 与其他组件的交互 算子与框架的其他组件有密切的交互： 与 Device 的交互： 算子通过 m_device 获取设备信息和内核实现。 与 Kernel 的交互： 算子通过 get_kernel() 获取内核实现，调用相应的计算函数。 与 Tensor 的交互： 算子管理输入、输出和权重张量，控制它们的生命周期。 与 KvStorage 的交互： 注意力算子使用 KvStorage 存储 K 和 V 矩阵，实现缓存机制。 12. 算子的使用示例 以下是一个使用算子构建简单计算图的示例： 通过这种方式，可以构建复杂的计算图，实现大型语言模型的推理。 总的来说，op.h 和 op.cpp 文件定义了 InferLLM 框架中的算子系统，它是框架的核心组件之一，负责实现各种神经网络操作。它的设计考虑了性能、扩展性和易用性，为大型语言模型的推理提供了高效的计算支持。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-11/"},{"title":"[InferLLM大模型推理框架项目](10)ThreadPool类的实现(src/core/thread_pool.h+.cpp)","content":"ThreadPool 类代码结构与功能实现分析 ThreadPool 类是 InferLLM 框架中的线程池实现，用于并行执行计算任务，提高模型推理的性能。下面对 thread_pool.h 和 thread_pool.cpp 的代码结构和功能实现进行详细分析。 1. INFER_PAUSE 宏定义 首先，框架定义了一个平台相关的 INFER_PAUSE 宏，用于实现 CPU 级别的暂停操作： 这个宏根据不同的平台和架构，使用不同的指令实现 CPU 暂停操作，主要用于自旋锁中减少 CPU 资源消耗。 2. Worker 结构体 Worker 结构体表示一个工作线程及其状态： 每个 Worker 包含一个线程和一个原子标志 work_flag，用于指示线程是否需要执行任务。 3. ThreadPool 类 ThreadPool 类是线程池的主要实现： 3.1 构造函数 构造函数创建指定数量的工作线程： 确保线程数至少为 1 检查线程数是否超过系统 CPU 核心数，如果超过则发出警告 创建 m_nr_threads - 1 个工作线程（主线程也会参与计算） 每个工作线程的主循环： 外层循环：当 m_stop 为 false 时持续运行 中层循环：当 m_active 为 true 时处于活动状态 内层逻辑： 如果 work_flag 为 true，执行任务并将 work_flag 设为 false 使用自旋等待检查是否有新任务 如果超过等待限制，则使用 std::this_thread::yield() 让出 CPU 当 m_active 为 false 时，使用条件变量等待唤醒 3.2 add_task 方法 add_task 方法向线程池添加任务： 如果只有一个线程或一个任务，直接在当前线程执行 否则： 激活线程池 设置任务数量和每个线程的任务数 设置任务函数 将所有工作线程的 work_flag 设为 true，通知它们开始工作 主线程也参与计算，处理剩余的任务 调用 sync() 等待所有线程完成任务 3.3 sync 方法 sync 方法等待所有工作线程完成任务： 检查是否有未完成的线程（work_flag 为 true） 如果有，使用自旋等待该线程完成 如果超过等待限制，则使用 std::this_thread::yield() 让出 CPU 重复上述过程，直到所有线程完成任务 3.4 active 和 deactive 方法 active 方法激活线程池，唤醒所有等待的工作线程 deactive 方法停用线程池，使工作线程进入休眠状态，减少 CPU 占用 3.5 析构函数 析构函数清理线程池资源： 设置 m_stop 为 true，通知所有工作线程退出 唤醒所有等待的工作线程 删除所有工作线程对象 4. 任务分配策略 ThreadPool 使用简单的任务分配策略： 将总任务数 nr_task 平均分配给所有线程 每个线程处理 m_task_per_thread = (nr_task + m_nr_threads - 1) / m_nr_threads 个任务 第 i 个线程处理的任务范围是 [i * m_task_per_thread, min((i + 1) * m_task_per_thread, m_nr_task)] 主线程（最后一个线程）处理剩余的任务 5. 同步机制 ThreadPool 使用多种同步机制： 原子变量： m_stop：指示线程池是否停止 m_active：指示线程池是否处于活动状态 work_flag：指示工作线程是否需要执行任务 条件变量： m_cv：用于在线程池激活时唤醒工作线程 自旋锁： 使用 INFER_PAUSE 和 std::this_thread::yield() 实现自旋等待，减少线程切换开销 6. 优化策略 ThreadPool 实现了几种优化策略： 主线程参与计算： 主线程也参与任务计算，减少线程创建和切换开销。 自旋等待： 使用自旋等待检查任务状态，减少线程切换开销。自旋等待分为两个阶段： 前 ACTIVE_WAIT_PAUSE_LIMIT 次迭代使用 CPU 级别的暂停操作 之后交替使用 CPU 级别的暂停和 OS 级别的让出 线程休眠： 当线程池不活动时，工作线程进入休眠状态，减少 CPU 占用。 任务合并： 当只有一个线程或一个任务时，直接在当前线程执行，避免线程调度开销。 动态休眠： 线程池可以通过 deactive() 方法进入休眠状态，在不需要并行计算时减少 CPU 占用。 快速检测： 在 sync() 方法中，记录上一个未完成的线程 ID，优先检查该线程，减少不必要的检查。 7. 使用示例 ThreadPool 的典型使用方式如下： 8. TaskId 结构体 TaskId 结构体用于描述任务的范围和执行线程，它包含三个字段： start：任务的起始索引 end：任务的结束索引 thread_id：执行任务的线程ID 9. MultiThreadingTask 类型 MultiThreadingTask 是一个函数类型，表示可以并行执行的任务。它接受一个 TaskId 参数，用于指定任务的范围和执行线程。 10. 线程池的生命周期管理 ThreadPool 的生命周期管理非常重要，它涉及到线程的创建、激活、停用和销毁： 创建： 在构造函数中创建工作线程，但线程初始处于非活动状态。 激活： 调用 active() 方法激活线程池，唤醒所有工作线程。 停用： 调用 deactive() 方法停用线程池，使工作线程进入休眠状态。 销毁： 在析构函数中设置 m_stop 为 true，通知所有工作线程退出，然后等待线程结束并释放资源。 11. 与其他组件的交互 ThreadPool 主要与 CPUDevice 类交互，用于并行执行 CPU 上的计算任务： 总结 ThreadPool 类是 InferLLM 框架中的线程池实现，用于并行执行计算任务，提高模型推理的性能。它采用了多种优化策略，如主线程参与计算、自旋等待、线程休眠和任务合并等，以减少线程调度开销并提高并行效率。它还提供了灵活的接口，支持动态激活和停用线程池，以适应不同的计算需求。 通过分析 thread_pool.h 和 thread_pool.cpp，我们可以看到 InferLLM 框架在并行计算方面的设计思想和实现细节，这对于理解框架的性能优化策略非常有帮助。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-10/"},{"title":"[InferLLM大模型推理框架项目](09)ModelImp类的实现(src/core/model_imp.h+.cpp)","content":"ModelImp 类代码结构与功能实现分析 ModelImp 类是 InferLLM 框架中 Model 类的具体实现，负责模型的加载、初始化和推理过程。下面对 model_imp.h 和 model_imp.cpp 的代码结构和功能实现进行详细分析。 1. 辅助函数 这个辅助函数将字符串表示的数据类型转换为枚举类型 DType，支持多种精度的数据类型，包括 float32、float16、int8、uint8、int4 和 uint4。 2. ModelImp 类 2.1 构造函数 构造函数根据配置创建相应的设备和计算图： 根据 device_type 创建 CPU 或 GPU 设备 对于 CPU，根据编译选项选择 X86、Arm 或 Naive 内核 对于 GPU，检查是否启用了 GPU 支持 创建用户配置，设置计算类型 创建计算图 初始化 m_past 为 0，表示已处理的 token 数量 2.2 load 方法 load 方法从指定路径加载模型： 创建词汇表对象 创建输入文件对象，支持内存映射 设置上下文长度 调用计算图的 load 方法加载模型 调整 logits 向量的大小为词汇表大小 2.3 init 方法 init 方法初始化模型的生成参数： 设置采样参数：top_k、top_p、温度、重复惩罚等 设置结束 token 初始化最近生成的 token 队列 使用指定的种子初始化随机数生成器 2.4 prefill 方法 prefill 方法处理初始提示文本： 将文本分词为 token 序列 调用计算图的 post_tokenize 方法进行后处理 更新最近生成的 token 队列 执行计算图，进行预填充（prefill=true） 更新已处理的 token 数量 2.5 decode 方法 decode 方法处理用户输入并生成下一个 token： 将用户输入分词为 token 序列 调用计算图的 post_tokenize 方法进行后处理 更新最近生成的 token 队列 执行计算图，生成 logits 采样下一个 token 并更新状态 更新已处理的 token 数量 返回生成的 token 对应的文本 2.6 decode_iter 方法 decode_iter 方法迭代生成下一个 token： 记录开始时间 执行计算图，使用上一个生成的 token 作为输入 记录结束时间并更新总计算时间 采样下一个 token 并更新状态 更新已处理的 token 数量 返回生成的 token 对应的文本 2.7 sample_and_update 方法 sample_and_update 方法采样下一个 token 并更新状态： 使用 top-p 和 top-k 采样方法从 logits 中采样下一个 token 更新最近生成的 token 队列 更新上一个生成的 token 如果生成的 token 是结束 token，则停用设备 返回生成的 token 2.8 tokenize 方法 tokenize 方法将文本分词为 token 序列： 使用动态规划算法找到最优的分词方式 前向传递：计算每个位置的最优分数和对应的 token 后向传递：从末尾开始，根据前向传递的结果构建 token 序列 如果需要，添加 BOS（Beginning of Sequence）token 反转 token 序列，使其按正确的顺序排列 返回 token 序列 这个分词算法使用了贪婪的方法，尽量选择长度更长的 token，以减少序列长度。 2.9 decode_summary 方法 decode_summary 方法返回模型运行的摘要信息： 总计算时间 总计算 token 数量 平均 token 计算时间 平均 token 生成速度 3. 工作流程 ModelImp 类的典型工作流程如下： 创建和初始化： 预填充： 迭代生成： 获取摘要： 4. 关键数据结构 ModelImp 类包含以下关键数据成员： 设备和计算图： m_device：计算设备（CPU 或 GPU） m_graph：计算图，负责模型的结构和执行 模型参数： m_param：LLM 模型参数 m_config：用户配置 m_vocab：词汇表 生成参数： m_top_k、m_top_p、m_temp：采样参数 m_repeat_penalty、m_repeat_last_n：重复惩罚参数 m_end_token：结束 token 状态变量： m_past：已处理的 token 数量 m_pre_token：上一个生成的 token m_last_queue：最近生成的 token 队列，用于重复惩罚 m_logist：模型输出的 logits，表示下一个 token 的概率分布 性能统计： m_timer：计时器，用于测量计算时间 m_time_cost：总计算时间 5. 采样算法 ModelImp 类使用 llama_sample_top_p_top_k 函数进行采样，这是一个结合了 top-k 和 top-p（nucleus sampling）的采样方法： 这个采样算法的步骤如下： 应用重复惩罚，降低最近生成的 token 的概率 应用温度参数，控制分布的平滑程度 计算 softmax，将 logits 转换为概率 选择概率最高的 top-k 个 token 应用 top-p 采样，只保留累积概率达到 p 的 token 按概率采样最终的 token 6. 分词算法 ModelImp 类使用动态规划算法进行分词，尽量选择长度更长的 token： 这个分词算法的特点是： 使用动态规划找到最优的分词方式 优先选择长度更长的 token，以减少序列长度 支持添加 BOS（Beginning of Sequence）token 7. 优化策略 ModelImp 类实现了几种优化策略： 预填充模式： 预填充模式可以一次性处理多个 token，提高效率。 延迟加载： 模型权重采用延迟加载策略，只在需要时才从文件读取。 性能统计： 记录计算时间，用于性能分析和优化。 设备管理： 在生成结束时停用设备，释放资源。 8. 与其他组件的交互 ModelImp 类与框架的其他组件有密切的交互： 与 Device 的交互： 创建并管理计算设备，负责内存分配和计算执行。 与 Graph 的交互： 创建并管理计算图，负责模型的结构和执行。 与 Vocab 的交互： 创建并管理词汇表，负责文本和 token 的转换。 总结 ModelImp 类是 InferLLM 框架的核心实现类，它负责模型的加载、初始化和推理过程。它通过与 Device、Graph 和 Vocab 等组件的交互，实现了高效的大型语言模型推理。它支持多种优化策略，如预填充模式、延迟加载和性能统计，以提高推理效率。它还实现了先进的采样算法和分词算法，以提高生成文本的质量。 通过分析 model_imp.h 和 model_imp.cpp，我们可以看到 InferLLM 框架的设计思想和实现细节，这对于理解和使用框架非常有帮助。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-09/"},{"title":"[InferLLM大模型推理框架项目](08)Model类的实现(src/core/model.h+.cpp)","content":"Model 类代码结构与功能实现分析 Model 类是 InferLLM 框架的对外接口层，它封装了底层模型实现的细节，提供了一套简洁的 API 供用户使用。下面对 model.h 和 model.cpp 的代码结构和功能实现进行详细分析。 1. API 宏定义 这个宏定义用于控制函数和类的导出属性，确保在不同平台上正确导出符号： 在 Windows 平台上使用 __declspec(dllexport) 在其他平台上使用 __attribute__((visibility(&quot;default&quot;))) 2. ModelConfig 结构体 ModelConfig 结构体定义了模型的配置参数： compt_type：计算类型，支持 &quot;float32&quot;、&quot;float16&quot;、&quot;int8&quot;、&quot;int4&quot; device_type：设备类型，支持 &quot;cpu&quot;、&quot;gpu&quot; nr_thread：线程数量 nr_ctx：上下文长度 device_id：设备 ID（对于 GPU） enable_mmap：是否启用内存映射 3. Model 类 Model 类是框架的主要接口类，它使用了桥接模式，将具体实现委托给 ModelImp 类： 3.1 构造函数 构造函数接收模型配置和模型名称，创建对应的 ModelImp 实例。注释中的 TODO 表明这里未来可能会根据模型名称创建不同的实现类。 3.2 load 方法 load 方法从指定路径加载模型文件，它直接调用 ModelImp 的 load 方法。 3.3 init 方法 init 方法初始化模型的生成参数： top_k：保留概率最高的 k 个 token top_p：保留累积概率达到 p 的 token temp：温度参数，控制生成的随机性 repeat_penalty：重复惩罚，降低已生成 token 的概率 repeat_last_n：考虑最后 n 个 token 进行重复惩罚 seed：随机数种子 end_token：结束 token 3.4 token 管理方法 这两个方法用于管理 token： get_remain_token：获取剩余可生成的 token 数量 reset_token：重置 token 计数，通常在开始新的生成任务时调用 3.5 生成方法 这些方法用于文本生成： prefill：预填充模型，处理初始提示文本 decode：解码用户输入，生成下一个 token decode_iter：迭代解码，生成下一个 token（不需要用户输入） decode_summary：获取生成摘要，通常包含生成统计信息 4. 桥接模式的应用 Model 类采用了桥接模式，将接口与实现分离： Model 类提供稳定的对外接口 ModelImp 类负责具体实现 这种设计使得可以在不改变接口的情况下，更换或升级底层实现 桥接模式的优点在这里得到了充分体现： 接口稳定：用户代码只需要依赖 Model 类的接口，不需要关心具体实现 实现可替换：可以根据不同的模型类型或硬件平台，提供不同的 ModelImp 实现 隐藏复杂性：复杂的模型加载、推理逻辑都封装在 ModelImp 中，对用户透明 5. 工作流程 使用 Model 类的典型工作流程如下： 创建模型： 加载模型： 初始化参数： 预填充模型： 迭代生成： 总结 Model 类是 InferLLM 框架的对外接口层，它通过桥接模式封装了底层模型实现的细节，提供了一套简洁、稳定的 API 供用户使用。它支持模型加载、参数配置和文本生成等核心功能，使用户可以方便地使用大型语言模型进行推理。 通过分析 model.h 和 model.cpp，我们可以看到 InferLLM 框架采用了良好的设计模式和接口抽象，使得框架具有良好的可扩展性和可维护性。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-08/"},{"title":"[InferLLM大模型推理框架项目](07)KvStorage类的实现(src/core/kvstorage.h+.cpp)","content":"KvStorage 类代码结构与功能实现分析 KvStorage 类是 InferLLM 框架中用于存储注意力机制中的 Key 和 Value 缓存的特殊张量类。它继承自 Tensor 类，并提供了动态扩展内存的能力，以适应长序列生成过程中不断增长的 KV 缓存需求。 1. KvStorageConfig 类 首先，框架定义了一个单例类 KvStorageConfig，用于管理 KV 缓存的全局配置： 这个类使用单例模式，确保全局只有一个实例，用于跟踪和管理所有 KV 缓存的分配。它定义了两个关键常量： START_KV_INDEX：初始 KV 缓存大小，设为 100 KV_STEP：KV 缓存扩展步长，也设为 100 2. KvStorage 类 KvStorage 类继承自 Tensor 类，并添加了管理 KV 缓存的特殊功能： 2.1 构造函数 构造函数初始化了 KV 缓存的关键参数： 设置当前存储索引 m_store_id 为 0 记录总索引数 m_total_id 从 KvStorageConfig 获取一个新的 KV 缓存 ID 获取初始分配的索引数 m_curr_id 调整形状，只分配初始需要的内存 使用 aligned_alloc 分配对齐内存 设置为共享内存 注意，这里没有使用设备的内存池，而是直接分配对齐内存，这是因为 KV 缓存需要长期存在，不适合使用内存池管理。 2.2 set_shared_memory 方法 这个方法重写了 Tensor 类的 set_shared_memory 方法，除了设置共享内存外，还更新了当前数据指针 m_curr_data。 2.3 prepare_data_with_length 方法 这是 KvStorage 类的核心方法，它实现了动态扩展内存的功能： 首先调用基类的 prepare_data 方法确保数据已准备好 检查当前内存是否足够存储新的数据 如果内存不足，则： 计算新的形状，增加 KV_STEP 个索引 记录旧内存的长度和指针 更新形状 分配新的内存 将旧内存的数据复制到新内存 释放旧内存 设置新的共享内存 更新当前分配的索引数 更新当前数据指针 返回 TensorState::Own 表示数据已准备好 这个方法确保了 KV 缓存可以在需要时动态扩展，而不会丢失已有的数据。 2.4 其他方法 get_current_data 方法返回当前数据指针，它首先检查数据是否已准备好，然后计算当前数据的位置并返回。 add_id 方法增加当前存储索引，并更新当前数据指针。它首先检查增加后的索引是否超过总索引数，然后更新索引和数据指针。 reset_id 方法重置当前存储索引和数据指针，通常在处理新的序列时调用。 3. KvStorage 的工作流程 KvStorage 的典型工作流程如下： 创建 KvStorage： 这会分配初始大小的内存。 准备数据： 这会确保有足够的内存存储新的数据，必要时会扩展内存。 获取当前数据指针： 这会返回当前数据的指针，用于读写数据。 增加索引： 这会增加当前存储索引，通常在处理完一个 token 后调用。 重置索引： 这会重置当前存储索引，通常在处理新的序列时调用。 4. KvStorage 的优化策略 KvStorage 实现了几种优化策略： 延迟分配：初始只分配一部分内存，随着需求增加再扩展 内存对齐：使用对齐内存分配，提高内存访问效率 内存复用：在扩展内存时，复制已有数据，避免重新计算 索引管理：通过索引管理数据位置，避免不必要的内存移动 总结 KvStorage 类是 InferLLM 框架中用于管理注意力机制 KV 缓存的特殊张量类。它通过动态扩展内存的方式，支持长序列生成过程中不断增长的 KV 缓存需求。它的设计考虑了内存效率和访问效率，是框架中实现高效推理的关键组件之一。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-07/"},{"title":"[InferLLM大模型推理框架项目](06)Graph类的实现(src/core/graph.h+.cpp)","content":"Graph 类代码结构与功能实现分析 Graph 类是 InferLLM 框架中的核心组件，负责构建和执行模型的计算图。下面对 graph.h 和 graph.cpp 的代码结构和功能实现进行详细分析。 1. 核心数据结构 1.1 LlmParams 结构体 这个结构体存储了 LLM 模型的核心参数，包括模型结构信息和计算配置。 1.2 UserConfig 结构体 这个结构体存储了用户配置的计算类型。 2. OprModuleBase 类 OprModuleBase 是所有操作模块的基类，封装了一组相关的算子： OprModuleBase 的主要功能： 管理一组相关的算子 提供统一的执行接口 管理输入输出张量 计算工作空间需求 2.1 execute 方法实现 这个方法按顺序执行模块中的所有算子，并支持性能分析。 3. 特定模块实现 框架定义了多种特定的模块，用于构建不同类型的模型： 3.1 AttentionModule 这是一个模板类，可以使用不同的注意力实现（如 LlamaAttention、GlmAttention 等）。 3.2 FFN 模块 框架实现了多种 FFN（前馈网络）模块，适用于不同的模型架构： LlamaFFNModule GlmFFNModule Glm2FFNModule 3.3 HeadModule HeadModule 实现了模型的输出头，将隐藏状态映射到词汇表空间。它有一个特殊的 execute 方法，在预填充模式下不执行。 3.4 EmbdModule EmbdModule 实现了模型的嵌入层，将输入 token 映射到嵌入空间。 3.5 OneOpModule OneOpModule 是一个辅助类，用于创建只包含一个算子的模块。 4. Graph 类 Graph 类是整个计算图的管理者，负责构建模型结构、加载权重和执行推理： 4.1 execute 方法实现 这个方法是模型推理的核心，它执行以下步骤： 检查输入形状，必要时重新分配工作空间 准备输入数据 按顺序执行所有模块 复制输出数据（非预填充模式） 同步设备并释放输出内存 4.2 load 方法实现 这里采用了延迟加载策略，只记录权重在文件中的位置，实际读取在需要时进行，这样可以减少内存使用。 4.3 collect_weights 方法 这个方法收集所有模块的权重，并将它们存储在权重映射表中，以便后续加载。 4.4 reset_ctx 方法 这个方法重置所有模块的上下文，通常在处理新的序列时调用。 5. 模型构建流程 InferLLM 框架中的模型构建流程如下： 创建 Graph 对象： 加载模型参数： 这会调用 load_param 加载模型参数，然后调用 construct_llm 构建模型结构。 构建模型结构： construct_llm 方法由派生类实现，用于构建特定模型的结构。例如，LLaMA 模型的构建过程会包含以下步骤： 创建输入张量 创建嵌入模块 创建多个 Transformer 层，每层包含： 层归一化 注意力模块 残差连接 层归一化 FFN 模块 残差连接 创建输出头模块 收集权重： 这会收集所有模块的权重，并将它们存储在权重映射表中。 设置权重别名： 这会设置权重的别名，用于处理不同模型格式的权重名称差异。 6. 模型执行流程 InferLLM 框架中的模型执行流程如下： 准备输入： 这会将输入 token 复制到设备内存，并准备工作空间。 执行模块： 这会按顺序执行所有模块。 获取输出： 这会将输出 logits 复制到主机内存。 同步和清理： 这会等待所有操作完成，并释放输出内存。 7. 优化策略 Graph 类实现了几种优化策略： 延迟加载：权重数据只在需要时才从文件读取，减少内存使用 工作空间复用：使用同一块内存作为所有算子的工作空间，减少内存分配 预填充模式：支持预填充模式，可以跳过输出头的计算，提高性能 上下文重置：支持重置上下文，便于处理新的序列 8. Tensor 类与 Graph 类的交互 Tensor 类是 Graph 类的基础，它提供了数据存储和管理功能。Graph 类通过以下方式与 Tensor 类交互： 创建张量：Graph 类创建输入、输出和中间张量 管理权重：Graph 类管理模型权重，这些权重是特殊的张量 数据传输：Graph 类负责在主机和设备之间传输数据 内存管理：Graph 类通过 Tensor 类的 prepare_data 和 recall_data 方法管理内存 总结 Graph 类是 InferLLM 框架的核心组件，它通过模块化的设计，实现了灵活、高效的模型构建和执行。它支持多种模型架构（如 LLaMA、ChatGLM 等），并提供了统一的接口进行模型加载和推理。通过延迟加载、工作空间复用等优化策略，它可以在有限的内存资源下高效运行大型语言模型。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-06/"},{"title":"[InferLLM大模型推理框架项目](05)Device类的实现(src/core/device.h+.cpp)","content":"Device 类代码结构与功能实现分析 Device 类是 InferLLM 框架中的设备抽象层，负责内存管理和数据传输。它提供了统一的接口，使上层代码可以在不同硬件平台（CPU、GPU）上运行而无需修改。 1. 基类 Device 1.1 核心接口设计 Device 是一个抽象基类，定义了以下核心接口： 1.2 内存对齐实现 Device 类提供了跨平台的内存对齐分配实现： 这里使用了条件编译，根据不同平台选择合适的内存对齐函数。对齐大小固定为 32 字节，这有利于 SIMD 指令的优化。 1.3 内存池管理 Device 类使用两个映射表实现简单的内存池： 2. CPUDevice 实现 CPUDevice 是 Device 的 CPU 实现，它使用线程池进行并行计算。 2.1 构造与初始化 构造函数创建了线程池和对应类型的计算内核。 2.2 内存分配与回收 allocate 方法实现了内存池复用： 首先查找大小大于等于请求大小的最小空闲块 如果找到，则复用该块 否则，分配新的内存块并记录 free_device 方法不会真正释放内存，而是将其放回内存池中以便复用。 2.3 数据传输 由于 CPU 是统一内存架构，数据传输操作只是简单的内存拷贝： 2.4 析构函数 析构函数负责释放内存池中的所有内存块。 3. GPUDevice 实现 GPUDevice 是 Device 的 GPU 实现，它使用 CUDA 进行内存管理和计算。这部分代码通过条件编译 #if ENABLE_GPU 控制。 3.1 构造与初始化 构造函数初始化 CUDA 环境，创建 CUDA 流和 cuBLAS 句柄。 3.2 内存分配与回收 与 CPUDevice 类似，GPUDevice 也实现了内存池复用，但使用 cudaMalloc 分配 GPU 内存。 allocate_host 使用 cudaMallocHost 分配可锁页（pinned）内存，这种内存可以更高效地与 GPU 进行数据传输。 3.3 数据传输 数据传输方法使用 CUDA 内存拷贝函数，支持同步和异步两种模式。异步模式使用 CUDA 流进行操作，可以与计算重叠。 3.4 同步操作 sync 方法等待 CUDA 流中的所有操作完成。 3.5 内存特性 GPU 不是统一内存架构，因此 unified_memory 返回 false。 3.6 析构函数 析构函数释放所有 GPU 内存，并销毁 CUDA 流和 cuBLAS 句柄。 4. 内存管理策略 Device 类实现了几种内存管理策略： 内存池复用：通过 m_free_memory 和 m_alloc_memory 实现内存块的复用，减少内存分配和释放的开销 内存对齐：使用对齐内存分配，提高内存访问效率，特别是对 SIMD 指令 可锁页内存：在 GPU 实现中使用可锁页内存，提高主机和设备之间的数据传输效率 异步操作：支持异步数据传输，可以与计算重叠，提高整体性能 5. 设备抽象的意义 Device 类的抽象设计有以下几个重要意义： 硬件无关性：上层代码可以不关心具体硬件细节，通过统一接口操作不同设备 代码复用：共同的内存管理逻辑可以在基类中实现，派生类只需关注特定设备的实现 扩展性：可以方便地添加新的设备支持，如 TPU、NPU 等 优化空间：可以在不改变上层代码的情况下，优化特定设备的内存管理和数据传输 总结 Device 类是 InferLLM 框架中的关键组件，它提供了统一的设备抽象，封装了内存管理和数据传输的细节。通过内存池、内存对齐等优化策略，提高了内存使用效率。同时，它的设计使得框架可以在不同硬件平台上高效运行，为上层提供了一个稳定、高效的基础设施。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-05/"},{"title":"[InferLLM大模型推理框架项目](04)Tensor类的实现(src/core/tensor.h+.cpp)","content":"Tensor 类代码结构与功能实现分析 Tensor 类是 InferLLM 框架中的核心数据结构，用于表示和管理多维数组数据。下面对其代码结构和功能实现进行详细分析。 1. 数据类型定义 首先，框架定义了多种数据类型，以支持不同精度的计算： 同时定义了两个辅助函数： dtype_in_byte：获取数据类型的字节大小 dtype_block_size：获取数据类型的块大小（用于量化类型） 2. Tensor 类的内存管理 Tensor 类支持三种内存来源方式： 自己分配的内存 外部共享的内存 从文件映射的内存（主要用于权重） 这通过 TensorState 枚举进行管理： 3. Tensor 类的主要成员变量 4. 主要功能实现 4.1 构造函数 Tensor 类提供了两种构造方式： 初始状态都是 TensorState::OutSide，表示数据不在设备上。 4.2 形状和数据类型管理 这里的步长计算采用了行优先（row-major）的存储方式，与 C/C++ 的多维数组存储一致。 4.3 内存管理核心方法 prepare_data() 该方法确保张量数据在设备上可用： 如果数据不存在且状态为 OutSide 如果有关联文件，从文件读取 否则，从设备分配内存 将状态设置为 Own recall_data() 该方法释放张量占用的设备内存： 如果是共享内存，不做任何操作 如果不是从文件读取的，且数据存在且状态为 Own，释放设备内存 将状态设置为 OutSide 4.4 文件数据读取 该方法从文件读取数据到张量： 支持内存映射和直接读取两种方式 针对统一内存设备（CPU）和非统一内存设备（GPU）有不同处理 支持权重预处理 4.5 引用计数管理 这组方法实现了引用计数机制： add_user()：增加用户计数 decrease_curr_user_count()：减少当前用户计数，当计数为0时释放内存 resume_user_count()：恢复用户计数（用于重用张量） 4.6 数据访问 这些方法提供了对张量数据的访问： 确保数据在设备上可用 支持类型转换 5. WorkSpace 类 WorkSpace 是一个简单的内存容器类，用于临时存储计算过程中的数据： 它不负责内存的分配和释放，只是提供一个访问接口。 6. 内存优化策略 Tensor 类实现了几种内存优化策略： 延迟分配：只有在实际需要时才分配内存 引用计数：通过引用计数管理内存生命周期 内存映射：支持从文件直接映射内存，减少内存拷贝 内存共享：支持共享外部内存，避免不必要的拷贝 总结 Tensor类是 InferLLM 框架的核心数据结构，提供了灵活的内存管理和数据访问机制。它支持多种数据类型、多种内存来源方式，并通过引用计数优化内存使用。同时，它还提供了从文件读取数据和数据预处理的功能，为模型权重的加载和处理提供了支持。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-04/"},{"title":"[InferLLM大模型推理框架项目](03)InferLLM核心模块core(src/core)综合介绍","content":"InferLLM核心模块core(src/core)结构与功能 InferLLM 的 core(src/core) 模块包含了框架的核心组件，负责模型的基础数据结构、内存管理、计算图构建和执行等功能。下面对 core 模块下的主要文件进行分析： 1. 核心数据结构 1.1 Tensor (tensor.h/cpp) Tensor 是框架的基础数据结构，用于表示多维数组数据： 内存管理：支持三种内存来源 自己分配的内存 外部共享的内存 从文件映射的内存（权重） 数据类型：支持多种数据类型（Float32, Int4, Int8等） 内存状态：通过 TensorState 管理（Own, OutSide） 用户计数：通过引用计数管理内存生命周期 数据预处理：支持权重数据的预处理 关键方法： 1.2 WorkSpace (tensor.h) WorkSpace 是一个简单的内存容器，用于临时存储计算过程中的数据： 2. 设备抽象 2.1 Device (device.h/cpp) Device 是对计算设备的抽象，提供内存分配和数据传输功能： 内存管理：分配和释放设备内存 数据传输：在主机和设备之间传输数据 内核管理：持有对应设备的计算内核 派生类： CPUDevice：CPU设备实现 GPUDevice：GPU设备实现（条件编译） 关键方法： 3. 计算图构建 3.1 Graph (graph.h/cpp) Graph 是模型的骨架，定义了计算流程和模型结构： 模块管理：管理所有计算模块 权重管理：管理模型权重 执行流程：控制模型的前向计算 工作空间：管理计算过程中的临时内存 关键方法： 3.2 OprModuleBase (graph.h) OprModuleBase 是计算图中的基本单元，封装了一组相关的算子： 算子管理：管理一组相关的算子 输入输出：管理模块的输入输出张量 执行控制：控制模块的执行 派生类： AttentionModule：注意力模块 LlamaFFNModule：LLaMA前馈网络模块 GlmFFNModule：GLM前馈网络模块 HeadModule：输出头模块 EmbdModule：嵌入模块 4. 键值存储 4.1 KvStorage (kvstorage.h/cpp) KvStorage 继承自 Tensor，专门用于存储和管理键值对： 索引管理：管理存储的键值对索引 内存管理：管理键值对的内存 数据访问：提供对当前键值对的访问 关键方法： 5. 模型接口 5.1 Model (model.cpp) Model 提供了对外的模型接口，封装了内部实现： 解码接口：提供文本解码功能 模型管理：管理模型的生命周期 关键方法： 6. 线程池 6.1 ThreadPool (thread_pool.h/cpp) ThreadPool 提供了线程管理和任务调度功能： 线程管理：创建和管理工作线程 任务调度：分配任务给工作线程 同步机制：提供任务同步机制 7. 核心功能流程 模型加载： 创建 Graph 对象 构建模型结构 (construct_llm) 加载模型权重 推理执行： 准备输入数据 执行计算图 (Graph::execute) 模块按顺序执行 (OprModuleBase::execute) 获取输出结果 内存管理： 按需分配内存 (Tensor::prepare_data) 使用完毕释放内存 (Tensor::recall_data) 通过引用计数管理内存生命周期 总结 InferLLM 的 core 模块提供了一个灵活、高效的框架，用于构建和执行 LLM 模型。它通过抽象设备、张量、计算图等概念，实现了对不同硬件平台的支持和高效的内存管理。核心模块的设计清晰，各组件职责明确，便于扩展和维护。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-03/"},{"title":"[InferLLM大模型推理框架项目](02)InferLLM的src模块","content":"接下来来详细介绍InferLLM项目 src 模块的代码结构和功能： 1. 核心组件 (core/) 设备和内存管理 name=&quot;device.h/cpp&quot; path=&quot;src/core/device.h&quot;: 设备抽象层，管理计算设备(CPU/GPU)和内存分配 name=&quot;tensor.h/cpp&quot; path=&quot;src/core/tensor.h&quot;: 张量数据结构，支持多种数据类型和设备 计算图和模型 name=&quot;graph.h/cpp&quot; path=&quot;src/core/graph.h&quot;: 计算图的基础定义和管理 name=&quot;model.cpp&quot; path=&quot;src/core/model.cpp&quot;: 模型加载和管理 name=&quot;model_imp.h/cpp&quot; path=&quot;src/core/model_imp.h&quot;: 模型实现的具体细节 算子系统 name=&quot;op.h/cpp&quot; path=&quot;src/core/op.h&quot;: 基础算子定义，包括： Embedding LayerNorm MatMul Elemwise 等基础算子 其他核心功能 name=&quot;kvstorage.h&quot; path=&quot;src/core/kvstorage.h&quot;: KV缓存管理 name=&quot;thread_pool.h/cpp&quot; path=&quot;src/core/thread_pool.h&quot;: 线程池实现 2. 具体模型实现 (graph/) ChatGLM系列 name=&quot;chatGLM.h/cpp&quot; path=&quot;src/graph/chatGLM.h&quot;: ChatGLM-6B实现 name=&quot;chatGLM2.cpp&quot; path=&quot;src/graph/chatGLM2.cpp&quot;: ChatGLM2实现 name=&quot;chatGLM3.cpp&quot; path=&quot;src/graph/chatGLM3.cpp&quot;: ChatGLM3实现 LLaMA系列 name=&quot;llama_like.h/cpp&quot; path=&quot;src/graph/llama_like.h&quot;: LLaMA系列模型的基础实现 name=&quot;ggml_llama.h/cpp&quot; path=&quot;src/graph/ggml_llama.h&quot;: GGML格式的LLaMA模型支持 3. 计算内核 (kern/) 优化实现 name=&quot;optimized&quot; path=&quot;src/kern/optimized&quot;: ARM优化实现 RISC-V向量扩展优化 x86优化实现 GPU支持 name=&quot;gpu&quot; path=&quot;src/kern/gpu&quot;: GPU计算内核实现 基础实现 name=&quot;naive&quot; path=&quot;src/kern/naive&quot;: 朴素CPU实现，作为基准和后备方案 内核定义 name=&quot;kernel.h&quot; path=&quot;src/kern/kernel.h&quot;: 内核接口定义 name=&quot;kernel_define.h&quot; path=&quot;src/kern/kernel_define.h&quot;: 内核相关常量和类型定义 4. 工具类 (src/) 文件操作 name=&quot;file.h/cpp&quot; path=&quot;src/file.h&quot;: 文件读写和内存映射支持 通用工具 name=&quot;utils.h/cpp&quot; path=&quot;src/utils.h&quot;: 通用工具函数和宏定义 主要特点 模块化设计 清晰的层次结构：核心组件、模型实现、计算内核 良好的扩展性：易于添加新模型和优化实现 多平台支持 CPU/GPU计算支持 多架构优化：x86、ARM、RISC-V 性能优化 针对不同硬件的专门优化 量化支持：INT4/INT8 多线程并行计算 灵活性 支持多种模型架构 可配置的计算精度 可扩展的算子系统 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-02/"},{"title":"[InferLLM大模型推理框架项目](01)项目概括","content":"InferLLM是一个来自旷视天元MegEngine的 C++ 大模型推理框架项目，主要参考和借鉴了llama.cpp工程。 为什么不选择llama.cpp做为大模型推理框架学习入门？ llama.cpp核心代码大都在少量文件中，并且使用大量的宏。 InferLLM对 Kernel 部分进行解耦；定义 KVstorage 类型，方便缓存管理；对多平台进行了优化。 InferLLM是一个 C++ 大语言模型推理框架项目。首先从整体上分析其结构和功能： InferLLM 代码库结构分析 InferLLM 是一个轻量级的 LLM (大型语言模型) 推理框架，主要参考和借鉴了 llama.cpp 工程，但进行了更好的结构化设计。下面是对整个代码库结构的分析： 1. 顶层目录结构 2. 核心代码结构 (src/) 2.1 核心模块 (src/core/) 核心模块包含了框架的基础组件： tensor.h: 张量定义，是框架的基本数据结构 graph.h: 计算图定义，包含了模型的结构和执行逻辑 op.h: 算子定义，实现各种计算操作 kvstorage.h: KV存储，用于缓存和管理键值对 thread_pool.h: 线程池实现，用于并行计算 2.2 计算内核 (src/kern/) 计算内核部分包含了不同平台的优化实现： naive/: 朴素实现，作为基准和兜底方案 optimized/: 针对不同平台的优化实现 arm/: ARM 平台优化 x86/: x86 平台优化 rvv/: RISC-V 向量扩展优化 gpu/: GPU 实现，主要是 CUDA 相关代码 kernel_define.h: 内核定义和接口 2.3 图结构 (src/graph/) 包含了不同模型的图结构实现： 各种模型的具体实现，如 LLaMA、ChatGLM 等 3. 应用程序 (application/) 包含了各种模型的应用实现： chat.cpp: 通用聊天应用 llama/: LLaMA 模型应用 chatglm/: ChatGLM 模型应用 baichuan/: Baichuan 模型应用 quantizer.cpp: 量化工具 4. 测试代码 (test/) 测试代码分为 CPU 和 GPU 两部分： CPU/: CPU 相关测试 GPU/: GPU 相关测试 checker.h/cpp: 测试辅助工具 op_create.cpp: 算子创建测试 5. 关键数据结构和类 5.1 Tensor 张量是基本数据结构，用于表示多维数组数据。 5.2 Graph 计算图是模型的骨架，定义了计算流程和模型结构： 5.3 OprModuleBase 算子模块是计算图中的基本单元，实现各种计算操作： 5.4 KvStorage KV存储用于缓存和管理键值对，特别是在推理过程中： 5.5 WorkSpace 工作空间用于临时存储计算过程中的数据： 6. 优化策略 代码库针对不同平台实现了不同的优化策略： ARM 优化: 使用 NEON 指令集进行向量化计算 x86 优化: 使用 AVX/AVX2 指令集进行向量化计算 RISC-V 优化: 使用 RVV 向量扩展进行向量化计算 GPU 优化: 使用 CUDA 进行并行计算 7. 模型支持 根据代码库，InferLLM 支持多种模型格式： LLaMA 系列模型 ChatGLM 系列模型 Baichuan 系列模型 支持 int4 量化模型 8. 主要功能特性 多模型支持： ChatGLM 系列（ChatGLM、ChatGLM2、ChatGLM3） LLaMA 系列 Baichuan 系列 多平台优化： x86 CPU 优化 ARM 处理器优化 RISC-V 向量扩展优化 GPU 加速支持 工具支持： 模型量化工具 模型转换工具 Android 构建支持 核心功能： 高效的张量计算 动态计算图 模型推理优化 多线程支持 KV 缓存管理 9. 项目特点 跨平台支持：支持多种硬件平台和架构 模块化设计：核心功能、计算图、内核优化分层清晰 性能优化：针对不同平台提供优化的计算核心 完整的测试框架：包含单元测试和性能测试 实用工具集：提供模型转换和量化等实用工具 总结 InferLLM是一个结构清晰、模块化设计的 LLM 推理框架，通过解耦框架部分和内核部分，使得代码更易于阅读和修改。它支持多种模型格式，并针对不同平台进行了优化，可以在包括移动设备在内的多种平台上高效运行。 ","link":"https://solahome.github.io/post/InferLLM-Large-Language-Model-infer-framework-project-01/"},{"title":"数字设计与计算机架构课程的第三讲笔记","content":"数字设计与计算机架构课程笔记 ETH Zürich 2023春季学期·第三讲：组合逻辑 一、现代计算机的基本构建模块 晶体管 类型：n型MOS（nMOS）和p型MOS（pMOS）。 功能：作为电压控制开关，通过栅极电压控制导通/截止。 互补作用： nMOS：栅极高电平导通，用于“拉低”输出（连接地）。 pMOS：栅极低电平导通，用于“拉高”输出（连接电源）。 逻辑门 CMOS结构： NOT门：单个nMOS和pMOS互补连接。 NAND/AND门：串联nMOS和并联pMOS（NAND），或相反（AND）。 优势：低静态功耗、高抗干扰能力。 延迟与功耗 动态功耗：由信号翻转（充放电）引起，与频率和负载电容成正比。 静态功耗：由泄漏电流导致，随晶体管尺寸缩小而增加。 延迟影响因素：晶体管尺寸、负载电容、互联线长度。 摩尔定律 核心趋势：集成电路上可容纳的晶体管数量每18-24个月翻一番。 技术挑战： 面积：晶体管尺寸缩小受物理极限限制（量子隧穿效应）。 功耗：动态功耗因频率提升而增加，静态功耗因泄漏电流增大。 延迟：互联线延迟占主导，需通过并行处理、流水线等优化。 二、组合逻辑电路设计 逻辑电路基础 规范： 功能规范：输入输出关系（布尔方程）。 时序规范：延迟、建立/保持时间。 布尔代数与逻辑简化 核心目标： 减少逻辑门数量，降低面积、功耗和延迟。 优化电路结构，提升性能。 工具： 布尔代数规则：交换律、结合律、分配律、德摩根定理等。 卡诺图：图形化简化多变量逻辑表达式。 标准形式 积之和（SOP）：最小项的逻辑或（如：AB + AC）。 和之积（POS）：最大项的逻辑与（如：(A+B)(A+C)）。 唯一性：通过最小项/最大项索引确保唯一表示。 组合逻辑块 译码器：将n位输入转换为2ⁿ个输出，用于地址解码。 多路选择器（MUX）：通过控制信号选择多个输入之一。 全加器：实现两个1位加数及进位输入的加法，输出和与进位。 PLA（可编程逻辑阵列）：基于SOP形式的可编程逻辑电路。 三、扩展知识 逻辑完备性 定义：一组逻辑门可实现任意布尔函数。 完备集：{AND, OR, NOT}、{NAND}、{NOR}。 摩尔定律论文阅读 关键内容：历史趋势、物理限制、未来技术方向（如3D封装、新材料）。 四、总结 晶体管与逻辑门：CMOS技术通过互补开关实现高效逻辑。 布尔代数与简化：减少电路复杂度，提升性能。 组合逻辑块：构建复杂数字系统的基础模块。 摩尔定律挑战：需通过材料创新、架构优化突破物理极限。 ","link":"https://solahome.github.io/post/Digital-Design-and-Computer-Architecture-03/"},{"title":"数字设计与计算机架构课程的第二讲笔记（3）","content":"数字设计与计算机架构课程笔记 ETH Zürich 2023春季学期·第二讲：权衡、指标和设计心态 一、不同计算平台的架构设计 （一）通用处理器（CPU） 设计目标 高性能：能执行科学计算、商业应用、游戏等各类通用计算任务。 高能效：在有限功耗预算下实现高性能。 通用性：支持多种操作系统与应用软件。 架构设计特点 复杂指令集：具备丰富指令集，可处理复杂计算任务。 多核架构：通过多核并行执行多任务提升性能。 多层次缓存：减少内存访问延迟，提高性能。 指令级并行：实现同时执行多个指令，提升性能。 例子：Intel Core i7处理器、AMD Ryzen处理器 （二）图形处理器（GPU） 设计目标 高并行度：执行图形渲染、机器学习等大量并行计算任务。 高性能：在有限功耗下提供高性能。 可编程性：支持可编程着色器，完成各类图形渲染任务。 架构设计特点 SIMD架构：采用该架构同时执行多个相同指令，提高并行度。 大量处理单元：包含众多处理单元，并行执行大量计算任务。 高带宽内存：可快速传输数据。 例子：NVIDIA GeForce RTX系列、AMD Radeon RX系列 （三）张量处理器（TPU） 设计目标 高性能：执行大规模机器学习训练与推理任务。 高能效：在有限功耗下实现高性能。 专用于机器学习：专为机器学习任务设计，提升性能与能效。 架构设计特点 定制数据路径：更高效执行机器学习运算。 大量矩阵乘法单元：并行执行大量矩阵乘法运算。 高带宽内存：快速传输数据。 例子：Google Tensor Processing Unit (TPU) （四）现场可编程门阵列（FPGA） 设计目标 可定制性：依据不同应用场景定制硬件电路。 高性能：在有限功耗下提供高性能。 灵活性：通过重新编程适应不同应用场景。 架构设计特点 可编程逻辑单元：可配置成各种逻辑电路。 可编程互连：将逻辑单元连接成不同电路结构。 片上存储器：存储程序和数据。 例子：Xilinx Virtex系列、Intel Stratix系列 （五）近内存计算（NMC） 设计目标 减少数据移动：将计算单元移至靠近内存处，降低数据移动次数。 提高性能：减少数据移动以提升系统性能。 降低能耗：减少数据移动以降低系统能耗。 架构设计特点 在内存中执行计算：计算单元位于内存模块，直接在内存中计算。 定制数据路径：高效执行内存计算。 可编程性：重新编程适应不同应用场景。 例子：UPMEM Processing - in - DRAM Engine （六）处理内存（PIM） 设计目标 在内存中执行计算：计算单元集成在内存芯片，直接在内存中计算。 减少数据移动：减少数据移动提升系统性能。 降低能耗：减少数据移动降低系统能耗。 架构设计特点 计算单元集成在内存芯片中：直接在内存中执行计算。 定制数据路径：高效执行内存计算。 可编程性：重新编程适应不同应用场景。 例子：Samsung AxDIMM 二、逻辑门的选择 （一）CMOS技术优势 功耗低：稳态时电流几乎为零，静态功耗极低。 集成度高：可实现高密度集成，将大量晶体管集成在单个芯片。 可靠性高：不易受噪声干扰。 成本效益高：技术成熟，生产成本低，性价比高。 （二）其他逻辑门技术 TTL（晶体管 - 晶体管逻辑）：曾为主流，现因功耗高、集成度低，逐渐被CMOS取代。 ECL（发射极耦合逻辑）：速度快，但功耗极高，用于高速应用，如高性能计算、电信设备。 BiCMOS（双极 - CMOS）：结合双极与CMOS晶体管优点，速度和功耗介于TTL和CMOS之间。 GaAs（砷化镓）：速度比硅基CMOS快，成本高，用于高性能通信和军事应用。 （三）选择逻辑门的因素 性能需求：高速运算应用选ECL或GaAs技术电路。 功耗要求：电池供电或移动设备选低功耗的CMOS电路。 成本：成本敏感应用选成熟的CMOS技术。 可用性：选择成熟易获取的技术缩短设计周期。 （四）特定应用场景 通用处理器：常用CMOS技术实现高性能与低功耗。 GPU：采用CMOS技术，辅以流水线等特殊电路设计提高图形渲染速度。 FPGA：使用可编程CMOS逻辑门实现灵活逻辑功能。 ASIC：根据特定应用需求选择合适逻辑门技术。 总结：选择逻辑门需综合考虑性能、功耗、成本和可用性，CMOS是主流，但特定场景下其他技术可能更合适。 三、布尔代数与逻辑电路简化 （一）布尔代数规则 基本规则：AND、OR、NOT、XOR等运算符定义与运算规则。 等幂律：X + X = X，X * X = X 零律：X + 0 = X，X * 1 = X 吸收律：X + X * Y = X，X * (X + Y) = X 分配律：X + (Y * Z) = (X + Y) * (X + Z)，X * (Y + Z) = (X * Y) + (X * Z) 德摩根定律：(X + Y) = X * Y，(X * Y) = X + Y （二）逻辑简化方法 布尔代数规则：运用规则转换布尔表达式，消除冗余项或合并项。 合并项：合并相同变量项，如A * B和A * C合并为A * (B + C)。 消除冗余项：去除不影响电路输出的项，如A * (A + B)简化为A。 替换运算符：用等价运算符替换，如德摩根定律将AND换为OR。 卡诺图：可视化布尔函数相邻性，发现简化机会。 Quine - McCluskey算法：自动找到布尔表达式最小化形式。 逻辑综合工具：现代EDA工具自动进行逻辑综合，转换为最优逻辑电路结构。 （三）逻辑简化的目标 减少电路面积：减少晶体管数量，减小电路尺寸。 降低功耗：减少开关活动，降低功耗。 提高速度：减少电路级数，提高运算速度。 降低成本：减少晶体管数量与电路面积，降低生产成本。 总结：布尔代数是逻辑电路优化重要工具，除卡诺图外，还可通过多种方法实现逻辑电路简化。 四、逻辑电路功耗设计 （一）降低动态功耗 降低供电电压：显著降低动态功耗，但可能影响电路速度与性能。 降低开关频率：减少电容充放电次数，降低动态功耗。 减少翻转率：如使用时钟门控或低功耗设计技术减少信号翻转次数。 优化电路结构：选择CMOS等功耗低的逻辑门技术，采用流水线等优化电路结构。 （二）降低静态功耗 使用低泄漏晶体管：如SOI晶体管，减少静态功耗。 降低晶体管尺寸：尺寸减小，泄漏电流减小，降低静态功耗。 使用功率门控：将部分或全部电路区域置于低功耗模式。 优化电路结构：选择低功耗逻辑门技术并优化结构。 （三）其他功耗降低技术 电源管理：如动态电压和频率调节（DVFS），根据需求调整供电电压和频率。 时钟门控：使部分或全部电路区域进入低功耗模式。 低功耗设计技术：多阈值电压、低功耗逻辑门、电路级功耗降低等技术。 总结：设计逻辑电路时，可通过降低动态和静态功耗降低整体功耗，方法包括选择合适技术、优化结构、采用功耗降低与电源管理技术等。 ","link":"https://solahome.github.io/post/Digital-Design-and-Computer-Architecture-02-3/"},{"title":"数字设计与计算机架构课程的第二讲笔记（2）","content":"数字设计与计算机架构课程笔记 ETH Zürich 2023春季学期·第二讲：权衡、指标和设计心态 一、计算机系统基础 1.1 系统组成 计算：执行算术/逻辑运算（如ALU功能）。 通信：数据在组件间传输（如总线架构）。 存储/内存：快速访问程序与数据（如缓存层次结构）。 1.2 体系结构层次 从抽象到物理的层级： 问题 → 2. 算法 → 3. 逻辑 → 4. 设备 → 5. 电子 二、处理器类型与设计目标 2.1 通用 vs 专用处理器 类型 优势 劣势 通用CPU 灵活性高、可编程性强 能效/性能非最优 专用ASIC 高性能、高能效 灵活性差、开发成本高 2.2 现代处理器架构 平台 功能特点 CPU 执行通用指令（如程序控制、分支预测） GPU 并行处理图形与大规模数据（如SIMD架构） FPGA 可重构逻辑功能（如硬件原型设计） ASIC 定制化优化特定任务（如AI推理） 三、基础构建模块 3.1 晶体管与逻辑门 MOS晶体管： n型：电子导电（低电平导通）。 p型：空穴导电（高电平导通）。 CMOS逻辑门： 优势：静态功耗低、噪声容限高。 实现：互补nMOS/pMOS网络（如NAND门由并联nMOS与串联pMOS构成）。 3.2 组合逻辑电路 功能特性： 输出仅依赖当前输入（无记忆）。 标准形式：SOP（积之和）与POS（和之积）。 逻辑简化方法： 布尔代数规则（如吸收律、德摩根定律）。 Karnaugh图：通过相邻项合并最小化表达式（如BCD增量函数简化）。 3.3 组合逻辑构建块 模块 功能描述 解码器 二进制输入→独热码输出 多路选择器 按选择信号选通输入 全加器 实现二进制加法（含进位） PLA 可编程逻辑阵列（SOP/POS配置） 四、数据移动与能耗瓶颈 4.1 能量对比 计算能耗：执行算术和逻辑运算所需的能量。 数据移动能耗：将数据从内存移动到处理器，或在不同内存层次之间移动所需的能量。 4.2 瓶颈成因 内存层次结构：数据需跨多层传输（寄存器→缓存→主存）。 访问模式：缓存未命中导致频繁主存访问。 物理距离：存储器与处理器分离增加延迟与能耗。 4.3 优化技术 存内计算 (PIM)：在DRAM内集成计算单元（如Samsung AxDIMM）。 近内存计算：将处理器嵌入内存芯片（如UPMEM引擎）。 缓存优化：提升命中率以减少主存访问。 五、逻辑设计关键问题 5.1 逻辑门选择 CMOS技术：主流方案（低功耗、高集成度）。 特殊场景： ECL：高速（如高性能计算）。 GaAs：高频（如通信设备）。 5.2 功耗优化 动态功耗：降低电压、时钟门控。 静态功耗：低泄漏晶体管、功率门控。 总结：本笔记整合了数字设计的核心构建模块（晶体管、逻辑门、组合逻辑）、现代架构挑战（数据移动瓶颈）及优化策略，强调权衡分析与能效设计原则，为后续微处理器设计提供理论基础。 ","link":"https://solahome.github.io/post/Digital-Design-and-Computer-Architecture-02-2/"},{"title":"数字设计与计算机架构课程的第二讲笔记（1）","content":"数字设计与计算机架构课程笔记 ETH Zürich 2023春季学期·第二讲：权衡、指标和设计心态 一、课程概述 1.1 课程目标 核心目标 理解数字设计与计算机架构的基础原理，包括组合逻辑、时序逻辑、处理器微架构等，掌握硬件与软件接口设计，如指令集架构ISA、汇编语言编程。 评估设计中的权衡，如性能、能效、成本等，并学会系统性调试，鼓励创新设计，探索新型内存技术、数据流架构等。 1.2 课程内容 模块划分 基础设计：组合逻辑、时序逻辑、硬件描述语言（Verilog）。 处理器架构：单周期/多周期/流水线微架构、乱序执行、超标量执行。 存储技术：缓存层次、虚拟内存、多核缓存一致性。 新兴范式：数据流架构、VLIW（超长指令字）、SIMD/GPU并行化。 其他：计算机架构的现状、不同平台的设计目标和权衡、设计原则探讨。 二、核心概念与设计原则 2.1 权衡 (Tradeoffs) 定义：在互相冲突的设计目标间进行优先级决策。 关键权衡维度： 维度 示例 性能 vs 能效 高频CPU提升性能但增加功耗，如Intel Alder Lake的混合核心设计。 灵活性 vs 效率 CPU通用性强但能效低，ASIC专用性强但灵活性差，如Google TPU。 成本 vs 可靠性 冗余设计提高可靠性但增加成本，如航天级处理器。 2.2 指标 (Metrics) 性能指标：指令吞吐量（IPC）、时钟频率、延迟。 能效指标：每瓦特性能（TOPS/W）、动态功耗。 其他指标：面积（晶体管密度）、可扩展性（多核扩展效率）、用户体验（响应时间）。 2.3 设计原则 (Design Principles) 原则 vs 先例 基于原则的设计：从基础物理/数学规律出发，如CMOS电路优化。 基于先例的设计：沿用历史方案，如x86指令集兼容性。 跨领域启发 Santiago Calatrava（建筑师）：材料最优利用，类比芯片面积优化。 Frank Lloyd Wright（建筑师）：形式追随功能，如RISC指令集简化设计。 三、基础构建模块 3.1 晶体管与逻辑门 晶体管类型 nMOS：低电平导通，用于下拉网络。 pMOS：高电平导通，用于上拉网络。 CMOS逻辑门 结构：互补nMOS/pMOS组合，如NAND门由并联nMOS与串联pMOS构成。 优势：静态功耗低、噪声容限高。 3.2 组合逻辑电路 功能规范 输入到输出的纯逻辑函数，无记忆功能。 标准形式：SOP（积之和）与POS（和之积）。 逻辑简化方法 布尔代数：应用结合律、分配律等，如(A + A'B = A + B)。 Karnaugh图：通过相邻单元格合并最小项，如4变量K-map覆盖BCD增量函数。 3.3 组合逻辑构建块 模块 功能 应用示例 解码器 二进制输入到独热码输出 指令译码，如MIPS ISA。 多路选择器 从多路输入中选择一路输出 数据通路选择，如ALU输入。 可编程逻辑阵列 (PLA) 可配置SOP/POS逻辑实现 定制化ALU功能。 四、现代架构挑战与平台 4.1 计算机架构的现状 面临挑战：数据密集型工作负载、新的（设备）技术、日益复杂的应用程序和系统。 数据的重要性：数据在现代计算中起关键作用，数据移动对性能和能源效率影响显著，数据搬运能耗远高于计算。 新颖概念：新的计算范式、新的加速器和系统、新的内存、存储系统、互连和设备。 4.2 数据驱动的设计 数据移动瓶颈解决方案： 存内计算 (PIM)：三星Function-in-Memory DRAM直接在内存中处理数据。 近内存计算：UPMEM引擎将处理器嵌入DRAM芯片。 4.3 处理器平台对比 平台 优势 劣势 应用场景 CPU 通用性强、编程灵活 能效低 通用计算，如服务器。 GPU 高并行吞吐量 延迟敏感型任务性能差 图形渲染、AI训练。 FPGA 可重构硬件加速 开发复杂度高 原型验证、低延迟处理。 TPU 张量计算优化、能效高 专用性强 机器学习推理。 五、新兴技术与趋势 5.1 新兴内存技术 存算一体技术： ReRAM/CBRAM：非易失性内存，支持存内计算，如IBM的模拟AI加速器。 3D集成技术： 晶圆级堆叠。 5.2 新型计算范式 数据流架构：按数据依赖关系动态调度指令，如Google的TPU v4。 量子计算：超导量子位与纠错码（当前课程未深入涉及）。 总结：本讲系统阐述了数字设计的核心构建模块（晶体管、逻辑门、组合逻辑）、权衡与指标方法论，以及现代架构的挑战（数据移动、异构计算）。通过结合经典设计原则与新兴技术（如PIM），为后续处理器设计与优化奠定基础。 ","link":"https://solahome.github.io/post/Digital-Design-and-Computer-Architecture-02-1/"},{"title":"Leetcode每日一题：9. 回文数","content":"9. 回文数 题目地址https://leetcode.cn/problems/palindrome-number/ 题目内容 中等 给你一个整数 x ，如果 x 是一个回文整数，返回 true ；否则，返回 false 。 回文数是指正序（从左向右）和倒序（从右向左）读都是一样的整数。 例如，121 是回文，而 123 不是。 示例 1： 示例 2： 示例 3： 提示： 进阶：你能不将整数转为字符串来解决这个问题吗？ 解题思路 不转换为字符串解决问题最直接的办法就是将数倒过了，然后判断和原数是否相等，先判定是否为负数，如是负数直接返回false，时间复杂度O(logn)，空间复杂度O(1) 我的解答 ","link":"https://solahome.github.io/post/leetcode-mei-ri-yi-ti-palindrome-number/"},{"title":"Leetcode每日一题：11. 盛最多水的容器","content":"11. 盛最多水的容器 题目地址https://leetcode.cn/problems/container-with-most-water/ 题目内容 中等 给定一个长度为 n 的整数数组 height 。有 n 条垂线，第 i 条线的两个端点是 (i, 0) 和 (i, height[i]) 。 找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。 返回容器可以储存的最大水量。 说明：你不能倾斜容器。 示例 1： 示例 2： 提示： 解题思路 直接全部循环搜索会超时，使用双指针，短板会决定水最高的高度，当左右两个挡板向内移动的时候，移动长板一定会使面积变小，移动短板则不能确定，两指针从左右边缘开始搜索至它们相邻，时间复杂度O(n),空间复杂度O(1) 我的解答 ","link":"https://solahome.github.io/post/leetcode-mei-ri-yi-ti-container-with-most-water/"},{"title":"Leetcode每日一题：893. 特殊等价字符串组","content":"893. 特殊等价字符串组 题目地址https://leetcode.cn/problems/groups-of-special-equivalent-strings/ 题目内容 中等 给你一个字符串数组 words。 一步操作中，你可以交换字符串 words[i] 的任意两个偶数下标对应的字符或任意两个奇数下标对应的字符。 对两个字符串 words[i] 和 words[j] 而言，如果经过任意次数的操作，words[i] == words[j] ，那么这两个字符串是 特殊等价 的。 例如，words[i] = &quot;zzxy&quot; 和 words[j] = &quot;xyzz&quot; 是一对 特殊等价 字符串，因为可以按 &quot;zzxy&quot; -&gt; &quot;xzzy&quot; -&gt; &quot;xyzz&quot; 的操作路径使 words[i] == words[j] 。 现在规定，words 的 一组特殊等价字符串 就是 words 的一个同时满足下述条件的非空子集： 该组中的每一对字符串都是 特殊等价 的 该组字符串已经涵盖了该类别中的所有特殊等价字符串，容量达到理论上的最大值（也就是说，如果一个字符串不在该组中，那么这个字符串就 不会 与该组内任何字符串特殊等价） 返回 words 中 特殊等价字符串组 的数量。 示例 1： 示例 2： 提示： 解题思路 特殊等价情况意味着奇数位和偶数位上的字符完全相同，所以先将奇偶位分开，然后排序后对比，同时在遍历对比过程中，进行计数，时间复杂度O(n^2)，空间复杂度O(n) 我的解答 ","link":"https://solahome.github.io/post/leetcode-mei-ri-yi-ti-groups-of-special-equivalent-strings/"},{"title":"Leetcode每日一题：7. 整数反转","content":"7. 整数反转 题目地址https://leetcode.cn/problems/reverse-integer/ 题目内容 中等 给你一个 32 位的有符号整数 x ，返回将 x 中的数字部分反转后的结果。 如果反转后整数超过 32 位的有符号整数的范围 [−2^31, 2^31 − 1] ，就返回 0。 假设环境不允许存储 64 位整数（有符号或无符号）。 示例 1： 示例 2： 示例 3： 示例 4： 提示： 解题思路 先正负判断，只考虑数值，然后将数取余倒过来，进行迭代，时间复杂度O(n)，空间复杂度O(n) 我的解答 ","link":"https://solahome.github.io/post/leetcode-mei-ri-yi-ti-7-reverse-integer/"},{"title":"每日一题：1006 换个格式输出整数","content":" 题目地址https://pintia.cn/problem-sets/994805260223102976/problems/994805318855278592 题目内容 作者 CHEN, Yue 单位 浙江大学 代码长度限制 16 KB 时间限制 400 ms 内存限制 64 MB 让我们用字母 B 来表示“百”、字母 S 表示“十”，用 12...n 来表示不为零的个位数字 n（&lt;10），换个格式来输出任一个不超过 3 位的正整数。例如 234 应该被输出为 BBSSS1234，因为它有 2 个“百”、3 个“十”、以及个位的 4。 输入格式： 每个测试输入包含 1 个测试用例，给出正整数 n（&lt;1000）。 输出格式： 每个测试用例的输出占一行，用规定的格式输出 n。 输入样例1： 输出样例1： 输入样例2： 输出样例2： 我的解答 ","link":"https://solahome.github.io/post/everydayproblempatb1006/"},{"title":"每日一题：1005 继续(3n+1)猜想","content":" 题目地址https://pintia.cn/problem-sets/994805260223102976/problems/994805320306507776 题目内容 作者 CHEN, Yue 单位 浙江大学 代码长度限制 16 KB 时间限制 400 ms 内存限制 64 MB 卡拉兹(Callatz)猜想已经在1001中给出了描述。在这个题目里，情况稍微有些复杂。 当我们验证卡拉兹猜想的时候，为了避免重复计算，可以记录下递推过程中遇到的每一个数。例如对 n=3 进行验证的时候，我们需要计算 3、5、8、4、2、1，则当我们对 n=5、8、4、2 进行验证的时候，就可以直接判定卡拉兹猜想的真伪，而不需要重复计算，因为这 4 个数已经在验证3的时候遇到过了，我们称 5、8、4、2 是被 3“覆盖”的数。我们称一个数列中的某个数 n 为“关键数”，如果 n 不能被数列中的其他数字所覆盖。 现在给定一系列待验证的数字，我们只需要验证其中的几个关键数，就可以不必再重复验证余下的数字。你的任务就是找出这些关键数字，并按从大到小的顺序输出它们。 输入格式： 每个测试输入包含 1 个测试用例，第 1 行给出一个正整数 K (&lt;100)，第 2 行给出 K 个互不相同的待验证的正整数 n (1&lt;n≤100)的值，数字间用空格隔开。 输出格式： 每个测试用例的输出占一行，按从大到小的顺序输出关键数字。数字间用 1 个空格隔开，但一行中最后一个数字后没有空格。 输入样例： 输出样例： 我的解答 ","link":"https://solahome.github.io/post/everydayproblempatb1005/"},{"title":"每日一题：1004 成绩排名","content":" 题目地址https://pintia.cn/problem-sets/994805260223102976/problems/994805321640296448 题目内容 作者 CHEN, Yue 单位 浙江大学 代码长度限制 16 KB 时间限制 400 ms 内存限制 64 MB 读入 n（&gt;0）名学生的姓名、学号、成绩，分别输出成绩最高和成绩最低学生的姓名和学号。 输入格式： 每个测试输入包含 1 个测试用例，格式为 其中姓名和学号均为不超过 10 个字符的字符串，成绩为 0 到 100 之间的一个整数，这里保证在一组测试用例中没有两个学生的成绩是相同的。 输出格式： 对每个测试用例输出 2 行，第 1 行是成绩最高学生的姓名和学号，第 2 行是成绩最低学生的姓名和学号，字符串间有 1 空格。 输入样例： 输出样例： 我的解答 ","link":"https://solahome.github.io/post/everydayproblempatb1004/"},{"title":"wsl的相关命令","content":" 将wsl的位置转移： wsl的默认位置为C盘，由于系统盘通常没有足够多的空间，故想将其移动到其他的位置，直接移动文件夹显然是不行的，那么这里借用一个名为LxRunOffline的软件。 其GitHub地址为https://github.com/DDoSolitary/LxRunOffline 迁移位置的命令： 如要将其直接安装在'F:\\subsystem\\Ubuntu2004' 查看wsl的版本： 将wsl的版本由wsl1调整到wsl2(这里选择的是Ubuntu作为子系统) 取消注册（这个操作不会删除目录）: 卸载： wsl离线包： 下载地址 ","link":"https://solahome.github.io/post/wsl/"},{"title":"读书笔记(一) 人工智能导论 绪论","content":" 人工智能 artificial intelligence 以机器为载体，展式出的人类智能。 也被称为机器智能 machine intelligence 对人类智能的模拟： 符号主义-逻辑推理 概念符号化、从判断到新结论 问题求解-探寻搜索 由已有信息约束条件求解 数据驱动-机器学习 从数据出发发现内在模式 行为主义-强化学习 通过环境奖罚反馈施加最佳行动 博弈对抗-群体智能 从数据拟合优化解到均衡解 1.1 人工智能的起源 1955年8月《人工智能达特茅斯夏季研究项目提案》 A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence ① 自动计算模拟人脑高级功能 ② 使用通用语言进行计算机编程以模仿人脑推理 ③ 神经元相互连接形成概念 ④ 对计算复杂性的度量 ⑤ 算法自我提升 ⑥ 算法的抽象能力 ⑦ 随机性与创造力 人工智能是以机器为载体所实现的人类智能或生物智能。 产生两个问题： ① 承载计算的器械如何产生？ 从手工计算时代迈入自动计算时代 ② 如何用计算器模拟人工智能？ 符号逻辑-以推理为核心 联结主义-统计机器学习 行为学派-环境交互中学习策略 1.2 可计算载体：形式化与机械化 形式化系统：完备性、一致性、可判定性 1900年 算术公理的相容性 the compatibility of the arithmetical axioms 1931年 哥德尔不完备定理 1937年 图灵《论数字计算在决断难题中的应用》 1.3 智能计算方法 1.3.1 符号主义为核心的逻辑推理 人工智能问题求解中的三大方法：推理、搜索、约束满足 推理：高度概括、抽象、严格化和精确化的符号系统 亚里士多德：三段论 syllogism 归纳 inductive 推理：从特殊到一般由具体到抽象 演绎 deductive 推理：从前提推导到结论 因果 causality 推理：判断事物间存在原因和结果的关系 因果模型 structural causal model, SCM 因果图 causal diagram 推理由易到难程度分成三个层次： ① 关联 association 可直接从数据中计算得到统计相关 ② 干预 intervention 无法直接从观测数据中计算得到统计相关 ③ 反事实 counterfactual 某个事情已经发生了，则在相同环境中， 这个事情不发生会带来怎样的新结果 &quot;反事实&quot;可以用来定义事物间的因果关系。 反事实框架 counterfactual framework 或 potential outcomes是一种推断因果关系的标准。 事实是指在某个特定变量AAA的影响下可观测到的状态或结果BBB 反事实是指在该特定变量AAA取负向淔时可观测到的状态或结果BBB BBB与B′B&#x27;B′的差异存在且在统计上是显著的，则说明条件变量与结果变量之间存在因果关系。 归纳推理 如果AiA_iAi​（iii为若干取值），那么BBB：不是必然性 演绎推理 如果AAA,那么BBB：BBB是AAA的子集 因果推理 因为AAA,所以BBB：因果性不是相关性 逻辑推理推动了专家系流 expert system 的产生。 1.3.2 问题求解为核心的探寻搜索 ① 无信息搜索 uninformed search 盲目搜索方法：广度优先搜索、深度优先搜索等 ② 有信息搜索 informed search 启发式搜索 贪婪最佳优先搜索 greedy best first search A∗A*A∗搜索 ③ 对抗搜索 adversarial search 博弈搜索 game search 在一个竞争的环境中，智能体 agents 之间通过竞争实现相反的利益，一方最大化这个利益，另一方最小化这个利益 最小最大搜索 minimax search Alpha-Beta 剪枝搜索 pruning search 蒙特卡洛树搜索 Monte-Carlo tree search 1.3.3 数据驱动为核心的机器学习 数据驱动 data-drive 从数据出发，从承载表达某一概念的数据中直接学习该概念所涉及的模式，然后基于学习得到的模板对未知数据进行分类或识别。 机器学习算法可分为监督学习，无监督学习、半监督学习。 一些常见的概念： 假设空间 hypothesis space 卷积神经网络 convolutional neural network, CNN 池化层 pooling 激活函数 activation 全连接层 fully connected 误差后向传播 error back-propagation 监督学习算法：回归分析、提升算法 boosting、支持向量机和决策树等判别学习方法，隐狄利克雷分布 latent dirichlet allocation, LDA 和隐马尔可夫链等生成式学习方法。 无监督学习算法：聚类降维（主成分分析）和期望极大expectation maximization, EM 算法等。 1.3.4 行为主义为核心的强化学习 强化学习 reinforcement learning, RL 赋予智能体自监督学习能力，使其能够自主与环境交互，不故出序列决策，完成序列化形式的任务，向学会学习“learning to learn”这一能力塑造目标而努力。 强化学习起源于行为主义理论，强化学习解决的是序贯决策优化问题，即智能体与环境不断效，在某个状态采取某一行为后进入一个新的状态，根据环境给出的奖励或惩罚反馈 reward 来改进策略，以求获得最大的累积奖惩 accumulated reward。 马尔可夫决策过程 markov decision process. MDP 刻画了当前状态采取某一行动后进入后读状态，且因为采取了这一行动会从环境获得一定的奖励反馈或惩罚反馈的机制。 QQQ学习 qqq函数记录了某个状态下采取某一动作所能够收到的奖励值或惩罚值。 QQQ学习可以为智能体构造一个状态一行为效用 state-action utility 矩阵，矩阵中行和列分别代表状态和行为，矩阵的行列值为某一状态下采取某个行为所能够获得的回报。 将qqq函数参数化 parametrize,用神经网络来拟合qqq函数，深度学习与强化学习结合形成 深度强化学习 deep reinforcement learning, DRL 1.3.5博弈对抗为核心的决策智能， 博弈论 Game Theory 纳什均衡：非合作博弈 non-cooperation Games 及其均衡解定存在 现代博弈论推动机器学习从“数据拟合”过程中以“求取最优解”为核心向博弈对抗过程中“求取均衡解”为核心的转变。 Reference 吴飞编著. 人工智能导论. 北京：高等教育出版社, 2020.05. ","link":"https://solahome.github.io/post/readingnotes001/"},{"title":"计算机发展历程","content":" 第1章 计算机系统概述 1.1 计算机发展历程 分类：数字计算机、模拟计算机 1.1.1 国外计算机发展概况 1. 电子管计算机： 几千个存储单元、每秒几千次至几万次运算 输入输出：穿孔卡片或纸带 列表项体积大、功耗高、可靠性较差 列表项机器语言 2. 晶体管计算机： 磁芯存储器、存储容量10万个存储单元以上 每秒数十万次到数百万次运算 输入输出：穿孔卡片或打印机 汇编语言 开始使用 FORTRAN、COBOL和ALGOL等高级语言 3. 集成电路计算机： 主存：半导体存储器，辅存：磁盘 外部设备、操作系统 每秒数百万次至数千万次运算 计算机类型多样化、系列化 微程序、流水线、并行性 4. 超大规模集成电路计算机： 微处理器 MIPS （每秒10^6条指令） GIPS （每秒10^9条指令） TIPS 1每秒10^12条指令） 精减指令系统计算机 RISC 复杂指令系统计算机 CISC 多机并行处理与网络化 大规模并行处理系统、分布式系统、计算机网络 1.1.2 摩尔定律 “当价格不变时，集成电路上可容纳的晶体管数量大约18~24个月翻一番，性能也将提升一倍。” 2013年开始逐步放缓至3年翻一番 意义与影响 （1）单个芯片集成度提高，成本变化不大，总成本明显下降 （2）高集成度芯片中，电路间距离更近，连线更短、速度快 （3）增加芯片内部的连线，减少了外部连线，可靠性提高 （4）计算机体积越来越小，减少电能消耗，适应性更好 1.1.3 集成电路工艺发展概括 集成电路生产三大环节 IC设计、IC制造、IC封测 基本光刻工艺流程 清洗、前烘、底胶、软烘、曝光、显影、坚膜、刻蚀，去胶。 光刻的基本原理 在硅片表面覆盖层具有高度光敏感性的光刻胶，用紫外光透过掩模照射到晶圆表面，被光线照射到的光刻胶发生反应，通过蚀刻曝光或未受曝光的部分来形成沟槽，然而进行沉积、蚀刻、掺杂等操作，架构出不同材质和线路，生成基础轮廓，从而实现半导体器件在晶圆表面的构建过程。 目前晶体管制程工艺最大的挑战 有效地提高晶体管开关响应速度、减少漏电。 1.1.4 我国计算机发展概况 1958年 第一台小型电子管数字计算机。 1959年 第一台大型通用电子管数字计算机 1964年 第一台441-B晶体管通用电子计算机 ...... 1995年 “银河一五”通用并行巨型机 ...... 2009年 “天河一号”超级计算机 2016年 “神威太湖之光” Reference 谭志虎. 计算机组成原理 微课版. 北京：人民邮电出版社, 2021.02. ","link":"https://solahome.github.io/post/principlesofcomputerorganization01/"},{"title":"什么是因特网","content":" 写在前面 最近在学习计算机网络，于是就有了这篇笔记，笔记以华中科技大学 计算机通讯与网络 为基础，主要对老师在MOOC课堂上所讲授的部分内容进行了整理。 PS：我也不知道这次会写多少小节（笑） 计算机通讯与网络 第一章 计算机网络和因特网概述 1.1 什么是因特网 1.1.1 因特网的总概述 因特网是一个世界范围内的计算机网络，它将散落在世界各地的各种计算设备连接在一起。 现在因特网已经互联了遍及全世界数10亿的计算设备，预计到2020年，可能会增加到250亿的设备连接在因特网上。 1.1.2 因特网的特点 1.1.2.1 因特网的硬件构成 对于复杂的因特网，虽然它有成千上万的设备连接在一起，但是归纳起来它只有三大类的设备： 1.1.2.1.1 第一类：主机（端系统） 包括像PC机、服务器、智能手机等等，我们常用的是智能设备。 1.1.2.1.2 第二类：在网络中进行数据接收和转发的设备 包括像路由器和交换机。 1.1.2.1.3 第三类：连接各个设备的物理链路 包括有光纤、同轴电缆、双绞线、无线电等等的设备 1.1.2.2 构成因特网的软件 在这个复杂的因特网上面所跑的软件，包括在每个主机上所构成的一系列的协议： 1.1.2.2.1 应用层协议 比如HTTP协议、FTP协议 1.1.2.2.2 传输层协议 比如说TCP协议、UDP协议 1.1.2.2.3 网络层协议 比如说IP协议 1.1.2.2.4 链路层协议 比如说ppp协议、以太网协议 1.1.2.2.5 物理层协议 上面所有的协议构成了一个复杂的协议栈，这些协议从软件的角度，可以看得到因特网的复杂性。 1.1.3 什么是协议 1.1.3.1 协议的组成 人类活动是怎么进行交流的？我们人类是要通过语言进行互相交流的。 一种语言它定义了一些什么内容呢？主要是三个方面：第一个是要定义语法；第二个要定义语义；第三个要定义同步，也就是说要定义一个时序，谁先说谁后说。 在计算机的协议里面，也需要定义三个部分：包括语法、语义和同步关系。 1.1.3.1.1 语法 主要要定义一些消息的格式：就是这个什么数据该放在什么地方。 1.1.3.1.2 语义 描述网络实体之间发送的数据消息的含义。 1.1.3.1.3 同步 描述在发送消息的过程中间，事件谁先发生谁后发生。 1.1.3.2计算机通讯的过程 首先发送请求进行通讯的请求报文，第二步收到对方的请求以后，对方会发一个响应报文表示现在可以响应通讯。 接下来就开始进行具体的通讯了，计算机通讯它可能请求对方说现在要一个这样的一个文件内容，对方就会回答你要的文件，它把它的内容传给请求方。 1.1.4 因特网的结构 这样一个复杂的因特网，实际上它是有很多个不同的小网互联起来，形成的一个因特网。我们称因特网是万网之网，因特网就是指internet，也就是说很多网互联起来形成的网络，这些互联的网络它可能会包括一些子网： 1.1.4.1 第一类：接入网 也就是最先用到的边缘系统所要用的网络，通过接入网连到因特网上去。 1.1.4.2 第二类：区域主干网 1.1.4.3 第三类：主干网 通过这些不同的网络互联起来，形成了我们的因特网。 1.1.4 因特网上的服务 在因特网上面存在有大量的各种不同的服务，所以才有了成千上万的用户来使用因特网。 1.1.4.1因特网能有这么多的服务的原因 因特网向应用程序提供了统一的服务的基础设施,用一个统一的套接字编程接口来为大家服务，不同的应用都用统一的接口来编制自己的应用程序。 举例 自动驾驶、智能家电、智慧购物、社交网络、新闻浏览、电子游戏 Reference 华中科技大学计算机通讯与网络 第一章 计算机网络和因特网概述 1.1 什么是因特网 ","link":"https://solahome.github.io/post/computercommunicationandnetwork01/"},{"title":"每日一题：1002 写出这个数","content":" 题目地址https://pintia.cn/problem-sets/994805260223102976/problems/994805324509200384 题目内容 作者 CHEN, Yue 单位 浙江大学 代码长度限制 16 KB 时间限制 400 ms 内存限制 64 MB 读入一个正整数 n，计算其各位数字之和，用汉语拼音写出和的每一位数字。 输入格式： 每个测试输入包含 1 个测试用例，即给出自然数 n 的值。这里保证 n 小于 10100{10}^{100}10100。 输出格式： 在一行内输出 n 的各位数字之和的每一位，拼音数字间有 1 空格，但一行中最后一个拼音数字后没有空格。 输入样例： 输出样例： 我的解答 后记 第一次思考这个题目的时候，没有考虑到int型变量的范围限制，用int型进行输入的代码如下： 但将int型输入改为char型时，还要注意转换时的数值大小。 另外，如果在C++中使用string，需要包括头文件#include&lt;string&gt;，以及要加上using namespace std;。而且，在输出字符串时，要注意添加.c_str()。 ","link":"https://solahome.github.io/post/everydayproblempatb1002/"},{"title":"每日一题：1001 害死人不偿命的(3n+1)猜想","content":" 题目地址https://pintia.cn/problem-sets/994805260223102976/problems/994805325918486528 题目内容 作者 CHEN, Yue 单位 浙江大学 代码长度限制 16 KB 时间限制 400 ms 内存限制 64 MB 卡拉兹(Callatz)猜想： 对任何一个正整数 n，如果它是偶数，那么把它砍掉一半；如果它是奇数，那么把 (3n+1) 砍掉一半。这样一直反复砍下去，最后一定在某一步得到 n=1。卡拉兹在 1950 年的世界数学家大会上公布了这个猜想，传说当时耶鲁大学师生齐动员，拼命想证明这个貌似很傻很天真的命题，结果闹得学生们无心学业，一心只证 (3n+1)，以至于有人说这是一个阴谋，卡拉兹是在蓄意延缓美国数学界教学与科研的进展…… 我们今天的题目不是证明卡拉兹猜想，而是对给定的任一不超过 1000 的正整数 n，简单地数一下，需要多少步（砍几下）才能得到 n=1？ 输入格式： 每个测试输入包含 1 个测试用例，即给出正整数 n 的值。 输出格式： 输出从 n 计算到 1 需要的步数。 输入样例： 输出样例： 我的解答 后记 这是决定开始“每日一题”后的第一题，来自于 PAT (Basic Level) Practice （中文） 的第一题，题目没有什么难度，算是一个比较简单的开始吧，希望我能够一直坚持下去吧。 ","link":"https://solahome.github.io/post/everydayproblempatb1001/"},{"title":"冯•诺依曼结构的要点","content":" 写在前面 最近在看计算机组成原理，于是就有了这篇文章（其实也就是笔记），笔记参考了北京大学陆俊林（Lu Junlin）老师的Coursera的计算机组成课程，对老师在课程中所讲授的部分内容进行了整理。 冯•诺依曼结构主要论述的两个重要的思想 控制计算机的程序应该存放在存储器中，而不是用开关连线来实现。 开关连线会大大降低计算机的运行效率。 计算机应该采用二进制，而不是十进制。 十进制的方式会导致计算机的内部结构变得异常的复杂。 冯诺依曼结构中计算机的内部结构的五大组成部分 运算器、控制器、存储器、输入设备和输出设备。 运算器，简称CA，这是中央算术运算的缩写。 控制器，简称CC，这是中央控制的缩写。 存储器，简称M。 输入设备，简称I。 输出设备，简称O。 这五大部分连接形成一体。就构成了冯诺依曼结构的计算机。 冯诺依曼结构的小结 冯诺依曼结构说明了计算机应该由五大部分组成。 指出计算机的数据和程序均应该以二级制代码的形式，不加区别的存放在存储器中，存放的位置由存储器的地址指定。 计算机在工作时能够自动的从存储器中取出指令加以执行。 运算器、控制器和存储器是冯诺依曼结构的核心。 与现代计算机进行对照，运算器和控制器应该对应于现在计算机中的CPU， 而存储器应该对应于现在计算机中的主存储器。 我们又常称之为主存或者内存。 CPU和主存储器之间一般通过系统总线进行连接。 以个人计算机为例：在这块个人计算机的主板上会有CPU芯片和主存，通过主板上的系统总线进行连接。 冯诺依曼结构的运转方式 计算机运转的核心内容就是执行指令， 计算机执行一条指令的主要步骤呢包括如下四步： 第一步称为取址， 第二步称为译码， 第三步称为执行， 第四步称为回写。 依次执行完这四部，计算机就完成了一条指令的执行。 第一步取址 查看下一条指令的地址，找到地址所在的位置后， 向主存发送请求，他并不知道存放的指令是什么，他只是根据地址向主存发出申请。主存并不是一些简单的存储单元， 他还有一些控制逻辑，这类控制逻辑会响应控制器的请求， 他会找到指定地址内存储的内容，并把内容给送回去， 控制器收到了地址里面的内容后，就把他放在存放当前位置， 这就完成了第一步，取址的工作。 而实际上这一步还需要有一项任务。 就是更新下一条指令的位置。只是先更新， 等一会去执行下一条指令的时候再根据这个位置去向主存发出请求。 第二步译码 译码就是分析刚才得到的内容。 这个内容第一说明了这项任务所需要用的方法。 第二说明了这项任务所需要的东西。 任务完成之后，把结果存放在某个地方， 这个取址得到的内容上指定了，存放到指定的位置里。这个任务就分析完了。 控制器就把他转换成若干组的控制信号一一完成， 这样译码这个阶段就算完成了。 第三步执行 第三步，对计算机执行指令是其核心的步骤， 就叫做执行。对程式来说一样，执行这个已经分析完的任务， 这个任务一共有4步：第一步是要去取运算要用的操作数，与刚才一样， 向主存发出请求，主存会响应，控制器会把返回的操作数放在运算器的其中一个入口；第二步是取这个运算的第二个操作数；第三步是执行这项运算，控制器会给出对应的信号给运算器，让它执行对应的运算， 经过短暂的时间之后，运算结果产生了。 执行阶段就到此结束了。 运算结果还放在运算器的这个输出端口上。 第四步回写 最后一步回写也就是保存结果的过程，用控制器把运算结果存放在A的通用计算器中， 就完成了执行指令的全部过程。 结束这条指令后，控制器也不会停下，会去执行下一条指令，重复上面的四个步骤。不断的重复，计算机就会自动运转起来了。 ","link":"https://solahome.github.io/post/vonneumannstructure/"},{"title":"浅谈C和C++中的头文件","content":" 选择什么计算机语言呢？ 在谈谈头文件之前，先让我们简单的来看看刷题时选择什么计算机语言。 可以用来刷题的语言很多，比如说：C、C++、Java、Python、Go等等，都可以用来刷题。一般情况下，大多数平台都会支持C、C++、Java、Python，一般建议选择C或C++，因为它们通常更加通用，而且会更少出现问题。 这样对C和C++的头文件有一个简单了解就非常有必要了。 先看看两段入门级的代码 C语言 输出结果： C++ 输出结果： 先来看一下常见的C语言的代码结构： 头文件在最前面，如#include&lt;stdio.h&gt; 这一行就是头文件，也是最常见的一行代码，基本上每个C语言程序代码都有这一行。下面我们就来简单地介绍一下这行代码。 其中，stdio.h是标准输入输出库，stdio的全称是standard input output，h就是head的缩写，.h是头文件的文件格式。 如果在程序中需要进行输入输出，那么就需要这个头文件。一般情况下，程序都是有输入和输出的，所以基本上每一个C语言程序都需要加上这个头文件。 直观上来说，stdio.h就是一个文件（也就是我们常说的头文件），这个文件中包含了一些跟输入输出有关的代码（函数），如果一个程序需要输入输出，就要通过#include&lt;头文件名称&gt;这样的写法来包含这个文件，这样才可以使用stdio.h这个文件里与输入输出有关的函数。 刷题时，我们通常会选择使用C++，而在C++的标准中，stdio.h更推荐改为：cstdio。 因为#include&lt;stdio.h&gt;和#include&lt;cstdio&gt;的写法在一定程度上来说是等价的。 还有一些这样的例子，比如说： #include&lt;math.h&gt;和#include&lt;cmath&gt; 是等价的； #include&lt;string.h&gt;和#include&lt;cstring&gt; 也是等价的。 ","link":"https://solahome.github.io/post/cheaderfile/"},{"title":"Python中Tab键与空格键在处理代码缩进时的问题","content":" 在Python中，缩进相同但不挨着的代码什么关系？代码缩进Tab键和四个空格有什么区别吗？ Python语言是通过缩进来组织代码块 Python语言是通过缩进来组织代码块。 在Python中具有相同缩进的代码会被自动视为一个代码块，无论有几个空格的缩进都是被允许的，但是要求缩进空格的数量统一。 代码缩进中tab和四个空格有什么区别 在ASCII码中，Tab键的编码是9，空格键的编码是32。当我们输入一个Tab键的时候，它可能看起来像是8个空格或者4个空格，但实际上，却不是同一个东西。 不同的编辑器里Tab键表示的长度也可能不一致，在一个编辑器里用Tab键进行缩进后，在其它编辑器里可能会得到并不相同的缩进。 Tab键和空格在Python中都可以表示缩进，在一般情况下，使用4个空格来表示一个缩进；也可以使用一个Tab键来表示一个缩进。 空格代替Tab键的优点 空格键在各种情况下的代码中都仍然是空格键。而Tab键则会在不同的标准下变为不同的字符宽度。 而且Tab键是制表符而不是缩进符，大量使用制表符当作缩进符有时会出现一些不必要的错误。 注意 需要注意的是在一个Python文件中，只使用一种缩进方式，要避免制表符Tab键和空格键的混用，以避免造成不必要的错误。 解决方法 通常建议可以对编辑器进行设置，将一个Tab键设置为4个空格，输入Tab键时进行自动转换。 Reference 初学Python：写代码时应该缩进使用 tab还是空格? ","link":"https://solahome.github.io/post/pythontabkongge/"},{"title":"关于scanf/printf及cin/cout在刷题时的选择","content":" 输入与输出问题 最近在准备刷题的时候，才突然意识到关于输入和输出的一些小问题。 大部分刷题网站、OJ和测试都是支持C和C++的，而且一般对C和C++的支持相比于其他计算机语言要更加的完备。而C++又兼容大部分C语言的语法，所以一般都是编写C++代码文件，但这个时候关于输入和输出用什么方式的选择就成为了一个问题：是用scanf/printf呢？还是cin/cout呢？ 关于scanf/printf和cin/cout两者的基本用法 scanf/printf 格式化输入、输出，通过格式控制符来控制输入输出的样式： 输入： 输出： cin/cout 通过流来输入、输出数据： 流读取运算符&gt;&gt;和cin一起使用，可以输入数据。 流插入运算符&lt;&lt;和cout一起使用，可以输出数据。 两者比较 由于cin和cout可以不指定输入输出格式，所以相比而言比scanf和printf方便，但是cin和cout消耗的时间比scanf和printf要多很多，在很多对时间有要求题目中，可能输入还没结束就已经超时了。 另外需要注意的是，为了避免不必要的麻烦，在一个程序中最好不要同时使用cout和printf。 ","link":"https://solahome.github.io/post/aboutscanfprintfcincoutchoose/"},{"title":"C++运行中的错误：error: stray '\\302'(或'\\240') in program","content":" 最近准备写一下简单的C++代码，因为不太喜欢vc++和codeblocks的图形化界面，又不太想用Visual Studio这种大型的IDE，所以就配置了一下之前用的Sublime Text3，为了检测环境变量是否配置成功，我从复制了一段C++常见的“hello world”代码： 源代码 注意：你现在看到的上面的这段代码是我已经修改过的正确的代码。 错误信息 源代码与上面这段代码从直观上可以说没有什么明显的区别，代码在运行过程中出现了如下两种报错信息： 解决方式 这个问题看起来一般是很少会遇到的，大概率程序本身是没有语法错误，它的问题就出在源代码中可能含有一些隐藏的非ascii字符，或者是代码的编码方式有所不同。 一个常见的情况就是复制的时候，有些空格并不是普通的空格，但我们却没有看出来。 简单的解决办法是将出现空格的地方的空格去掉，在需要空格的地方将空格重新补上即可。 Reference error: stray ‘\\302’ in program 原因与解决办法 C/C++编译报错 error: stray '\\240'('\\302') in program ","link":"https://solahome.github.io/post/c-error-stray-302-240-in-program/"},{"title":"Python编辑器的选择","content":" 前言 想要对Python以及它的包管理器Anaconda有一个大致的了解可以看看前面的两篇文章： 写给刚刚开始学习Python的初学者 关于Python的包管理器Anaconda的一些常见问题 本文主要介绍Python的一些常用编辑器的特点，方便大家选择合适的编辑器。 Visual Studio Code(VSCode) VSCode在安装Anaconda可能会提示安装，也可以自己下载安装，在Anaconda中安装，配置更加简便，VSCode作为轻量级编辑器，有种类繁多的插件，支持多种计算机语言，比较方便进行工程项目的编写，语法高亮功能可以通过一些插件实现。 VSCode常用Python插件 Python Anaconda Extension Pack Chinese (Simplified) Language Pack for Visual Studio Code vscode-icons One Dark Pro Brackets Light Pro Bracket Pair Colorizer Pycharm Pycharm社区版免费，其大部分功能足够一般的个人开发，其功能相对VSCode更加完善，但占用空间更大，启动速度相对较慢，比较适合大型工程项目。 Sublime Text3 Sublime Text3是一个相比VSCode可能更加轻量级的编辑器，支持多种计算机语言。个人认为比较适合进行单个Python文件的编写。 Jupyter(JupyterLab和Jupyter Notebook) Jupyter Notebook是一种笔记本形式的编写代码方式，比较适合将代码展现给其他人，也比较合适对一个Python文件进行分段调试结果。JupyterLab是Jupyter Notebook的一种升级情况，拥有更强大的功能，这两者都是依靠浏览器来显示的，只要安装了浏览器的设备都可以使用，甚至可以远程连接服务器使用。 后记 我也没有想到前面关于Python学习的准备工作能够写这么多（哈哈），不过仔细回忆起来，作为初学者，初学Python的时候也确乎是遇到了各种问题，好像想写都写不完的样子。关于Python学习的准备工作的文章到这里大概就差不多结束了，后面可能会有文章来介绍Python编程中的实际问题吧。 ","link":"https://solahome.github.io/post/pythoneditor/"},{"title":"关于Python的包管理器Anaconda的一些常见问题","content":" 前言 关于Python的一些学习建议可以看看下面的这篇文章 写给刚刚开始学习Python的初学者 通过Anaconda安装 Python安装过程存在环境配置的问题，直接安装Python的安装包后需要通过命令行来编写运行程序，或者通过配置相关的编译器或是大型的IDE来编写运行程序。而且由于Python具有多个不同的版本，它的一些工具包存在不同版本间的兼容问题，需要对Python进行环境管理。 这里推荐使用方便进行环境管理的Anaconda来进行安装，它有关于Python2和Python3两个版本，由于Python2已经停止维护，所以建议使用Python3的版本，Anaconda支持Windows、MacOS、Linux平台，关于安装包以及工具包可以把源换到 清华大学开源软件镜像站的Anaconda镜像。 Anaconda镜像换源 以清华大学开源软件镜像站的Anaconda镜像其中的Anaconda包镜像之一https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/为例 添加源 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ 设置搜索时显示通道地址 conda config --set show_channel_urls yes 查看当前的config conda config --show 查看添加的镜像 conda config --get channels 删除源 conda config --remove-key channels 一些可用的镜像源 清华大学开源软件镜像站 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/ 上海交通大学开源镜像站 https://mirrors.sjtug.sjtu.edu.cn/anaconda/pkgs/main/ https://mirrors.sjtug.sjtu.edu.cn/anaconda/pkgs/free/ https://mirrors.sjtug.sjtu.edu.cn/anaconda/cloud/conda-forge/ 关于Anaconda的安装 整个安装过程几乎都是可以一直Next进行下去，有几个小地方值得稍微注意一下： 一般情况下Install for: 可以选择Just Me，这样就不需要电脑的管理员权限 另外一个是安装的位置不一定是要在C盘，也就是默认的文件夹，安装位置也可以是其他的地方 添加环境变量，关于这个的说法比较多，其实它的意思可以理解成就是你是否可以在系统命令行中使用conda命令。如果添加了，就可以在系统命令行中直接使用conda命令，否则就只能在Anaconda Powershell Prompt和Anaconda Prompt中使用。 ","link":"https://solahome.github.io/post/aboutpythonanaconda/"},{"title":"写给刚刚开始学习Python的初学者","content":" Python入门 Python和其他大多数计算机语言一样，相比于其他的入门语言，比如说C语言，Python的语法甚至相对更简单一些（比如数字与字符串在一些情况下甚至不用int和char等来进行区分），这些会让Python入门相对更加容易，但相对的也可能会带来一些问题。由于Python是一种解释性语言，所以与C语言有所不同，不太方便编译成如以.exe为后缀的可执行文件，所以有用C语言开发的CPython解释器。也正是因为Python的一些特性，Python也存在一些安全性相关的问题。 这里，我的个人建议是如果是第一次接触计算机编程，可以先从其他的语言开始（我的从C语言开始的），虽然也有朋友推荐通过简单的Python入门，不过个人感觉正是由于Python在某些语法的简化，可能会使一些编程的初学者不太注意一些细节，之后在学习其他编程语言时缺少一些规范性的基本知识。 Python学习方法 和其他的一些语言一样，Python最好的学习方法就是看它的官方文档，如果稍微有一些其他计算机语言的基础，直接活用官方文档进行学习是效率和效果最好的。相比于现在已经停止更新的Python2版本，现在我们最好学习Python的Python3版本。如果需要在较短的时间掌握一定的基础，也可以挑选一些网上的视频快速观看，有一个大概的了解即可，之后遇到问题再去查找相关的解答。 Python在数据分析和科学计算中有比较多的应用，因为Python具有较多的包来满足各种计算的需要，也正是因为这个，Python才更需要包管理和环境管理，不同版本的包之间是否相互协调是一个需要注意的问题（后面我会介绍一下使用Anaconda的进行环境管理中会遇到的一些问题的解决方案）。 Python这一适合进行科学计算的性质使它广泛的被用于机器学习、深度学习中（我也是因为这一原因而接触到Python的）。 最后推荐一个可以方便大家了解Python在运行中的内存处理机制的（即电脑上的Python代码是如何进行运行的）网站http://pythontutor.com/。 ","link":"https://solahome.github.io/post/topythonbeginner/"},{"title":"漫漫长路一相逢，便胜却人间无数","content":" 一个人与另一个人的相遇是时光中的一瞬，分别却是我们的常态，茫茫人海，我们相遇便是缘分 关于博主 （趁着这个想写点什么的机会重新介绍介绍一下自己吧哈哈哈哈。） 博主是前数学竞赛、物理竞赛选手； 博主的涉猎研究方向有： 深度学习框架 高性能计算 量化金融研究 基于深度学习算法的加速芯片设计 自动驾驶感知算法&amp;端到端算法 模型压缩与量化 大语言模型 声纹识别 主要实习经历 金融量化研究员 大模型算法工程师 还有一些其它的实习及项目经历就不赘述啦，博主在涉猎的研究方向都有实习或者项目经历（属于什么都懂点啦哈哈哈哈），目前在关注于深度学习框架方向，以后可能也会对AI编译有所涉猎吧 update 2025.02.25 一转眼时间过得好快呀哈哈哈哈 博主最近有时间可能会更新一些深度学习框架方面的内容（如果鸽了就当博主没有说过吧） 2023.03.10 好久没有更新了（笑），最近准备再次更新起来，Leetcode可能要刷起来了 博主最近对统计推断，图神经网络，量化金融比较感兴趣，可能会持续分享一些阅读笔记吧（如果鸽了就当博主没有说过吧） 2020.07.25 博主可能对python、c、c++等语言有一些了解，还稍微懂点汇编语言 博主对机器学习、深度学习比较感兴趣 博主可能之后也会学学一些Java、Cocos Creator等等（也许还会学习一下前端吧） 关于博客 之前一直想有一个自己的博客，可惜又不想花费太多时间，借着一个暑假里面的空闲，用Gridea工具搭建，主题使用的是fog，博主稍微做了一点点修改 在这个博客上，博主可能会分享一些学习感悟、学习笔记，当然可能也还会分享一些好玩的小东西 通过“标签”界面，你可以看到博主文章的大致分类，找到你想看的内容 update 2023.03.10 最近把封面图都换成了Colorhubhttps://www.colorhub.me/上的免费高清无版权图片了，如有侵权，欢迎联系删除。 ","link":"https://solahome.github.io/post/about/"}]}